На предыдущей лекции, посвященной терминам машинного обучения, мы выяснили, что процедура машинного обучения невозможна без оптимизации — с помощью неё мы подбираем параметры модели и улучшаем показатели функции ошибки и метрик качества.  

На этом занятии мы распахнём завесу тайны, стоящей за алгоритмами оптимизации, и узнаем про основные подходы, существующие в этом направлении  

### Общая интуиция

**Визуализация**

Прежде всего стоит вспомнить геометрическую интерпретацию параметров модели. Напомним: все параметры модели являются числами, а значит множество этих чисел можно записать в виде вектора.

Двумерный вектор можно изобразить на двумерной плоскости, тогда как трёхмерный возможно проиллюстрировать только в трёхмерном пространстве. Значит, вектор, состоящий из d параметров указывает на какое-то место в d-мерном пространстве. Любой набор значений параметров модели можно проиллюстрировать  . Назовём этот график поверхностью ошибок

> **Поверхность ошибок** — график, показывающий зависимость функции ошибок от параметров модели

Но не будем же мы рисовать кривую ошибок в стомерном пространстве? Для наглядности в рамках этого урока давайте рассмотрим простую модель с двумя параметрами

### Градиентный спуск

#### **Введение в градиентный спуск**

Градиентный спуск — один из наиболее популярных алгоритмов оптимизации, который используется для нахождения минимума или максимума функции. Он основан на вычислении градиента функции и движении в направлении, противоположном направлению градиента.

> **Градиент функции в точке** — это вектор, составленный из частных производных функции по каждому из её аргументов. Градиент указывает направление наибольшего возрастания функции в данной точке.

Интуитивная картина происходящего заключается в том, что мы начинаем с некоторой начальной точки и вычисляем градиент функции в этой точке. Затем мы делаем шаг в направлении, противоположном градиенту, то есть в сторону уменьшения функции. Мы повторяем этот процесс до тех пор, пока не достигнем точки минимума или максимума.

Геометрически градиентный спуск можно представить как движение вниз по склону горы. В этом случае функция представляет собой поверхность, а градиент — направление наибольшего наклона поверхности. Мы начинаем с вершины горы и спускаемся вниз, двигаясь в направлении, противоположном вектору градиента. Это позволяет нам найти точку минимума функции.

#### **Алгоритм градиентного спуска**

Алгоритм градиентного спуска состоит из следующих шагов:

1. Выбрать начальную точку $x_0$.
    
2. Вычислить градиент функции $f(x)$ в точке $x_0$: $\nabla f(x_0)$.
    
3. Сделать шаг в направлении, противоположном градиенту: $x_{n+1} = x_n - \alpha \nabla f(x_n)$, где $\alpha$ — размер шага.
    
4. Повторять шаги 2 и 3 до достижения точки минимума.

Размер шага $\alpha$ выбирается таким образом, чтобы обеспечить сходимость алгоритма к точке минимума. Если размер шага слишком мал, алгоритм будет сходиться медленно, а если слишком велик — может расходиться.

#### **Скорость сходимости градиентного спуска**

Скорость сходимости градиентного спуска зависит от нескольких факторов, включая размер шага $\alpha$, начальную точку и свойства функции $f(x)$. В общем случае градиентный спуск сходится линейно, но при определённых условиях может сходиться квадратично. Квадратичная сходимость означает, что расстояние до точки минимума уменьшается квадратично с каждым шагом.

Для обеспечения сходимости к точке минимума необходимо выбирать размер шага таким образом, чтобы он был достаточно малым, но не слишком маленьким. Существует несколько методов выбора размера шага, включая адаптивные методы, которые автоматически подстраивают размер шага в процессе работы алгоритма.

#### **Стохастический градиентный спуск (SGD)**

Стохастический градиентный спуск является модификацией классического градиентного спуска. В SGD вместо вычисления градиента на всей выборке данных используется только одна случайно выбранная точка. Это позволяет ускорить сходимость алгоритма и уменьшить риск переобучения модели.

Однако стохастический градиентный спуск имеет и недостатки, такие как более высокая дисперсия оценок градиента и возможность расхождения алгоритма при некоторых условиях. Для обеспечения сходимости SGD необходимо выбирать размер шага с учётом свойств функции и характеристик данных.

В этой главе мы рассмотрели основные аспекты градиентного спуска, включая его геометрическую интерпретацию, алгоритм, скорость сходимости и стохастическую версию. Градиентный спуск остаётся одним из основных инструментов оптимизации в машинном обучении и других областях.

## Разнообразие выборок

### Обучающая и тестовая выборки

В машинном обучении выборка данных играет ключевую роль в процессе обучения модели. Выборка делится на две основные категории: обучающую и тестовую.

> **Обучающая выборка** — это набор данных, который используется для обучения модели. Модель обучается на основе этой выборки, чтобы научиться распознавать закономерности и делать предсказания.

> **Тестовая выборка** — это отдельный набор данных, который не используется в процессе обучения. Он предназначен для оценки производительности модели после её обучения. Использование тестовой выборки позволяет получить объективную оценку точности модели и избежать переобучения.

Обучающая выборка содержит данные, на которых модель учится делать предсказания, а тестовая выборка используется для проверки этих предсказаний. Это разделение помогает определить, насколько хорошо модель обобщает полученные знания на новые, ранее не встречавшиеся данные.

### Гиперпараметры

Гиперпараметры — это параметры, которые задаются перед началом процесса обучения и влияют на саму структуру и поведение модели. Например, в градиентном спуске Они не изменяются в процессе обучения, в отличие от весов модели, которые обновляются на основе обучающих данных. Выбор гиперпараметров может существенно повлиять на производительность модели и её способность к обобщению.

> Гиперпараметр — настраиваемый параметр всей работы алгоритма

Настройка гиперпараметров — это важный этап в разработке модели машинного обучения. Она включает в себя выбор оптимальных значений гиперпараметров, которые обеспечивают наилучшую производительность модели на обучающей и тестовой выборках. Существует несколько методов настройки гиперпараметров, включая поиск по сетке (grid search) и случайный поиск (random search).

### Валидационная выборка

Валидационная выборка — это дополнительный набор данных, используемый для тонкой настройки гиперпараметров модели. В отличие от тестовой выборки, валидационная выборка используется в процессе настройки гиперпараметров.

Модель обучается на обучающей выборке, затем производительность проверяется на валидационной выборке. Если производительность на валидационной выборке ухудшается, это может указывать на переобучение модели. В этом случае можно попробовать изменить гиперпараметры или добавить регуляризацию.

![](/api/attachments.redirect?id=3f6b1558-0037-4bfa-9032-d58ae614706c)

Использование валидационной выборки помогает выбрать оптимальные гиперпараметры и предотвратить переобучение. Это особенно полезно при работе с большими наборами данных, где тестирование на тестовой выборке может быть затруднено из-за вычислительных ограничений.

Таким образом, обучающая, тестовая и валидационная выборки являются важными инструментами в арсенале машинного обучения. Понимание их роли и правильное использование позволяет создавать более точные и надёжные модели, способные эффективно работать с новыми данными.

## Табличные данные

### Пояснение про таблицы

### Типы признаков

### Отбор признаков