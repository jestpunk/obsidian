# Классический ML

### Метрические методы

Как перевести задачу многоклассовой классификации в задачу бинарной классификации
> Ответ: Сделать а-ля one-hot encoding, но можно и еще как-то

Recall, precision итд кто такие

F-мера кто такая

- - -

>[!question]- В чём заключается метод граничных деревьев?
>Сначала по выборке строим дерево, а затем идём от корня до предсказываемой вершины, пока не окажется так, что у текущей вершины дерева локально нет более близких вершин.

>[!question]- Как строить граничное дерево?
>Выбираем случайный объект за корень. Далее применяем ко всей выборке алгоритм. Если угадали, оставляем всё как есть. Если не угадали, добавляем её в дерево

>[!question]- Что обеспечивает "граничность" дерева?
>Именно то, что точки, предсказываемые верно, не берутся в дерево

- - -

### Линейные методы

>[!question]- Какие плюсы и минусы у линейных моделей?
>+: Они просты, быстры, интерпретируемы, экстраполируемы, обобщаемы до нелинейных
>-: редко когда линейная зависимость, страдает от выбросов, неоднородности признаков, линейной зависимости признаков

>[!question]- Откуда появляется необходимость в регуляризации?
>Если оставить веса большими, то ошибка на объекте с большим весом будет очень велика, что нежелательно

>[!question]- Какая у регуляризации есть геометрическая интерпретация?
>Она отвечает за "зазор", устойчивость алгоритма к маленьким изменениям. На примере с SVM при увеличении зазора мы как раз минимизируем вес

- - -
### Решающие деревья



- - -

### Ансамбли

>[!question]- Основные особенности catboost
>- Хорошо работает с дефолтными параметрами
>- Хорошо работает с категориальными фычами
>- Строит дерево поуровнево, сбалансированно
>- Может подобрать темп обучения по мета-признакам линейной моделью

>[!question]- OOB score кто такой?
>Берем объект выборки и считаем лосс между ним и каждым алгоритмом беггинга, в который этот объект не попал (оказался out-of-bag)

Random forest кто такой

- - -

### Градиентный бустинг

Откуда тут градиент

Как рассчитывается важность признаков в случайном лесу (интерпретируемость модели)?

# DL
## Классический DL

>[!question]- Что такое Batch norm?
>**Batch norm**  – нормировка батча с последующим линейным сдвигом в слоях с сжимающей функцией активации
>- Создана Google в 2015 году
>- *Интуиция:* Рассмотрим на примере. Мы не хотим передавать сжимающей функции $tanh$ очень маленькие или очень большие по модулю числа – в первом случае функция активации не будет ничего делать, во втором случае нейрон будет выдавать константу, то есть умрёт. Так почему бы перед этим не нормировать батч? В добавок к этому добавим обучающиеся параметры линейного сдвига, чтобы нейрон мог контролировать "нормализацию"
>- Для разных функций активации есть скаляры, умножая на которые мы приводим к нормализации, но делать это вручную для глубокой сети гораздо сложнее чем просто поставить нормализацию по выборке, которой является батч

>[!important]- Бесплатная аугментация в batch norm
>До этого метода объединение в батчи было исключительно вопросом *производительности*, однако теперь каждая из активаций после batch norm начинает зависеть ото всех остальных объектов в батче, что делает выбор батча более сложной задачей (не хочется делать это вслепую, как мы делали, когда думали только об эффективности программы)
>
>Удивительно, но это только помогает нашей модели – когда она получает функцию, зависящую от одного объекта, есть шанс переобучения. Однако "смазывая" и зашумляя объект всем оставшимся батчем, мы "варьируем" наш обьект. Обучаясь на в том числе слегка других данных, мы делаем что-то в духе *аугментаций*. Можно представить, что батч-норм буквально растряхивает наш обьект в вероятностном пространстве. Этот эффект является в том числе *регуляризирующим*

>[!important]- Ненужный bias
>При нормализации, идущей после линейного слоя (любого, в котором есть сдвиг на вектор $b$), мы вычитаем эмпирическое среднее, которое включает в себя сдвиг и отменяет его. Получается, мы зазря держим вектор $b$, ведь от него ничего не зависит и его градиент всегда равен нулю. 
>
>В частности поэтому мы тренируем веса для сдвига в рамках самого batch norm, уже ((после)) сдвига на выборочное среднее


>[!warning]- Проблема batch norm
>После обучения сети мы хотим давать ей один объект и выдавать один ответ, но теперь у нас внутри сети захардкожена необходимость подавать данные батчами. Что же делать?
>
>**Ответ** накапливаем среднее и дисперсию по ходу обучения

- - -

## Computer vision

## NLP

- - -

Лемматизация, стемминг

### Векторизация

>[!summary]- Виды эмбдеддингов
>- **Разреженные / sparce** *(TF-IDF, PPMI)* – слова представляются простой функцией от остальных слов
>
>&nbsp;
>
>- **Плотные / dense** *(word2vec)* – представление создаётся обучением классификатора. Например в *NPLM* (Neural Probabilistic Language Model, 2003) каждому токену соответствует обучаемая матрица эмбеддинга
>&nbsp;
>  ![[Pasted image 20230723203801.png]]
>  Именно плотные представления используются в глубоком обучении, так как лучше обрабатывают большое количество входящих признаков
>

>[!summary]- PMI
>**PMI** (*Pointwise mutual information*) – логарифмическое отношение вероятности слов оказаться рядом к такой же вероятности, но для случая их полной независимости (при независимости это 0)
>$$PMI(w_1, w_2) = \log_2 \dfrac{P(w_1, w_2)}{P(w_1)\cdot P(w_2)}$$

>[!summary]- PPMI
>PMI может быть отрицательным, если слова встречаются реже чем будучи независимыми. Тогда у нас скорее всего не будет достаточно данных, чтобы посчитать точное значение $PMI$, да и заострять на редких сочетаниях внимание не хочется, используем $$PPMI = \max(PMI, 0)$$

- - -

>[!summary]- word2vec
>Метод векторизации, основанный на предсказании. Обучаем нейросетью векторизовать слово с учётом его контекста. Есть два принципиально разных подхода –  skip-gram и CBOW
>
> - **skip-gram** – решаем промежуточную задачу: берем каждое слово датасета и маскируем контекст. Пытаемся настроить веса так, чтобы слово угадывало контекст
> ![[Pasted image 20230723214926.png]]
> Для этого задаём ширину окна, и для каждого слова + каждого слова из контекста создаём пару, которая будет участвовать в предсказании.
> *Плюсы* – хорошо работает на маленьких данных, хорошо репрезнтует даже редкие слова
>
> &nbsp;
> 
> - **CBOW** (*Continuous bag of words*) – наоборот, маскируем слово и пытаемся выдать его в соответствии с контекстом
>   *Плюсы* – в несколько раз быстрее, лучше точность для частых слов
 
>[!question]- Как с точки зрения архитектуры обучаются эти эмбеддинги?
>
>Skip-gram берет пары *слово-контекст* и учится на них
>![[Pasted image 20230723220402.png]]
>
>&nbsp;
>
>CBOW берет контекст как bag of words и учится на нём, суммируя вектора
>![[Pasted image 20230723220438.png]]

>[!summary]- doc2vec
>То же, только векторезует документы. Аналогом CBOW является PV-DM, в которой мы в дополнение к CBOW добавляем эмбеддинг документа.
>![[Pasted image 20230724025451.png]]
>&nbsp;
>Аналогом skip-gram будет PV-DBOW, в которой мы пытаемся угадать контекст по эмбеддингу документа
>![[Pasted image 20230724025542.png]]

- - -

>[!summary]- TF-IDF
>**TF-IDF** (*Term Freqency – Inverse Document Frequency*) – способ обозначить "качество" слов в наборе документов. Чем у слова больше TF-IDF, тем оно более ёмко описывает именно этот документ (то есть встречается в нём достаточно часто и при этом не встречается часто в других документах)
>
>- **TF** – удельная частота слова в документе
>$$TF(word, doc) = \dfrac{count(word\_in\_doc)}{len(doc)}$$
>- **IDF** – количество документов, в которых присутствует это слово (в знаменателе), количество документов (в числителе)
>$$IDF^*(word) = \dfrac{len(docs)}{count(docs\_with\_word)}$$
>так как это число потенциально очень большое, его логарифмируют и прибавляют единицу на случай, если слово никогда не встречается в списке
>$$IDF(word) = \log(\dfrac{len(docs)}{count(docs\_with\_word) + 1})$$

- - -

### Topic modeling

Предполагаем что документ состоит из набора топиков, топик состоит из набора слов. 

>[!summary]- LSA
>**LSA** (*Latent Semantic Analysis*) – строит матрицы документ-топик и слово-топик.
>
>- Сначала строит документ-слово $M$ (только не просто считает слова, а берет tf-idf слова)
>
>- Затем, так как матрица разрежена, для поиска зависимостей (топиков) используется понижение размерности – а именно усеченное SVD $$M = USV$$
>Размерность усечения – количество желаемых топиков. $U$ – *матрица документ-топик*, $V$ – *матрица слово-топик*. 
>
>**МИНУСЫ**:
>- плохо интерпретируется (не знаем названий топиков, на какие слова они заточены)
>- нужно очень много данных для хорошей работы

>[!summary]- pLSA
>То же, но вместо SVD используется вероятностный подход. Основная суть – найти вероятностную модель, которая могла бы *генерировать* те данные, что мы наблюдаем. 
>
>Пусть есть документ $D$, топик $T$, слово $W$, то вероятность встретить слово в документе$$P(W, D) = P(D)\cdot\sum\limits_{T \in topics}P(T | D)\cdot P(W | T)$$
>Именно эти вероятности (не считая $P(D)$, которые мы получим из датасета), являются параметрами модели, обучаемыми как правило на *EM (Expectation maximization)* алгоритме
>
>**МИНУСЫ**:
>- Так как берем готовое $P(D)$, непонятно что делать с новым документов
>- Количество параметров растёт линейно от размера выборки, поэтому может переобучаться

>[!summary ]- LDA
>**LDA** (*Latent Dirichlet Allocation*) – pLSA, но использующее байесовский подход. Берем распределения Дирихле, из которых семплим во-первых распределение топиков, во-вторых распределение слов, а дальше так же настраиваем веса модели
>
>**ПЛЮСЫ**:
>- отлично экстраполируется на новые документы (у нас есть целое распределения Дирихле документ-топик)

# Математика

## Статистика

>[!question]- $p_{value}$ кто такой?
>Наименьший уровень значимости, на котором $H_0$ отвергается
>
>То есть если $p_{value} \leq \alpha$, то отклоняем гипотезу

- - -

>[!summary]-  Дивергенция Кульбака-Лейблера
> - **Дивергенция Кульбака-Лейблера** — несимметричная мера удалённости от вероятностного распределения $A$  распределения $B$ (можно понимать как количество потерянной информации об $A$, если мы будем приближать его распределением $B$)
> $$D(A\;||\;B) = \int\limits_X a\log\dfrac{a}{b}d\mu = \mathbb{E}(\log(a(x)) - \log(b(x) | \theta))$$
> - >[!question]- Что такое дивергенция Кульбака-Лейблера?
> >написано сверху
> - >[!question]- симметрична ли дивергенция Кульбака-Лейблера?
>>Нет
> - >[!question]- В чём её физический смысл?
>>Потерянная информация при использовании B вместо A

>[!summary]-  Коэффициенты корелляции
>- **Коэффициент корелляции Пирсона** — нормированная ковариация, описывающая линейность зависимости
>	$$corr(X, Y) = \dfrac{cov(X, Y)}{\sqrt{var(X)var(Y)}} =$$
>	$$= \dfrac{
>	\mathbb{E}((X - \mathbb{E}(X))(Y - \mathbb{E}(Y)))}
>	{\sqrt{
>	\mathbb{E}((X - \mathbb{E}(X))(X - \mathbb{E}(X)))\cdot\mathbb{E}((Y - \mathbb{E}(Y))(Y - \mathbb{E}(Y)))}
>	}$$
>- **Коэффициент корелляции Спирмена** — ранговая корелляция, описывающая монотонную связанность
>	$$corr(X, Y) = \dfrac{\sum\limits_{i=1}^{m}(rank(x_i) - \dfrac{m+1}{2})\cdot(rank(y_i) - \dfrac{m+1}{2})}{\dfrac{1}{12}(m^3-m)}$$
>	
>- >[!question]- Как коэффициент Пирсона соотносится с зависимостью переменных
>	>Независимые переменные некореллированы, наоборот не всегда работает
>- >[!question]- Какие зависимости обнаруживает каждый коэффициент
>	>Пирсон линейные, Спирмен монотонные
>	

>[!question]-  Какие бывают ядра у непараметрических оценок?
>- **Треугольное**:  $\max(\min(1+x, 1-x), 0)$
>-
>- **Квартическое**: $\dfrac{15}{16}(1-x^2)^2\cdot I(|x| < 1)$ 
>- 
>- **Прямоугольное** $\dfrac{1}{2}I(|x| < 1)$
>-
>- **Гауссовское** $\dfrac{1}{\sqrt{2\pi}} \exp(-\dfrac{x^Tx}{2})$
>- 
>- **Епанечникова** $\dfrac{3}{4}(1-x^2)\cdot I(|x|\leq 1)$

- - -

>[!question]- Что такое Bootstrap?
>Приём, позволяющий искусственно увеличивать объём выборки и генерировать её распределение. В нём мы отбираем объекты из нашей выборки так, будто берём их из реального распределения. С повторениями.

>[!question]- Чем Bootstrap помогает в машинном обучении?
>Позволяет не уменьшать размер выборки при разделение на трейн и тест, а просто набирать с повторениями (всё, что не вошло из-за повторов, идёт в тест)

>[!question]- Сколько объектов в среднем будет попадать в тест при Bootstrap (если мы добиваем бутстрепом до размера изначальной выборки)?
>Шанс этого равен шансу ни разу не быть выбранным. $$(1-\dfrac{1}{n})^n \sim e^{-1} \sim 37\%$$

- - -

>[!question]- Что такое MAP оценка?
>Максимум Апостериорной Вероятности это оценка, связанная с ММП, но берущая во внимание априорное распределение величины. Если ММП выглядит так:
>$$ML = \arg\max\limits_{\theta} p(x|\theta)$$
>То MAP берет во внимание некоторое апостериорное распределение параметра $g(\theta)$. Тогда по теореме Байеса функция MAP будет выглядеть как
>$$MAP = \arg\max\limits_{\theta}\dfrac{p(x|\theta) \cdot g(\theta)}{\int p(x|\theta') \cdot g(\theta') d\theta'}$$

>[!question]- Как MAP оценка связана с ММП?
>MAP превращается в ММП при константном апостериорном распределении
 
## Линал

>[!question]-  что такое SVD

>[!question]-  почему SVD хорошо обрезать матрицы
>Truncated SVD матрица является ближайшей к изначальной матрице среди матриц своего размера с точки зрения фробениусовой метрики

# Computer science

>[!question]- Какие существуют принципы ООП?
наследование, полиморфизм, инкапсуляция. Есть еще абстракция

>[!question]- Что такое полиморфизм?
Способность методов работать с данными разных типов (и для насекомого, и для самолёта можно запустить команду "лети")

>[!question]- Стек vs куча
>**СТЕК**: один из пяти сегментов памяти, в котором данные (локальные переменные и вызовы функций) хранятся по принципу LIFO, у стека есть указатель, хранящий текущее положение стека, и жестко лимитированный размер
>- очень быстрый доступ
>- не нужно напрямую освобождать память
>- память эффективно лежит в CPU
>- хранит только локальные переменные
>- ограничен в размерах (зависит от ОС)
>- нельзя поменять размер переменных
>
>&nbsp;
>
>**КУЧА**: один из пяти сегментов памяти, размер которого не лимитирован конкретным числом до компиляции, размер выделяемой памяти на стеке динамически контролируется пользователем. 
>- глобальные переменные
>- нет лимита на размер памяти
>- более медленный доступ
>- никаких гарантий по эффективности выделения памяти
>- надо самим подчищать память
>- можно менять размер выделенной памяти
## Database

>[!summary]- Нормальные формы
>- **1НФ**: все строки уникальны, атомарны, нет повторяющихся атрибутов
>- **2НФ**: 1НФ + есть первичный ключ, все атрибуты зависят от всего ключа (если он сложный, то от всех его частей)
>- **3НФ**: 2НФ + все атрибуты зависят от ключа напрямую, нетранзитивно

## Python

Питон кто такой
> Высокоуровневый интерпретируемый язык программирования с динамической *(присваиваем тип сколько угодно раз, а не при инициализации)* строгой*(не всегда делает автоматический каст)* типизацией и автоматическим управлением памятью