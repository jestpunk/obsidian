# Классический ML


# DL
## Классический DL

## Computer vision

## NLP
# Математика

## Статистика

>[!summary]-  Дивергенция Кульбака-Лейблера
> - **Дивергенция Кульбака-Лейблера** — несимметричная мера удалённости от вероятностного распределения $A$  распределения $B$ (можно понимать как количество потерянной информации об $A$, если мы будем приближать его распределением $B$)
> $$D(A\;||\;B) = \int\limits_X a\log\dfrac{a}{b}d\mu = \mathbb{E}(\log(a(x)) - \log(b(x) | \theta))$$
> - >[!question]- Что такое дивергенция Кульбака-Лейблера?
> >написано сверху
> - >[!question]- симметрична ли дивергенция Кульбака-Лейблера?
>>Нет
> - >[!question]- В чём её физический смысл?
>>Потерянная информация при использовании B вместо A

>[!summary]-  Коэффициенты корелляции
>- **Коэффициент корелляции Пирсона** — нормированная ковариация, описывающая линейность зависимости
>	$$corr(X, Y) = \dfrac{cov(X, Y)}{\sqrt{var(X)var(Y)}} =$$
>	$$= \dfrac{
>	\mathbb{E}((X - \mathbb{E}(X))(Y - \mathbb{E}(Y)))}
>	{\sqrt{
>	\mathbb{E}((X - \mathbb{E}(X))(X - \mathbb{E}(X)))\cdot\mathbb{E}((Y - \mathbb{E}(Y))(Y - \mathbb{E}(Y)))}
>	}$$
>- **Коэффициент корелляции Спирмена** — ранговая корелляция, описывающая монотонную связанность
>	$$corr(X, Y) = \dfrac{\sum\limits_{i=1}^{m}(rank(x_i) - \dfrac{m+1}{2})\cdot(rank(y_i) - \dfrac{m+1}{2})}{\dfrac{1}{12}(m^3-m)}$$
>	
>- >[!question]- Как коэффициент Пирсона соотносится с зависимостью переменных
>	>Независимые переменные некореллированы, наоборот не всегда работает
>- >[!question]- Какие зависимости обнаруживает каждый коэффициент
>	>Пирсон линейные, Спирмен монотонные
>	

>[!summary]-  Ядра непараметрической оценки
## Линал


# Бизнес