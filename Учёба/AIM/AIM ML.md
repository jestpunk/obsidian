# ML
#ml #ozon #prog #course | [[ml]] [[ozon]] [[prog]] [[course]]
Курс Дьяконова

- - -

&nbsp;

## Content
### [[AIM ML#AIM ML Content Постановка задач|Постановка задач]]
1. [[AIM ML#AIM ML Content Обучение с учителем (*supervised*)|Обучение с учителем]]
2. [[AIM ML#AIM ML Content Генерация признаков|Генерация признаков]]
3. [[AIM ML#AIM ML Content Подход к решению задач|Подход к решению задач]]
4. [[AIM ML#AIM ML Content Переобучение (*overfitting*)|Переобучение]]
5. [[AIM ML#AIM ML Content Обучение без учителя (*unsupervised*)|Обучение без учителя]]
6. [[AIM ML#AIM ML Content Обучение с частично размеченными данными (*semi-supervised*)|Обучение с частично размеченными данными]]
7. [[AIM ML#AIM ML Content Привелегированное обучение (*LUPI*)|Привелегированное обучение]]

### [[AIM ML#AIM ML Content Математика|Математика]]
1. [[AIM ML#AIM ML Content Связь плотности и вероятности|Связь плотности и вероятности]]
2. [[AIM ML#AIM ML Content Параметры случайной величины|Параметры случайной величины]]
3. [[AIM ML#AIM ML Content Оценки|Оценки]]
4. [[AIM ML#AIM ML Content Максимальное правдоподобие|Максимальное правдоподобие]]
5. [[AIM ML#AIM ML Content Дивергенция Кульбака-Лейблера|Дивергенция Кульбака-Лейблера]]
6. [[AIM ML#AIM ML Content Ковариация и корреляция|Ковариация и корреляция]]
7. [[AIM ML#AIM ML Content Оценка плотности|Оценка плотности]]
8. [[AIM ML#AIM ML Content Непараметрические методы|Непараметрические методы]]
9. [[AIM ML#AIM ML Content Теория информации|Теория информации]]
10. [[AIM ML#AIM ML Content Проклятие размерности|Проклятие размерности]]
11. [[AIM ML#AIM ML Content Сингулярное разложение (SVD)|Сингулярное разложение]]
12. [[AIM ML#AIM ML Content Матричное дифференцирование|Матричное дифференцирование]]

### [[AIM ML#AIM ML Content Метрические методы|Метрические методы]]
1. [[AIM ML#AIM ML Content Метрики|Метрики]]
1. [[AIM ML#AIM ML Content Алгоритм ближайших центроид|Метод ближайших центроид]]
1. [[AIM ML#AIM ML Content KNN|KNN]]
1. [[AIM ML#AIM ML Content Сглаживание Надарая-Ватсона|Сглаживание Надарая-Ватсона]]
1. [[AIM ML#AIM ML Content Обучение метрик|Обучение метрики]]

### [[AIM ML#AIM ML Content Приближенные метрические методы|Приближенные метрические методы]]
1. [[AIM ML#AIM ML Content kD-дерево|kD-дерево]]
1. [[AIM ML#AIM ML Content Random Trees|Random trees]]
1. [[AIM ML#AIM ML Content LSH|LSH]]
1. [[AIM ML#AIM ML Content Boundary trees|Boundary trees]]
1. [[AIM ML#AIM ML Content Квантование|Квантование]]
1. [[AIM ML#AIM ML Content Navigable Small World (NSW)|Navigable Small World (NSW)]]

### [[AIM ML#AIM ML Content Контроль качества и выбор модели|Контроль качества и выбор модели]]
1. [[AIM ML#AIM ML Content Train test split|Train test split]]
2. [[AIM ML#AIM ML Content Дубликаты|Дубликаты]]
3. [[AIM ML#AIM ML Content Random subsampling CV|Random subsampling CV]]
4. [[AIM ML#AIM ML Content Bootstrap|Bootstrap]]
5. [[AIM ML#AIM ML Content K-fold CV|K-fold CV]]
6. [[AIM ML#AIM ML Content LOOCV|LOOCV]]
7. [[AIM ML#AIM ML Content Out of time CV|Out-of-time CV]]
8. [[AIM ML#AIM ML Content Перебор параметров|Перебор параметров]]
9. [[AIM ML#AIM ML Content Байесовская оптимизация|Байесовская оптимизация]]

### [[AIM ML#AIM ML Content Оптимизация|Оптимизация]]
1. [[AIM ML#AIM ML Content Покоординатный спуск (0 порядок)|Покоординатный спуск]]
2. [[AIM ML#AIM ML Content Градиентный спуск (1 порядок)|Градиентный спуск]]
3. [[AIM ML#AIM ML Content Стохастический градиентный спуск / SGD (1 порядок)|Стохастический градиентный спуск]]
4. [[AIM ML#AIM ML Content Градиентный спуск и ML|Градиентный спуск и ML]]
5. [[AIM ML#AIM ML Content Метод Ньютона (2 порядок)|Метод Ньютона]]
6. [[AIM ML#AIM ML Content BFGS (1 порядок)|BFGS]]
7. [[AIM ML#AIM ML Content Условная оптимизация|Условная оптимизация]]

### [[AIM ML#AIM ML Content Линейные методы|Линейные методы]]
1. [[AIM ML#AIM ML Content Линейная регрессия|Линейная регрессия]]
2. [[AIM ML#AIM ML Content Обобщенная линейная регрессия|Обобщенная линейная регрессия]]
3. [[AIM ML#AIM ML Content Регуляризация|Регуляризация]]
4. [[AIM ML#AIM ML Content Гребневая регрессия|Гребневая регрессия]]
5. [[AIM ML#AIM ML Content LASSO|LASSO]]
6. [[AIM ML#AIM ML Content Elastic Net (LASSO + Ridge)|Elastic Net]]
7. [[AIM ML#AIM ML Content Селекция признаков|Селекция признаков]]
8. [[AIM ML#AIM ML Content Уменьшение размерности|Уменьшение размерности]]
9. [[AIM ML#AIM ML Content Увеличение выборки|Увеличение выборки]]
10. [[AIM ML#AIM ML Content Устойчивая регрессия|Устойчивая регрессия]]
11. [[AIM ML#AIM ML Content RANSAC|RANSAC]]
12. [[AIM ML#AIM ML Content Проекционная матрица|Проекционная матрица]]
13. [[AIM ML#AIM ML Content Линейная регрессия и SVD|Линейная регрессия и SVD]]
14. [[AIM ML#AIM ML Content Поиск ортогонального соответствия|Поиск ортогонального соответствия]]
15. [[AIM ML#AIM ML Content Бинарная классификация и сигмоида|Бинарная классификация и сигмоида]]
16. [[AIM ML#AIM ML Content Многоклассовая логистическая регрессия|Многоклассовая логистическая регрессия]]
17. [[AIM ML#AIM ML Content Итоги|Итоги]]

### [[AIM ML#AIM ML Content Линейная классификация|Линейная классификация]]
1. [[AIM ML#AIM ML Content Бинарная классификация|Бинарная классификация]]
2. [[AIM ML#AIM ML Content Персептронный алгоритм|Персептронный алгоритм]]
3. [[AIM ML#AIM ML Content SVM|SVM]]
- - -

&nbsp;


## [[AIM ML#Content|Постановка задач]]

&nbsp;
### [[AIM ML#Content|Обучение с учителем (supervised)]]
У нас есть размеченная обучающая выборка, на которой мы должны обучиться и предсказывать метки для новых объектов. Формально
$$X_{train} = \{(x_1,y_1), \dots, (x_m, y_m)\}\text{ - обучающая выборка}$$ 
Необходимо приблизить ту целевую функцию $y: X \rightarrow Y$, для которой
$$y(x_1)=y_1,\; y(x_2)=y_2\;\text{ и так далее}$$

**Цели**
1. Восстановить зависимости
2. Объяснить устройство и природу объектов
3. Оценить ошибку нашего решения

 &nbsp;
**Типы задач обучения с учителем**
1. ***Классификация*** -- разбиваем на классы
	- *Бинарная классификация* -- два класса
	- *Скоринговая бинарная* -- находим вероятность принадлежности одному из двух классов
	- *Multiclass classification* -- $K$ непересекающихся классов, надо присвоить один
	- *Multi-label classification* -- $K$ пересекающихся классов, тогда на выходе даем вектор из $\{0,1\}^K$, то есть для каждого класса метка, лежит ли объект в классе
2. ***Регрессия*** -- присваиваем число
3. ***Прогнозирование*** -- получаем не только значение, а еще и время замера, и должны предсказать значение на $k$ замеров вперед

&nbsp;
### [[AIM ML#Content|Генерация признаков]]

Если у задачи нет признаков или они плохие, их надо формализовать и создать. Чем лучше сгенерируем признаки, тем проще ML нам потребуется (если признак прямо указывает на то что нам нужно, то чтобы это увидеть нужен очень простой алгоритм и наоборот)

&nbsp;
### [[AIM ML#Content|Подход к решению задач]]

Можно основываться на близости -- брать ближайшие объекты из выборки и на основе их меток давать метку нашему. То есть либо брать среднюю метку между всеми совпадающими объектами, либо, если совпадающих нет (как в задаче регрессии) брать среднюю метку между всеми *близкими* объектами

Можно основываться на предсказывании функции и брать ее значения. Тогда можно попытаться предсказать вид нашей функции (линейная, полиномиальная, тригонометрическая) и подобрать параметры этой функции

&nbsp;
### [[AIM ML#Content|Переобучение (*overfitting*)]]

Чем сложнее модель мы придумали (чем сложнее формула, которой мы описываем задачу), тем проще эту формулу подобрать, но тем выше шанс переобучения, когда мы слишком хорошо ложимся на выборку, даже на выбросы, и упускаем общую тенденцию

&nbsp;
### [[AIM ML#Content|Обучение без учителя (*unsupervised*)]]

Нет размеченности, просто даны данные. Что тут можно делать? Если дана задача разделения на классы, можно по виду объектов понять, какие близко а какие далеко, или в задаче детектирвания аномалий нужно выделить выбросы из общей тенденции

&nbsp;
### [[AIM ML#Content|Обучение с частично размеченными данными (*semi-supervised*)]]

Из названия понятно, что это когда у части значений мы знаем метку, а у части нет. Например, вообще без меток мы бы просто определили два кластера объектов и одному бы дали наугад красную, а другому синюю метки, а благодаря части размеченных данных мы знаем, какие именно кластеры красные, а какие синие

![[Pasted image 20220711024714.png]]

&nbsp;
### [[AIM ML#Content|Привелегированное обучение (*LUPI*)]]

Полностью называется *Learning Using Priveleged Information*. Это когда кроме датасета есть еще данные, но их значения мы на контроле не знаем. Например, если нужно предсказать наличие вируса у человека, то в больнице для тестовой выборки можно померить много каких признаков -- давление, состав крови, рост, вес. Однако по итогу хотелось бы давать результат на основе менее замороченных величин, то есть не смотреть их значение вообще. 

То есть дополнительные данные могут подсказать нам, как будут выглядеть важные для нас признаки и от чего будут зависеть, и на основе этого уже давать ответ

Также существует *SLUPI* как объединение идей *LUPI* и *SSL*

![[Pasted image 20220711024627.png]]
- - -

&nbsp;
## [[AIM ML#Content|Математика]]

Пусть есть случайная величина $\xi$, принимающая значения $x_1, \dots, x_n$. 
Очевидно, что $$\sum\limits_{i}P(\xi = x_i) = 1;\;p_i \geq 0$$

Если $\xi$ непрерывна, есть функция плотности $p(x)$ и функция распределения $$F(x) = \int\limits_{-\infty}^xp(z)dz$$

Будем активно пользоваться тем, что $$\mathbb{P}(a \leq \xi \leq b) = \int\limits_a^bp(z)dz$$
&nbsp;
### [[AIM ML#Content|Связь плотности и вероятности]]

Рассмотрим вероятность попадания $\xi$ в небольшую окрестность точки $x_0$
$$\mathbb{P}(x - \varepsilon \leq \xi \leq x + \varepsilon) = \int\limits_{x - \varepsilon}^{x + \varepsilon}p(z)dz \approx 2\varepsilon p(x)$$

Значит вероятность прямо пропорциональна значению плотности, что в целом логично

&nbsp;
### [[AIM ML#Content|Параметры случайной величины]]

Из очевидного -- *матожидание* (центр масс) $$\mathbb{E}(\xi) = \int xp(x)dx$$
*Дисперсия* (средний квадрат отклонения от $\mathbb{E}(\xi)$)
$$\mathbb{E}(\xi - \mathbb{E}(\xi))^2 = \int(x - \mathbb{E}(\xi))^2p(x)dx$$
*Условная плотность* (по сути просто сужение вероятностного пространства до $y$)
$$p(x\;|\;y) = \frac{p(x,y)}{p(y)}$$
И очевидный симметричный перерасчет
$$p(x\;|\;y)p(y) = p(x,y) = p(y\;|\;x)p(x)$$

- - -

Можно переходить от совместной плотности к плотности от одного аргумента двумя способами

1. *Маргинализация плотности* по неизвестной компоненте (интегрируем по одной из компонент) $$p(x) = \int p(x,y)dy = \int p(x\;|\;y)p(y) dy$$
2. *Обуславливание плотности* по известной компоненте (поиск условной плотности) $$p(x\;|\;y) = \frac{p(x,y)}{p(y)}$$

То есть когда получили совместное, много от чего зависящее распределение есть два варианта -- если значение одного из параметров нам становится известно, просто считаем плотность при условии что параметр принимает это значение. А если неизвестно, то интегрируем по всем возможным значениям (что-то в духе формулы полной вероятности, надо сложить все случаи)

**Правило произведения**
$$p(x_1,\dots,x_n) = p(x_1\;|\;x_2,\dots,x_n)p(x_2\;|\;x_3,\dots,x_n)\dots p(x_{n-1}\;|\;x_n)p(x_n)$$
&nbsp;

### [[AIM ML#Content|Оценки]]

Есть выборка. Задача матстатистики -- найти (*оценить*) параметры модели, по которой была выбрана выборка.

*Статистика*, или *точечная оценка* -- это функция, зависящая от выборки
$$\hat \theta = g(x_1,\dots, x_n)$$
Это, по сути, тоже случаная величина

&nbsp;

#### Требования к оценке

>[!done]- Несмешенность
> 
> *Смещение оценки* -- это отклонение матожидания оценки от истинного значения
> $$bias(\hat\theta) = \mathbb{E}(\hat\theta) - \theta$$
> *Несмещенная оценка* -- оценка без смещения
> $$bias(\hat\theta) = 0$$
> *Асимптотически несмещенная оценка*
> $$bias(\hat\theta) \rightarrow 0$$
> 
> Например, для нормального распределения несмещенные оценки это
> $$\hat\mu = \frac{x_1 + x_2 + \dots + x_n}{n}$$
> $$\hat\sigma^2 = \frac{1}{n-1}\sum\limits_i(x_i - \hat\mu)^2$$

>[!done]- Устойчивость
> 
> Оценка не должна сильно меняться в зависимости от выборки. То есть $var(\hat\theta) \rightarrow \min$.
> К примеру, для указанной выше оценки разброс следующий 
> $$var(\hat\mu) = \frac{\sigma^2}{n} \rightarrow 0$$

>[!done]- Состоятельность
> С ростом числа наблюдений должна быть сходимость куда надо (например, для нормального распределения первый элемент сам по себе -- несмещенная оценка, однако он не приближает нас к ответу по мере увеличения значений)
> 
> Выборка *состоятельна*, если она сходится по вероятности куда надо $$\hat\theta \xrightarrow[P]{} \theta$$
> То есть
> $$\forall \; \varepsilon > 0\;\;P(|\hat\theta - \theta| > \varepsilon) \rightarrow 0\;\;\text{при}\;\;n \rightarrow \infty $$

&nbsp;

### [[AIM ML#Content|Максимальное правдоподобие]]

Если $y_1, \dots, y_n$ - *Н.О.Р.С.В.*, то *максимальное правдоподобие* есть
$$\prod\limits_ip(y_i\;|\;\theta)$$
То есть мы проверяем для каждой величины, насколько вероятно ее значение при именно этом параметре, и перемножаем результат

Соответственно, *оценка максимального правдоподобия* есть
$$\hat\theta = arg\max\limits_\theta\prod\limits_ip(y_i\;|\;\theta)$$

Она может быть смещенная, но зато она состоятельна и *асимптотически эффективна* среди асимптотически нормальных (имеет наименьшую дисперсию из всех)

Так как это произведение, на самом деле это будет очень небольшая величина

&nbsp;

### [[AIM ML#Content|Дивергенция Кульбака-Лейблера]]

Есть выборка ${x_1, x_2, \dots, x_n}$, взятая из распределения, у которого плотность $p$

Попробуем найти некоторое распределение $q(x\;|\;\theta)$. Для этого применим к параметру $\theta$  ММП

$$\hat\theta = arg\max_\theta \prod\limits_i q(x_i\;|\;\theta)$$

Приведем к сумме логарифмов

$$arg\max_\theta \sum\limits_i log\;q(x_i\;|\;\theta)$$

Вот этот переход под вопросом

$$arg\max_\theta \frac{1}{n}log\;q(x_i\;|\;\theta) - \frac{1}{n}log\;p(x_i)$$

Ну а дальше всё это приблизительно равно

$$arg\min_\theta \frac{1}{n} \sum\limits_ilog\frac{p(x_i)}{q(x_i\;|\;\theta)} \approx arg\min_\theta \int log\frac{p(x)}{q(x\;|\;\theta)}p(x)dx$$

Последнее выражение и есть *дивергенция Кульбака-Лейблера* $DKL(p || q)$ показывающая степень близости распределений друг между другом. Это так называемое статистическое расстояние. Выглядит определение следующим образом
$$DKL(p || q) = \mathbb{E}(log(p(x)) - log(q(x\;|\;\theta))) = \int log\frac{p(x)}{q(x\;|\;\theta)}p(x)dx$$
 При этом мы обычно не знаем истинное распределение, поэтому можем орудовать только частью с $q(x)$ и для минимизации дивергенции Кульбака-Леблейра нам достаточно $$\mathbb{E}(log(q(x\;|\;\theta))) \rightarrow \max$$
 А это и есть ММП
 
- - -

*Энтропия* $$H(p) = \mathbb{E}_{x \sim p}(-ln(p(x)))$$
*Перекрестная энтропия (CrossEntropy)* $$H(p,q) = \mathbb{E}_{x \sim p}(- ln( q(x)))$$
*KL - дивергенция* -- та же самая дивергенция, однако ее еще можно записать как разницу между двумя энтропиями
$$KL = H(p,q) - H(p) = \mathbb{E}_{x \sim p}(ln\frac{p(x)}{q(x)})$$
*Взаимная информация*
$$I(x,y) = KL(p(x,y) || p(x)p(y)) = \int\int p(x,y)ln\frac{p(x,y)}{p(x)p(y)}dxdy =$$
$$= H(x) - H(x\;|\;y) = H(y) - H(y\;|\;x)$$

&nbsp;

### [[AIM ML#Content|Ковариация и корреляция]]
*ковариация* двух случайных величин
$$cov(X, Y) = \mathbb{E}((X - \mathbb{E}(X))(Y - \mathbb{E}(Y)))$$
То есть мы хотим понять, насколько синхронно $X, Y$ отклоняются от своего среднего (заметим, что дисперсия это ковариация с.в. с самой собой)

*корреляционный коэффициент Пирсона* -- нормированная ковариация
$$corr(X,Y) = \frac{cov(X, Y)}{\sqrt{var(X)var(Y)}} \in [-1;\;1]$$
Этот коэффициент опеределяет меру корреляции или по-другому меру *линейной зависимости*  между случаными величинами. Если величины пропорциональны, то коэффициент пирсона равен $\pm1$, независимые переменные некоррелированы

![[Pasted image 20220712151208.png]]
Видно, что коэффициент корреляции не говорит о независимости. Он говорит лишь о *линейной*  независимости

- - -

*корреляционный коэффициент Спирмена* опеределяет меру монотонной зависимости. Считается как коэффициен Пирсона между рангами (номерами элементов в выборке)
$$r({x_i}, {y_i}) = \frac{\sum(rank(x_i) - \frac{n+1}{2})(rank(y_i) - \frac{n+1}{2})}{\frac{1}{12}(m^3 - m)} = 1 - \frac{6}{m^3-m}\sum(rank(x_i) - rank(y_i))^2$$
Этот коэффициент помогает оценить и нелинейную зависимость

&nbsp;

### [[AIM ML#Content|Оценка плотности]]

Есть три способа оценить плотность

1. *Параметрический* -- его мы озвучивали выше. Знаем плотность с точностью до параметра, ищем этот параметр (например, *ММП*)
2. *Непараметрический* -- когда не знаем, к какому семейству отнести распределение, а значит не знаем вид формулы и необходимые параметры
3. *Смеси* представляем наше распределение как сумму распределений
$$p(x) = \sum\pi_ip_i(x\;|\;\theta_i)$$ $$\sum\pi_i = 1$$  Например, *EM-алгоритм*

&nbsp;

### [[AIM ML#Content|Непараметрические методы]]

*Гистограммный подход* -- просто разбиваем область значений с.в. на отрезочки и сколько куда попало такая и плотность. Из недостатков такая себе точность и куча пустых бинов при выбросах 

*Парзеновский подход* -- считаем что каждая точка создает вокруг себя некоторую плотность (например, треугольную, зависит от ядра). Площадь ядра должна быть равна 1. Нормируем то что получается, и оцениваем плотность так.

![[Pasted image 20220712154700.png]]

Вот более конкретный пример парзеновского подхода

![[Pasted image 20220712154747.png]]

При этом сама плотность считается по формуле $$\frac{1}{nh}\sum K(\frac{x-x_i}{h})$$

Примеры ядер:
- *Треугольное* $$K(x) = \max(\min(1+x, 1-x), 0)$$
- *Прямоугольное* $$K(x) = \frac{1}{2}\mathbb{I}(x \in [-1;\;1])$$
- *Гауссовское* $$K(x) = \frac{1}{\sqrt{2\pi}}exp(-\frac{x^Tx}{2})$$
- *Квартическое* $$K(x) = \frac{15}{16}(1-x^2)\mathbb{I}(x \in [-1;\;1])$$
- *Епанечникова* $$K(x) = \frac{3}{4}(1-x^2)\mathbb{I}(x \in [-1;\;1])$$

![[Pasted image 20220712155300.png]]

Есть теорема о сходимости парзеновского метода. То есть при определенных условиях эта оценка действительно сходится к истинному распределению

&nbsp;

### [[AIM ML#Content|Теория информации]]

Есть система $X$, которая принимает состояния $x_1,\dots,x_n$ с вероятностью $p_1, \dots, p_n$ (например, случайная величина)

Тогда *информационная энтропия* -- мера неопределенноси системы, записывается так $$H(X) = -\sum p_ilog(p_i)$$

Например, для монетки, падающей орлом с вероятностью $p$, энтропия равняется
$$H(x) = -plog(p) -(1-p)log(1-p)$$ Если мы нарисуем график, то увидим, что чем менее честная монета, тем меньше энтропия системы, а результат подбрасывания честной монеты -- 1 бит информации

&nbsp;

### [[AIM ML#Content|Проклятие размерности]]

Наша интуиция ломается при работе с многомерными измерениями. Примером такой поломки служит классический пример -- объем шара по отношению к объему куба стремится к нулю при увеличении размерности

А еще весь объем сосредеточен с краю шара
$$\frac{vol(r+\varepsilon)}{vol(r)} = (1 + \frac{\varepsilon}{r})^n \rightarrow \infty$$
То есть слой пыли на поверхности шара занимает почти всё его место

Поэтому следует ожидать, что соседи будут находится с краю, если они равномерно разбросаны. Это здорово, потому что все объекты лежат не в $n$-мерном пространстве, а на каком-то многообразии меньшей размерности (на поверхности шара, а не в самом шаре)

Также забавно, что объем шара не продолжает расти бесконечно, а делает это только до пятитимерного пространства, после чего начинает падать до нуля

- - -

Еще один пример проклятия размерности -- угол между векторами  $(1,0,0,\dots)$ и $(1,1,1,\dots)$ увеличивается до $\frac{\pi}{2}$

![[Pasted image 20220712162614.png]]

> [!danger]- Размер выборки
> Звучит очевидно, но чем больше размерность, тем экспоненциально больше объектов выборки нам нужно, чтобы нормально покрыть пространство и заметить какую-то тенденцию.
> 
> В одномерном случае нам достаточно 100 элементов, чтобы увидеть тенденцию, соответственно уже в трёхмерном случае эта цифра будет порядка 1000000

&nbsp;

### [[AIM ML#Content|Сингулярное разложение (SVD)]]

*SVD разложение* -- представление любой метрицы $m \times n$ ранга $k$ в виде
$$A_{m \times n} = U_{m \times k}\cdot \Lambda_{k \times k}\cdot V_{n \times k}^T$$
При этом матрицы $U, V$ ортогональные, а $\Lambda$ диагональна, причем на диагонали стоят сингулярные числа $\lambda_1, \lambda_2, \dots$ в порядке убывания

![[Pasted image 20220712171656.png]]

Проведем преобразвоваение для $A^TA$
$$A^TA = (U\Lambda V^T)^TU\Lambda V^T = V\Lambda^TU^TU\Lambda V^T = V\Lambda^2V^T$$

Отсюда получаем полезный вывод
$$A^TAV = V\Lambda^2$$
Значит матрица $V$ состоит из собственных векторов матрицы $A^TA$, и им соответствуют собственные значения $\lambda_1^2, \lambda_2^2, \dots$. Аналогично матрица $U$ состоит из собственных векторов матрицы $AA^T$ с теми же собственными значениями

&nbsp;

### [[AIM ML#Content|Усеченное (truncated) SVD]]

Что, если занулить сколько-то последних сингулярных чисел матрицы $\Lambda$? Тогда при перемножении мы потеряем хвост данных и уменьшим размерность наших матриц. Пусть мы оставили только первые $r$ чисел -- $\lambda_1, \lambda_2, \dots, \lambda_r, 0, 0, \dots$

![[Pasted image 20220712171720.png]]

Есть удивительное свойство такого усечения: среди всех матриц ранга $r$ наша матрица будет обладать самой маленькой *метрикой Фробениуса* -- суммой квадратов разностей элементов матриц. То есть в каком-то смысле *усеченное SVD* это самая похожая из маленьких матриц на нашу большую матрицу.

Этим можно нагло пользоваться при сжимании изображений. Попробуем сжать котёнка ранга 300 и заметим, что даже при $r = 50$ имеем нормальное качество, а пикселей в картинке стало сильно меньше ($300\times451 \rightarrow 300 \times 50 + 50 + 50 \times 451$)

![[Pasted image 20220712172345.png]]
>[!quote]- Код
> ```Python
> 
> from PIL import Image
> from numpy.linalg import svd
> import matplotlib.pyplot as plt
> from matplotlib.pyplot import imshow
> import numpy as np 
> 
> image = Image.open("Masha.jpeg").convert('L')
> U, L, V = svd(image)
> 
> k = 10
> matrix = Image.fromarray(U[:,:k].dot(np.diag(L[:k]).dot(V[:k,:])))
> matrix.convert('RGB').save(f'{k}.png')
> ```

&nbsp;

### [[AIM ML#Content|Матричное дифференцирование]]

```Python

pass
```
- - -
&nbsp;

## [[AIM ML#Content|Метрические методы]]

> [! quote]- Код-шпаргалка
> ```Python
> 
> sklearn.neighbors.NearestNeighbors
> 
> n_neighbors # число соседей (5)
> radius # ограничение пространства (1.0)
> algorithm # алгоритм для определения БС (auto, ball_tree, kd_tree, brute)
> leaf_size # параметр для BallTree / KDTree
> metric # метрика (функция или строка: ), 	см. `scipy.spatial.distance scikit-learn:[cityblock, cosine, euclidean, l1, l2, manhattan] scipy.spatial.distance: [braycurtis, canberra, chebyshev, correlation, dice, hamming, jaccard, kulsinski, mahalanobis, minkowski, rogerstanimoto, russellrao, seuclidean, sokalmichener, sokalsneath, sqeuclidean, yule]`
> p # параметр для minkowski (2)
> metric_params # дополнительные параметры для метрики
> n_jobs – ...
> ```
> 
> ```Python
> 
> KNeighborsClassifier/ KNeighborsRegressor
> 
> from sklearn.neighbors import KNeighborsClassifier
> knn = KNeighborsClassifier(n_neighbors=1)
> knn.fit(X_train, y_train)
> 
> n_neighbors # число соседей
> weights # веса («uniform», «distance», функция)
> algorithm # алгоритм для эффективного нахождения соседей («auto», «ball_tree», «kd_tree», «brute»)
> leaf_size # для BallTree / KDTree
> p # параметр для метрики Минковского
> metric # метрика («minkowski»)
> metric_params # параметры для метрики
> n_jobs # число процессов для нахождения соседей
> 
> from sklearn.neighbors import KNeighborsRegressor
> weights # весовая схема для объектов (uniform, distance, функция)
> ```
> 
> ```Python
> 
> from sklearn.neighbors.nearest_centroid import NearestCentroid
> # ближайший центроид
> 
> sklearn.neighbors.kneighbors_graph
> # граф соседей
> 
> sklearn.neighbors.RadiusNeighborsClassifier
> sklearn.neighbors.RadiusNeighborsRegressor
> # алгоритмы с соседством по радиусу
> 
> sklearn.neighbors.NeighborhoodComponentsAnalysis
> # обучение метрики
> 
> sklearn.neighbors.KernelDensity
> # KDE-оценка плотности
> ```

*Метрические алгоритмы* (*distant based*) – алгоритмы, анализирующие расстояния между объектами. Например алгоритм ближайших центроид, KNN

> [! danger]- Масштаб
> Очень важно помнить, что метрические методы не работают для величин разного масштаба (они просто будут затачиваться на признак большего масштаба).
> 
> И даже нормировка не поможет.
> 
> Поэтому использовать метрические признаки надо **только** для однородных признаков, не надо применять их там, где есть и температура, и давление, и рост

&nbsp;
### [[AIM ML#Content|Метрики]]

*метрика*, это фукнция $\rho(x,y): X\times X \rightarrow \mathbb{R}$, для которой верно
1. $\rho(x,y) \geq 0$
2. $\rho(x,y) = 0 \Leftrightarrow x = y$
3. $\rho(x,y) = \rho(y,x)$
4. $\rho(x,y) + \rho(y,z)  = \rho(x,z)$

- - -

**Примеры метрик**
*$L_p$ Минковского* (общий случай)
$$(\sum|x_i-y_i|^p)^{\frac{1}{p}}$$
*$L_2$ Евклидова*
$$\sqrt{\sum(x_i - y_i)^2}$$
*$L_{\infty}$ Чебышева*
$$(\sum|x_i-y_i|^{\infty})^{\frac{1}{\infty}} \sim \max|x_i - y_i|$$
*$L_{1}$ Манхеттенская*
$$\sum|x_i - y_i|$$
*$D_M$ Махалонобиса* (евклидово расстояние после небольшой деформации)
Сама деформация выглядит так
$$x \rightarrow \varphi(x) = \Sigma^{-\frac{1}{2}}(x - \mu)$$
Оно стандартизует нормальные данные $$norm(\mu, \Sigma) \rightarrow norm(0,1)$$
*$D_C$ Канберра*
$$\frac{1}{n}\sum\frac{|x_i - y_i|}{|x_i| + |y_i|}$$
*$D_H$ Хемминга*
$$\sum\mathbb{I}(x_i \neq y_i)$$
*Косинусная мера сходства* (**Не расстояние, но в том же KNN это и не требуется**). По сути угол между векторами
$$\cos(x,y) = \frac{x^Ty}{\| x \|\cdot\|y\|}$$
*Расстояние Джаккара* (на множествах)
$$1 - \frac{|X \cap Y|}{|X \cup Y|}$$
*Расстояние Левенштейна* (на строках) –  минимальное количество вставок/удалений/замен букв, необходимое чтобы превратить одну строку в другую
- - -

&nbsp;
### [[AIM ML#Content|Алгоритм ближайших центроид]]

Алгоритм решения задачи классификации. Для каждого класса вычислям центр масс, и присваем новому объекту метку ближайшей центроиды

Формально центроиду класса $k$ можно описать так (просто среднее арифметическое) $$c_k = \frac{\sum\limits_{y_i = k} x_i}{|{i:\;y_i = k}|}$$

А решением задачи классификации для новоприбывшего объекта $x$ так
$$arg \min\limits_i \rho (x, e_i)$$

 Примитивнейший и тупейший алгоритм (задачу двух классов он делит только гиперплоскостью), зато хранить надо только центроиды и их можно адаптивно пересчитывать
 
 >[!quote]- Код
> ```Python
> 
> from sklearn.neighbors.nearest_centroid import NearestCentroid
> 
> model = NearestCentroid()
> model.fit(X, y)
> a = model.predict(X2)
> # есть параметр metric=’euclidean’
> ```

&nbsp;
### [[AIM ML#Content|KNN]]

Идея тоже просто донельзя – упорядочиваем объекты $x_1, x_2, x_3, \dots$по расстоянию до новоприбывшего $x$ . Будем считать что первый объект ближайший. Тогда просто смотрим на метки $k$ ближайших соседей и присваиваем самую частую метку

Параметр $k$ можно выбрать на скользящем контроле. При $k=1$ мы смотрим на одного ближайшего соседа и копируем его метку. При $k \rightarrow \infty$ мы даем метку того класса, которого в выборке больше

![[Pasted image 20220713025149.png]]

> [!quote]- Код
> ```Python
> 
> from sklearn.neighbors import KNeighborsClassifier
> model = KNeighborsClassifier(n_neighbors=5)
> model.fit(X, y)
> a = model.predict(X2)
> p = model.predict_proba(X2)[:, 1]
> ```

Проблема метода в том, что он не учитывает близость соседа (если рядом с нами 2 синие, а где-то далеко 3 красные, то при $k=5$ мы дадим красную метку)

Можно дать каждому объекту вес, с которым он голосует, тогда ясно что первый объект имеет самый большой вес и функция должна постепенно убывать. Эту функцию можно задать какой угодно ядерной убывающей функцией (правда это так себе работает на практике)

Вот пример разных весовых схем

![[Pasted image 20220713030938.png]]

![[Pasted image 20220713031103.png]]

&nbsp;
### [[AIM ML#Content|Сглаживание Надарая-Ватсона]]

 Еще называемое *регрессией Надарая-Ватсона*. В своей идее это сопоставление каждой точке взвешенного среднего значений соседей (как правило вес определяется расстоянием до точки)
 $$a(x) = \frac{w_1(x)y_1 + \dots + w_n(x)y_n}{w_1 + \dots + w_n}$$
 Алгоритм абсолютно не годится для экстраполяции (потому что все что находится за краями выборки очень быстро превращается в константу), зато выполняет приятное сглаживание, зависящее от весов, которые можно настроить ядерными функциями. Например, для некоторого ядра вида $K(\frac{\rho(x, x_i)}{h})$ сглаживание при изменении $h$ существенно меняется
 
 ![[Pasted image 20220714003006.png]]

&nbsp;
### [[AIM ML#Content|Обучение метрик]]

Если метрика параметризуема, можно подбирать ее параметры. В примере ниже мы подбираем параметры метрики Махаланобиса для матрицы вида $\Sigma = [[\alpha, \beta],[\beta, 1]]$

> [!quote]- Код
> ```Python
> 
> from sklearn.neighbors import KNeighborsClassifier
> from sklearn.metrics import accuracy_score
> 
> Q = np.zeros([101, 101])
> LS1, LS2 = np.linspace(-0.9, 0.9, 101), np.linspace(-1, 1, 101)
> for i, b in enumerate(LS1):
>     for j, a in enumerate(LS2):
>         if a - b*b < 0.00001:
>             q = np.nan
>         else:
>             A = np.linalg.inv(np.array([[a, b], [b, 1]]))
>             model = KNeighborsClassifier(n_neighbors=5, metric='mahalanobis',
>             metric_params={'V': A})
>             model.fit(X, y)
>             a = model.predict(X2)
>             q = accuracy_score(y2, a)
>         Q[i, j] = q
> ```

Также в `sklearn` уже лежит преобразователь метрики в более удобную лдя кластеризации (элемента одного класса становятся близкими, а разного класса далёкими)

> [!quote]- Код
> ```Python
> 
> from sklearn.neighbors import NeighborhoodComponentsAnalysis
> 
> nca = NeighborhoodComponentsAnalysis(max_iter=30, random_state=10)
> nca = nca.fit(X, y)
> X_embedded = nca.transform(X)
> ```

Пример до и после

![[Pasted image 20220713141256.png]]

- - -

&nbsp;
## [[AIM ML#Content|Приближенные метрические методы]]

Большой минус метрических алгоритмов в том, что они должны просматривать всю выборку, что недопустмо по памяти при большом количестве объектов. Компромиссом может быть приближенный алгоритм, который даёт неточный, но зато очень быстрый ответ

&nbsp;
### [[AIM ML#Content|kD-дерево]]
Идея в том, чтобы искать не прямо ближайшего соседа, а каджый раз делать определенный срез по каждому параметру (тем самым разбивая прострнаство объектов на кластеры). Скажем, у нас есть признаки $f_1, f_2, \dots$. Будем класть объекты c $f_1 \geq \alpha$ в первую корзину, а остальные во вторую. Далее для объектов из первой корзины будем класть те, у которых $f_2 \geq \beta$ в одну корзину, а остальных в другую и так далее.

Тем самым получается кластеризация, задающаяся древесной структурой, у которой высота равна количеству признаков, и спустившись по этому дереву мы точно определим, в каком кластере лежит объект. Тогда и искать соседей будем только среди объектов того же кластера (и, пожалуй, соседних).

Теперь скорость поиска кластера равна $O(|\{f_i\}|)$,  а поиск в рамках кластера относительно полного перебора уменьшился в среднем во столько раз, сколько у нас кластеров, а это $2^{|\{f_i\}|}$, если мы разбиваем прям по всем признакам, однако это может быть излишне.

![[Pasted image 20220714025751.png]]


Вообще, как правильно разбивать дерево? Как вариант, находить признак с максимальной дисперсией и разбивать его по медиане (наибольший вклад в соседство вносит именно этот признак)

&nbsp;
### [[AIM ML#Content|Random Trees]]

Также можно разбивать границу основываясь на значении не одного признака, а линейной комбинации признаков. Тогда границы кластеров будут косыми и иметь более общий вид. Ниже сравнение kD-деревьев со случайными деревьями

![[Pasted image 20220714013543.png]]

&nbsp;
### [[AIM ML#Content|LSH]]

Идея уместится в предложение: хешируем объекты так, чтобы близкие объекты имели похожий хеш, а далее просто считаем хеш объекта и ищем соседей среди похожих хешов.

&nbsp;
### [[AIM ML#Content|Boundary trees]]

*Граничные деревья* – очень прикольный (и сравнительно новый) алгоритм. В нем мы сначала по выборке строим дерево, а затем делаем следующее – для нового объекта сравниваем его с корнем и со всеми детьми корня. Если корень ближайший, то даём объекту метку корня, а если ближайший сын, то спускаемся к нему и повторяем процедуру.

Звучит логично, но как нам построить дерево так, чтобы качество при таком обходе было максимальным? Очень просто: возьмем случаную точку за корень. Далее перебираем все точки выборки и выполняем для них алгоритм. Если точка была предсказана верно, то забываем про нее. Если же угадать ее метку не получилось, добавляем ее в качестве ребенка той вершины дерева, которая оказалось ближайшей

Поэтапное построение дерева и отображение предсказательной модели:
![[Pasted image 20220714014903.png]]

Заметим, что если у нас есть область точек с меткой $X$, то мы не будем добавлять в дерево точки из центра этой области – их алгоритм и так предсказывает верно. Поэтому дерево естественным образом вырастает вокруг границ классов, отсюда оно и граничное

*Вообще, различие точек по границе это довольно частая идея алгоритмов машинного обучения, просто зачастую спрятнная под капот*

Например, вот как выглядит готовое дерево для нашей выборки
![[Pasted image 20220714015143.png]]

Однако и это может показаться затратным и сложным, поэтому существует дополнительная мера оптимизации – строить несколько деревьев с разными корнями по подвыборкам (соответственно, деревья будут меньшего размера), и по итогу устраивать между ними голосование. Способов можно придумать сколько угодно

![[Pasted image 20220714015256.png]]

&nbsp;
### [[AIM ML#Content|Квантование]]

*Квантование* – это опять-таки идея разбиения выборки на кластеры и удобного их обозначения. На этот раз мы каждый вектор объекта (описываемый большим количеством признаков) разбиваем на несколько небольших подвекторов, создавая несколько подпространств признаков. Далее для каждого такого подпространства применяем кластеризацию так, чтобы всего было, скажем, 256 кластеров. Тогда то, как себя ведут признаки объекта, принадлежашие подпространству $X$, можно будет описать номером кластера, умещающимся в 1 байт.

Скажем, если у нас есть 50000 объектов, состоящих из 1024 признаков, то мы можем разбить признаки на 8 подпространств с 1 по 128, с 128 по 256 и так далее. Тогда описаное выше разбиение даст нам 8 однобайтных чисел, описывающих объект (изначально это было 1024 четырехбайтных числа). **Очень** весомое уменьшие  

Как правило для каждого подпространства  и для каждого кластера в нем находится центроид, и объекту присваевается не просто номер кластера, а номер центроида, по которому можно определить его местоположение (в данном примере будет 8 * 256 центроид). Это нужно, чтобы при добавлении нового объекта и разбиении на подпространства быстро посчитать ближайший кластер, сравнив расстояния до каждой из центроид

![[Pasted image 20220714025706.png]]

Проблема квантования в том, что оно делит области вслепую, зачастую оставляя либо почти пустые области, либо полностью пустые (это видно даже на картинке). Как с этим быть? Например, можно изначально провести какую-то кластеризацию объектов, а затем применять квантование на уже полученные кластеры, а еще лучше будет если мы для каждого кластера применим *метод главных компонент*, который поменяет систему координат и сдвинет подпространства лучшим образом

![[Pasted image 20220714023708.png]]

В общем и целом вот пример различных способов нахождения центроид, наглядно и понятно

![[Pasted image 20220714025628.png]]

&nbsp;
### [[AIM ML#Content|Navigable Small World (NSW)]]

Отечественная разработка, которая ставит скорость превыше всего. Идея тоже абсолютна простая – по выборке строится подграф, в котором у каждой точки есть $k$ близких соседей (как вариант, граф Делоне, который триангулирует пространство) и вдобавок к этому еще $t$ *длинных* связей до каких-то случайных удаленных точек графа.

Теперь для каждой новой точки мы запускаем такой же поиск по графу, что и в граничных деревьях, но от случайной точки. При этом благодаря дальним связям алгоритм быстро прилетает куда надо даже в случае неудачного выбора начала поиска.

Дополнительная оптимизация по памяти – сделать несколько *слоёв* графа, в каждом из которых он будет все менее и менее заполненным. Начинать искать точку в самом примерном графе, потом спускаться ниже и начинать поиск уже от полученной выше точки. Тогда требуется больше памяти, но первые шаги проходят более грубыми мазками, отчего быстрее

![[Pasted image 20220714025523.png]]

Алгоритм простой и эффективный по памяти и времени. Но не очень точный

> [!danger]- Подводный камень
> Реализованные в библиотеках метрические алгоритмы могут работать даже хуже, чем полный перебор (например, алгоритм может быть оптимизирован для какой-то одной метрики, а используем мы другую). Нужно быть аккуратным и перед использованием метода проводить тесты
- - -
&nbsp;

## [[AIM ML#Content|Контроль качества и выбор модели]]
Сразу же вспомним, что *ошибка на обучении* и *ошибка на контроле* зачастую очень различаются. Например, 1 ближайший сосед даёт 100% качество на обучении, но хорошим от этого алгоритм не становится. Будем называть *моделью* параметрическое семейство алгоритмов. А *алгоритм* – это какая-то реализация модели. Модель зависит от параметров и гиперпараметров 

Как выбирать алгоритм? Очевидно, надо смотреть на качество. Для этого датасет разбивается на 3 части

*Обучающая выборка* – тут мы тренируем алгоритм, подбирая его параметры

*Валидационная выборка* – тут мы выбираем модель и подбираем гиперпараметры

*Тестовая выборка* – сюда мы приходим в самом конце и узнаем получаемое качество

Общая логика следующая: подобрали гиперпараметры, обучились, проверили качество, подобрали гиперпараметры, обучились, проверили качество ...

&nbsp;
### [[AIM ML#Content|Train test split]]

Самый простой способ, как это сделать – разбить датасет на две части в какой-то пропорции (при этом потом можно поделить `train` на `train` и `val`). Делается это функцией `train_test_split`. При этом она может делать разбиение в том же порядке, что объекты лежат изначально, но гораздо лучше делать это случайным образом

Обычно тест – это 20% выборки, но все зависит от ситуации. Чем больше трейн, тем больше алгоритм похож на тот, который будет по итогу, когда избранную модель мы обучим на полном датасете, состоящим и из трейна, и из теста. А чем больше тест, тем надежнее мы отсеиваем алгоритмы

>[!tip]- 3 золотых правила разбиения выборки
>1. *Разбиение должно моделировать реальную работу алгоритма*
>
> 2. *Тест должен быть случайным или специально подготовленным*. Нужно, чтобы испытание проводилось в тех же условиях, что будут при работе с реальными данными
>
>3. *Нельзя использовать информацию об объектах из теста при обучении*. Даже неявно. Даже нечаянно. Если в тесте у нас фигурирует какое-то действие пользователя $X$, надо, чтобы этого пользователя не было в трейне

>[!quote]- Код
>```Python
>
>from sklearn.model_selection import train_test_split 
>X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 41)
>```

- - -

В зависимости от подходов к разбиению данных, существует 4 термина

*in-sample* – это когда мы записи, представляющие один и тот же объект, пихаем и в обучение, и в контроль

*out-of-sample* соответственно когда мы все записи, соответствующие пользователю А запихиваем либо в контроль, либо в тест

*in-time* – это когда мы кидаем в тест и контроль данные как из прошлого, так и из будущего

*out-of-time* это когда тест делается на данных из прошлого, а контроль на данных из будущего

- - -
&nbsp;
### [[AIM ML#Content|Дубликаты]]

Большой проблемой представления данных могут служить дубликаты. Плохо это тем, что если об этом не подумать, один и тот же объект может попасть и в трейн, и в тест, но это не прибавит нашей модели качества обучения. Наоборот, неоправданно завышается ожидание того, что дублированные данные появятся в будущем (если у нас лежит 10 одинаковых записей, соотстветсующих человеку без ноги, то алгоритм будет в 10 раз переоценивать шанс того, что следующий человек будет безногим)

Также стоит понимать, что существуют почти дубликаты. Например одинаковые записи, отличающиеся только `id` . Или замеры в соседние моменты времени, по факту говорящие об одном и  том же, но отличающиеся на сотые или тысячные. О таких дубликатах почти не думают и не обрабатывают, хотя следовало бы

- - -

>[!danger]- Подводный камень #1
>В реальности данные ведут себя коварно. Например, из-за *нестационарности* со временем меняются некоторые параметры (курс валюты после кризиса, посещаемость кинотеатров после ковида). Также существует *смещение распределений* (некоторые товары начинают скупать после того, как неожиданно выстрелил фильм по их мотивам, популярными становятся посты, следующие быстро меняющимся трендам)
>
>Поэтому алгоритмы неизбежно устаревают, будучи заточенными под уже неактуальные данные (может быть даже неявно) и это нужно держать в голове.

>[!danger]- Подводный камень #2
>**НИКОГДА** нельзя подглядывать в контроль. Это очень легко сделать, если начать как-то фильтровать данные и делать первые шаги перед разбиением на обучнеие и тест. Простое правило: сначала разбили, потом делаем всё остальное

&nbsp;
### [[AIM ML#Content|Random subsampling CV]]

`train_test_split` хорош, но его минус в том, что качество алгоритма зависит от выборки, выбор которой может порой существенно изменять алгоритм. Решением проблемы занимается `shuffle_split`.

![[Pasted image 20220813221417.png]]
>[!quote]- Код
>```Python
>
>sklearn.model_selection.ShuffleSplit(n_splits = 4, test_size = 0.3, train_size = None)
>```

&nbsp;
Если есть несколько групп объектов, и хочется оставлять все объекты группы целиком либо в тесте, либо в контроле, нужно воспользоваться рандомизацией без разделения по группам. За это отвечает метод `GroupShuffleSplit`

![[Pasted image 20220813221357.png]]
>[!quote]- Код
>```Python
>
>sklearn.model_selection.GroupShuffleSplit(n_splits = 4, test_size = 0.3, train_size = None)
>
># а потом используем так
>for t, (i_train, i_test) in enumerate(cv.split(x, groups = g)):
>```

&nbsp;
Если не хочется так топорно делить на группы, а хочется просто сохранять пропорцию элементов из каждой группы (например, чтобы в тест уходила треть элементов каждой группы), используем `StratifiedShuffleSplit`

![[Pasted image 20220813222517.png]]

>[!quote]- Код
>```Python
>
>sklearn.model_selection.StratifiedShuffleSplit(n_splits = 4, test_size = 0.3, train_size = None)
>
># а потом используем так
>for t, (i_train, i_test) in enumerate(cv.split(x, groups = g)):
>```

>[!tip]-  Лайфхак
>Если в выборке есть несколько аномально больших значений, и не хочется чтобы все они попадали в тест или обучение, проще всего задать некоторый порог и создать искусственные классы – класс аномально больших элементов, больших порога, и класс всех остальных элементов, после чего запустить `StratifiedShuffleSplit`. Теперь аномально большие объекты разделены в том пропорции, которую мы захотим

&nbsp;
### [[AIM ML#Content|Bootstrap]]

Легальный статистический чит, расширяющий выборку и искусственно делающий так, что для обучения будет использоваться столько же объектов, сколько их всего. Фокус в том, чтобы из, скажем, тысячи объектов случайно выбирать для обучения тысячу объектов *с повторениями*. Из-за повторений останутся невыбранные объекты, они пойдут в тест.

Из-за того, что мы обучаем алгоритм на выборке того же объема, что и все данные, его не сильно штормит после обучения на всём датасете (к тому же выборка будет иметь похожее на исходное распределение параметров). Но при этом мы используем не все объекты и добавляем искусственные дубликаты, которые искажают реальное положение вещей 

>[!quote]- Сколько элементов попадает в контроль?
>Чтобы элемент попал в контроль, мы не должны ни разу выбрать его в обучение. Для $n$ объектов это
>$$(1 - \frac{1}{n})^n \approx e^{-1} \approx 37\%$$
>Может быть многовато, но в целом вполне адекватная цифра (особенно при учёте того, что размер обучения от этого не меняется)

&nbsp;
### [[AIM ML#Content|K-fold CV]]

Некоторые понимают под кросс-валидацией именно этот алгоритм (но вообще говоря это любая проверка качества через разделение на `train` и `test`). Разбиваем выборку на несколько равных блоков (фолдов) и запускаем множество обучений, где в роли теста выступает каждый раз новый блок. В итоге усредняем возможные выбросы и неровности, считая за качество алгоритма, скажем, усреднение качеств (или можно учитывать дисперсию и доверять каждому результату по-разному)
>[!quote]- Код
>```Python
>
>sklearn.model_selection.KFold(n_splits = 'warn', shuffle = False, random_state = None)
>```

&nbsp;
без рандомизации
![[Pasted image 20220815015805.png]]
&nbsp;
с рандомизацией
![[Pasted image 20220815015841.png]]
&nbsp;
Для удобства, существует k-fold контроль стратифицированный (сохраняющий пропорции между группами) и групповой, все по аналогии с обычным разбиением

&nbsp;
### [[AIM ML#Content|LOOCV]]

*Leave one out CV* это предельный случай k-fold, в котором мы запускаем n обучений, где в тест идет по очереди только один из элементов выборки. То есть как будто количество фолдов равно размеру выборки

![[Pasted image 20220815173205.png]]


В теории хороший и качественный алгоритм, но невероятно долго считается из-за огромного количества экспериментов

По аналогии, есть групповое обобщение *leave one group out*. Важно при этом не слепо усреднять качества моделей, а учитывать мощность группы (если мы положили в тест большую группу, то этот эксперимент точнее и больше говорит нам о качестве алгоритма). Для этого можно просто использовать нормировку с весом  пропорциональным количеству элементов в группе

&nbsp;
### [[AIM ML#Content|Out of time CV]]

Для прогнозирования существует кросс-валидация по времени, когда мы делаем много экспериментов в порядке хронологического поступления информации. Каждый новый эксперимент, таким образом, как бы на шаг вперед заглядывает в будущее

![[Pasted image 20220815173235.png]]

Минус в том, что не получается сделать много экспериментов, так как для экспериментов с ранним временем предыстория, то есть количество доступных для обучения объектов слишком мало и качество алгоритма приходит в негодность. При этом модель понятна и имеет конкретное выражение в реальном мире

&nbsp;

>[!done]- cross val score
> Берет алгоритм, выборку, разбиение и выводит список значений на всех "засунутых" в нее разбиениях. Удобна, потому что не надо писать цикл где мы проходимся по каждому разбиению
> ```Python
> 
> from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit
from sklearn.linear_model import LogisticRegression
>
logreg = LogisticRegression()
cv = ShuffleSplit(n_splits=3, test_size=0.1,
train_size=None, random_state=None)
cross_val_score(logreg, X, y, cv=cv)
> ```

>[!done]- cross val predict
>Полезная штука, выдающая в ответ сборник из нескольких ответов. Ответ на каждый блок объектов получен экспериментом, в котором именно этот блок был в качестве теста. Таким образом, для $n$ блоков проводится $n$ экспериментов и объединяется $n$ предсказанных результатов

&nbsp;
### [[AIM ML#Content|Перебор параметров]]

Перебрать параметры можно, во-первых простенькой сеткой. То есть для каждого параметра модели мы составляем множество перебираемых значений и проходимся по получающейся сетке значений

>[!quote]- Код
>```Python
>
from sklearn.model_selection import GridSearchCV
parameters = {'metric':('euclidean', 'manhattan', 'chebyshev'),
> 'n_neighbors':[1, 3, 5, 7, 9, 11] , scoring='roc_auc'}
> 
> clf = GridSearchCV(estimator, parameters, cv=5)
> clf.fit(X, y)
> clf.cv_results_['mean_test_score']
> ```

Однако минус в том, что при таком подходе даже не самые важные параметры перебираются и порождают отдельные эксперименты. Поэтому предпочтительнее брать случайный перебор – так в каждом эксперименте все параметры будут принимать новые значения

![[Pasted image 20220816011502.png]]

&nbsp;
### [[AIM ML#Content|Байесовская оптимизация]]

Идея в том, чтобы постепенно замерять качество алгоритма на разных параметрах, и каждый раз все точнее предполагать функцию ошибки. При этом рядом с посчитанными значениями ожидается видеть похожие значения, а далеко от посчитанных значений мы по сути ничего не знаем. Получается гибкий коридор значений, и на каждом шаге имеет смысл считать качество на тех участках, где меньше всего ошибка и больше всего дисперсия (то есть мы смотрим ближе к оптимальному значению и устраняем больше неопределенности)

На картинке красная есть искомая функция ошибки, которую мы приближаем
![[Pasted image 20220816012121.png]]

- - -
&nbsp;
## [[AIM ML#Content|Оптимизация]]

*метод нулевого порядка* – использует только значение функции 
*метод первого порядка* – использует значение функции и первой производной
*метод второго порядка* – использует значение функции и производные до второй
...

&nbsp;

### [[AIM ML#Content|Покоординатный спуск (0 порядок)]]
Перебираем каждый параметр и оптимизируем (любым способом) по нему.
Медленно сходится, хоть каждая итерация и занимает мало времени, зато можно использовать тогда, когда производную нельзя вычислить

![[Pasted image 20220816013524.png]]

&nbsp;

### [[AIM ML#Content|Градиентный спуск (1 порядок)]]
Идем в сторону антиградиента. Очень важно правильно выбрать `learning rate`. Примерно так и работают современные алгоритмы оптимизации для многих моделей, в `scikit learn` при вызове `fit` происходит что-то такое для многих моделей.

В общем виде формула выглядит так
$$w^{(t + 1)} = w^{(t)} - \eta \nabla L(w^{(t)})$$

Мы знаем что шаг обучения может оказаться как слишком большим, так и слишком маленьким. И особенно злую шутку это играет при разных масштабах признаков: может оказаться, что для одних признаков шаг большой, а для других маленький. Именно поэтому надо нормировать признаки 
![[Pasted image 20220816015124.png]]

>[!quote]- Теорема о сходимости градиентного спуска
>$L:\;\mathbb{R}^d \rightarrow \mathbb{R}$ выпукла и дифференцируема,
>$\nabla L$ Липшецева с константной $\lambda > 0$
>$\|\nabla L(z_1) - \nabla L(z_2) \| \leq \lambda \|z_1 - z_2 \|$ для любых $z_1, z_2$
>
>Тогда градиентный спуск с фиксированным шагом $\eta \leq \frac{1}{\alpha}$ сходится и $$L(z^{(t)}) - L(z^{*}) \leq \frac{\|z^{(0)} - z^* \|}{2\eta t}$$
>

>[!warning]- Критерии останова
>1. Слабо меняется значение функции (идем по плато)
>2. Слабо меняется аргумент (топчемся на месте)
>3. Слишком много итераций (идём слишком долго)

&nbsp;

Есть алгоритмы и с переменным шагом в зависимости от какой-то функции по времени. Часто для этого функцию подгоняют под *условия Роббинса-Монро*, а именно такую функцию, что ряд из этой функции расходится на бесконечности, но ряд квадратов этой функции сходится. Классическим примером функции, подходящей под это условие есть $\frac{1}{n}$

Еще мы можем по-умному подбирать темп обучения. Например, метод *наискорейшего градиентного спуска* на каждом шаге перебирает сколько-то значений `learning rate` и идёт на такой шаг, который приводит в наименьшую точку

Плюс алгоритма в том что он простой, для выпуклых функций сходится к хотя бы локальному минимуму и может использоваться в онлайн режиме

&nbsp;

### [[AIM ML#Content|Стохастический градиентный спуск / SGD (1 порядок)]]

Обычно используется, когда мы минимизируем не просто функцию, а сумму нескольких функций. Тогда  $L(x)$ представима как $\sum L_i(x)$. Если сумма огромна, то считать полный градиент будет очень долго. Но так как мы знаем составляющие, из которых состоит наша функция, можно выбрать случайную функцию $L_i$ и сделать шаг в сторону только ее антиградиента

$$w^{(t+1)} = w^{(t)} - \eta L_i(w^{(t)})$$

Очевидные плюсы этого алгоритма – он быстрее обычного GD и можно учиться в онлайн-режиме. То есть если мы захотим добавить в функцию новое слагаемое, можно будет гибко пересчитать градиент

Здесь мы каждый раз выбираем одно из слагаемых и идем к минимуму в его направлении
![[Pasted image 20220816145714.png]]


>[!tip]- Стохастический средний градиент / SAG (1 порядок)
> 
> Считаем градиент изначальной суммы $\sum\limits L_i$, и на каждом шаге уточняем (пересчитываем) какое-то ее слагаемое (остальные слагаемые идут туда, куда шли до этого)
> 
> $$w^{(t + 1)} = w^{(t)} - \frac{\eta}{n}\sum \nabla L_i$$

 &nbsp;

### [[AIM ML#Content|Градиентный спуск и ML]]

Допустим, мы хотим минимизировать *эмпирический риск* (пока без регуляризатора)
$$\frac{1}{2}\sum (a(x | w) - y_i)^2 \rightarrow \min$$
($\frac{1}{2}$  нужна чтобы при дифференцировании сократились двойки, не более)

Тогда градиентный классический градиентный спуск выглядит следующим образом
$$w^{(t + 1)} = w^{(t)} - \eta \sum (a(x\;|\;w^{(t)}) - y_i)\frac{da(x | w^{(t)})}{dw}$$
Пересчитывать каждый раз такую вот сумму это жестко, долго и неразумно
- - -
Если мы применяем линейную модель, то есть $a(x\;|\;w) = w^Tx$, то формула для градиентного спуска становится сравнительно простой

$$w^{(t + 1)} = w^{(t)} - \eta\sum (a(x\;|\;w^{(t)}) - y_i)x_i$$
И вполне логичной – для каждого объекта смотрим, на сколько мы ошиблись, и делаем соответствующую пропорциональную поправку

- - -
Если применим SGD, будет еще проще, потому что пропадет сумма и смотреть надо будет только за одним слагаемым

$$w^{(t + 1)} = w^{(t)} - \eta_i(a(x_i\;|\;w^{(t)}) - y_i)x_i$$

> [!danger]- Проблема седловых точек
> Мы идем к месту обнуления градиента, при этом помимо желаемого глобального минимума мы так же можем прийти в седловую точку. Причем в многомерном случае это гораздо более вероятно, так как для минимума нам нужно, чтобы все вторые частные производные (матрица Гессе) были положительны, а для седловой точки нам нужен гораздо более вероятный общий случай
> 
> Таким образом, вероятность прийти в желаемую точку с увеличением размерности стремится к нулю
> 
> По возможности чтобы это избежать можно смотреть на матрицу Гессе после того, как получен результат, и запускать алгоритм заново из близкой точки

&nbsp;

### [[AIM ML#Content|Метод Ньютона (2 порядок)]]
Градиентный спуск, используя только первые производные, локально приближает алгоритм линейно, то есть гиперплоскостью. Метод Ньютона, как алгоритм высшего порядка,  приближает рядом Тейлора до второй степени. После этого мы спускаемся в вершину параболы и, если функция похожа на квадратичную, довольно точно предсказываем минимум

![[Pasted image 20220817010842.png]]

Общая формула имеет вид
$$w^{(t+1)} = w^{(t)} - H^{-1}_{(t)}g^{(t)}$$
Здесь нам нужно знать *матрицу Гессе* $H$,  то есть матрицу вторых производных, и обращать ее. Это долго, но зато теперь нам вообще не нужен `learning rate` – мы просто спускаемся в вершину параболы.
 
 >[!info]- Связь градиентного спуска и метода Ньютона
 >Градиентный спуск можно представить как вырожденный случай метода Ньютона, где вместо матрицы Гессе берется единичная матрица (в случае единичного шага обучения), которая как бы показывает наше предположение о линейной природе функции
 
 Если матрица необратима, то прибавив к матрице $\alpha I$ можно заставить ее обращаться. Однако, как писалось выше, это очень долго ($O(n^3)$), поэтому на практике чистые методы второго порядка почти не используются, но применяются некоторые приближения, которые называются *квази-Ньютоновские методы*

&nbsp;

### [[AIM ML#Content|BFGS (1 порядок)]]

Квази-Ньютоновский метод, который вместо обращения Гессиана считает его малоранговое приближение и делает это за квадрат. На самом деле это не метод второго порядка, так как под капотом не считаются вторые производные,  но идейные корни у него растут именно из Ньютоновского метода.

На самом деле он метод первого порядка в явном виде, потому что теряет основную фишку методов второго порядка – отсутствие необходимости в `learning rate`. Здесь для приближенного обращения Гессиана нужно подбирать специальный параметр.

&nbsp;

### [[AIM ML#Content|Условная оптимизация]]

Рассмотрим оптимизацию с ограничениями, а именно
$$f(x) \rightarrow \min; \;\;\; g_i(w) \leq 0; \;\;\; h_j(w) = 0$$
Выпишем *Лагранжиан*
$$L(w, \alpha, \beta) = f(w) + \sum\alpha_ig_i(w) + \sum\beta_jh_j(w)$$
И рассмотрим его максимум. Заметим, что если не выполнено хотя бы одно ограничение на $g_i$, $h_j$ , то этот максимум бесконечен, иначе он равен значению функции. Тогда для оптимизации можно искать следующее значение
$$\min\limits_w \max\limits_{\alpha, \beta} L(w, \alpha, \beta)$$
И, покуда функция выпукла, менять местами $\min$ и $\max$
- - -

&nbsp;
## [[AIM ML#Content|Линейные методы]]
### [[AIM ML#Content|Линейная регрессия]]
Предполагаем, что зависимость между целевой переменной и признаками линейна
$$a(x_1, x_2,\dots) = w_0 + w_1x_1 + w_2x_2 + \dots$$
Для удобства вводится фиктивный признак $x_0 = 1$ чтобы матричная форма имела удобный вид $a(x) = x^Tw$

Хотим решить систему

$$\begin{cases}x_1^Tw = y_1\\ \\x_2^Tw = y_2\\ \\ \cdots\\ \\ x_n^Tw = y_n\end{cases}$$

Оказывается полезным даже при нелинейной зависимости, а просто монотонной.
Хорошо работает для однородных признаков (так же, как и метрические методы)

Понятно, что мы почти никогда не сможем проложить гиперплоскость ровно через все объекты (они почти наверное не лежат на одной плоскости), поэтому надо считать невязку для каждого объекта $e_i = y_i - a_i$  и минимизировать некоторую функцию ошибки от этих невязок

- - -
 
В одномерном случае традиционно минимизируют *RSS*, или *Residual sum of squares*
$$RSS = e_1^2 + e_2^2 + e_3^2 + \dots \rightarrow \min$$
Получается в точности минимизация эмпирического риска.  Существует вероятностное обоснование, почему нужно использовать именно сумму квадратов, а не, скажем, модулей

>[!quote]- Вероятностное обоснование
>

Геометрический смысл RSS очевиден – сумма площадей квадратов, порожденных невязками

- - -

В многомерном случае часто минимизируют квадрат L2 нормы вектора отклонений (и записывают всё в матричной форме) $$\|Xw - a\|_2^2 \rightarrow \min$$
Задача выпукла и имеет единственное решение, его можно находить напрямую

>[!quote]- Прямой метод решения
Раскроем интересующую нас норму 
$$\|Xw - a\|_2^2 = (Xw - a)^T(Xw - a) = w^TX^TXw - w^TX^Ta - a^TXw + a^Ta$$
Чтобы минимизировать функцию, возьмем градиент и приравняем к нулю 
$$\nabla \|Xw - a\|_2^2 = 2X^TXw - 2X^Ta = 0$$
$$X^TXw = X^Ta$$
$$w = (X^TX)^{-1}X^Ta$$
При этом естественным образом получили *псевдообратную матрицу Мура-Пенроуза* $(X^TX)^{-1}X^T$, которая является обобщением на неквадратные матрицы
>
>Заметим, что ранги $X^TX$ и $X$ совпадают, и что $X^TX$ положительно определена, а значит редко когда вырождена, поэтому ее почти всегда можно обращать (вырожденный случай полностью ломает эту логику, но почти не встречается)

>[!danger]- Проблема вырожденности
>Матрица $X^TX$ может быть необратима. Но только ли это плохо? Ведь в природе вырожденность не встречается почти наверное (хотя если у нас, например, два столбца совпадают, то матрица сразу вырождается) 
>
>Даже если матрица и не вырождена,  она может быть близка к ней, и тогда решение будет неустойчивым (от небольших изменений околонулевых коэффициентов сильно меняется ответ линейных уравнений), поэтому нас не устраивают матрицы с высоким *числом обусловленности*, которое как раз и показывает степень близости матрицы к вырожденной
>
>##### Решения
>- Регуляризация признаков
>- Селекция признаков
>- Уменьшение размерности
>- Увеличение выборки



&nbsp;

![[Pasted image 20220818225552.png]]Отличие одномерного случая от многомерного 

>[!info]- Другие отклонения
>Также можно взять не вертикальные отклонения, а горизонтальные. По сути это значит поменять задачу регрессии для $y(x)$ на задачу регрессии для $x(y)$. Еще можно брать перепендикуляр к прямой (что логично и математично). Этим занимается метод главных компонент

&nbsp;

###  [[AIM ML#Content|Обобщенная линейная регрессия]]
Вместо того, чтобы линейно выражать функцию через признаки, будем линейно выражать функцию через функции над признаками $\varphi_i(x_1, x_2, \cdots, x_n)$ (она может быть и нелинейной, тогда метод тоже перестает таким быть)
$$a(x_1,\cdots,x_n) = w_0 + w_1\varphi_1 + \cdots + w_k\varphi_k$$
  Теперь пусть$$w = (w_0, w_1, \cdots, w_k)^T\;\;\;\;\;x = (x_0, x_1, \cdots, x_n)^T$$Хотим по аналогии минимизировать $$\|\varphi(X)w - y\|_2^2 \rightarrow \min$$где$$\varphi(X) = \begin{pmatrix}  
\varphi_0(x_1) & \cdots & \varphi_k(x_1)\\  
\cdots & \cdots & \cdots \\
\varphi_0(x_m) & \cdots & \varphi_k(x_m)\\  
\end{pmatrix}$$Но обо всем этом подробнее в нелинейных методах

> [!info]- One hot encoding
Часто возникают категориальные признаки, и с ними не получается работать так же, как с численными. Одним из хороших приёмов является *Ohe hot encoding*.
> 
> Допустим у нас есть признак роли человека, и у него есть три возможных состояния: физическое лицо, юридическое и учредитель. Если мы просто закодируем их цифрами 1,2,3, мы неявно добавляем в модель то, что учредитель в 3 раза важнее физ.лица, а юр.лицо в 2 раза важнее физ.лица. Мы не хотели этого делать.
> 
> И вот чтобы этого не было уществует простой трюк – создать 3 отдельных булевых признака, каждый из которых будет показывать, является ли объект физ.лицом, юр.лицом или учредителем. Теперь три категории учтены симметрично, и то, с каким масштабом их учитывать друг относительно друга, мы оставляем на совесть регрессии, которая поставит между этими признаками веса $w_1,\;w_2,\;w_3$, сама сказав, кого учитывать важнее


&nbsp;

### [[AIM ML#Content|Регуляризация]]
Представим, что мы строим линейную регрессию. Логично ожидать, что если объекты похожи, то похожими должны оказаться и метки. Представим, что у нас есть объекты, отличающиеся только в j-ом признаке на величину $\varepsilon_j$. Тогда для нашей линейной модели$$a(x_1, \cdots, x_n) = w_0 + w_1X_1 + \cdots + w_nX_n$$ответы будут отличаться на $\varepsilon_jw_j$ , то есть пропорциональны весу. Отсюда следует вывод, что нам нужны не очень большие веса, чтобы ответ не разносило. Поэтому параллельно с общей задачей оптимизации $\|Xw - y\|_2^2 \rightarrow \min$ хочется решать задачу $\|w\|_2^2 \rightarrow \min$

При этом $w_0$ может быть любым, потому что не влияет на учет какого-либо признака. Он, по сути, задает величину, масштаб в котором мы работаем, и в рамках этого масштаба мы хотим задать тенденцию (подобрать веса) с как можно меньшим разбросом и как можно большей устойчивостью

- - -

Есть три популярных вида регуляризации:
*Регуляризация по Иванову* $$\begin{cases}\|Xw - y\|_2^2\rightarrow\min\\ \\\|w\|_2^2\leq \lambda\end{cases}$$*Регуляризация по Морозову* $$\begin{cases}\|Xw - y\|_2^2 \leq \lambda\\ \\\|w\|_2^2 \rightarrow\min\end{cases}$$ *Регуляризация по  Тихонову* $$\|Xw - y\|_2^2 + \|w\|_2^2 \rightarrow \min$$
На самом деле эти регуляризации эквивалентны, и решение одной можно сформулировать как решение другой

 &nbsp;

### [[AIM ML#Content|Гребневая регрессия]]

Решение оптимизационной задачи регуляризации (с дополнительным параметром) известно в явном виде (так же приравнием к нулю градиент)
$$\arg\min\limits_w \|Xw-y\|_2^2 + \lambda\|w\|_2^2 = (X^TX+\lambda I)^{-1}X^Ty$$
Такая задача регрессии с параметризуемым регуляризатором называется *Гребневой регрессией (Ridge regression)*. При $\lambda=0$ мы получаем исходную задачу, при $\lambda=1$ получаем регуляризацию по Тихонову, а при $\lambda \rightarrow \infty$ мы все меньше и меньше затачиваемся на данные и все больше уменьшаем веса, как бы усредняя наш ответ

$\lambda$ называется *коэфициентом регуляризации*, или *shrinkage penalty*

![[Pasted image 20220819025202.png]]Сравнение устойчивости обычного RSS и с использованием гребневой регрессии

> [!warning]- Масштабируемость
>Линейная регрессия инвариантна относительно масштабирования признаков (то есть если признак уменьшается в два раза, то все зависящие от него веса корректируются и модель остается той же), а гребневая регрессия нет, как раз из-за регуляризатора, влияющего на веса

 Из-за того, что регуляризация работает по-разному на признаки разных масштабов, урезая самые большие величины, логичным и правильным тоном будет нормировка признаков. Поэтому еще раз – **перед регуляризацией идет стандартизация**

Как улучшить качество гребневой регрессии и загладить недостатки? Если добавить свободный член с помощью параметра `fit_intercept`, веса будут убывать гладко и монотонно. А если еще и нормализовать признаки с помощью `normalize`, то веса будут зануляться пропорционально, не пересекая друг друга и не смещая приоритет с одних весов на другие

![[Pasted image 20220823125131.png]]

Добавление свободного члена работает, потому что без него модель пытается приблизить зависимость, и свободный член имитирует всяким мусором, линейной комбинации каких-то признаков, как бы придавая им лишнее значение

&nbsp;

### [[AIM ML#Content|LASSO]]
*Least Absolute Selection and Shrinkage operator* предлагает другой штраф за сложность, а именно $L_1$ норму вместо $L_2$, как в ridge
$$\sum(y_i - a(x_i))^2 + \lambda\sum|w_i| \rightarrow \min$$
  Что от этого меняется? Меняется характер "зануления" весов.  Если в гребневой регрессии веса зануляются плавно, от здесь это происходит резко, вес как будто обрушивает. Также у коэффициентов получается другой масштаб, банально из-за того что регуляризация не возвожит веса в квадрат
  
  ![[Pasted image 20220823123929.png]] Сравнение регуляризаций. Слева гребень, справа лассо

В то время как гребневая регрессия всегда зависит ото всех признаков, просто с корректируемым масштабом, гребневая регрессия резко исключает самые ненужные признаки, что может быть полезно. 

Аналогично гребню, `fit_intercept` и `normalize` творят чудеса

![[Pasted image 20220823125510.png]]

&nbsp;

### [[AIM ML#Content|Elastic Net (LASSO + Ridge)]]

Буквально сумма двух предыдущих регуляризаций
$$\sum(y_i - a(x_i))^2 + \lambda_1\sum\|w_i\|_1  + \lambda_2\sum\|w_i\|_2^2\rightarrow \min$$
Геометрический смысл следующий:  ограничение второй нормы задает круг, в котором мы можем довольно точно подобрать минимум, в то время как первая норма задает квадрат, и велика вероятность того, что минимум будет именно на вершине квадрата. Смещаясь в эту сторону, мы резко увеличиваем один вес и уменьшаем другой. *Elastic Net* при этом комбинирует два подхода, имплицируя как бы скругленный квадрат

![[Pasted image 20220823130409.png]]

 При этом важно помнить о вероятностях зануления. Для разных норм они разные
 
 ![[Pasted image 20220823131116.png]]

- - -

Важно помнить, что всякая регуляризация – это упрощение модели, и это хорошо, но хорошо только до того момента, когда начинает теряться качество модели, поэтому стоит ориентироваться на ту же $R2$ ошибку и не перегибать с этим палку

&nbsp;

### [[AIM ML#Content|Селекция признаков]]
Зачем в принципе делать селекцию? Во-первых может быть наши признаки друг от друга линейно зависят, и тогда нужно исключить лишние, могут быть почти дубликаты, а еще при неотобранных признаках матрица, нужная для решения линейной регрессии может быть вырожденной (об этом выше). Также это банально уменьшает модель и удешевляет данные (если за замер каждого признака нужно платить компании, то здорово когда признаков мало). Есть несколько основных подходов к отбору признаков

1) *Автоматически внутри алгоритма*. Такого эффекта мы уже добивались, применяя лассо. В нём часть признаков занулялась как ненужная сама по себе, из под коробки. Так делает не только этот алгоритм

 2) *Фильтрация*. Эвристика, какой-нибудь параметр, по которому мы можем отсеить признаки. Например, можно посчитать корелляцию признаков с целевым, и оставить только признаки с высокой корелляцией. Это не самый элегантный пример (даже не сильно ккореллирующий признак может быть ценным), но помимо этого придумать можно что угодно
 
 3) *Перебор*.  Как-нибудь по-умному перебираем разные подмножества признакови проверяем, модель какого качества на них можно получить. Далее стараемся пополнить это множество признаков теми признаками, которые улучшаю качество, и таким образом оставляем только лучшие признаки

&nbsp;

### [[AIM ML#Content|Уменьшение размерности]]

Селекцию признаков и уменьшение размерности можно перепутать. И действительно, отбор признаков является одним из вариантов уменьшения размерности, но не наоборот. Например, мы можем перейти от пространства с сотней признаков в пространство с десятью признаками, правда там уже не будет ни одного изначального признака – все окажутся линейной комбинацией изначальных. Типичный представитель алгоритмов уменьшения размерности – *метод главных компонент*

&nbsp;

### [[AIM ML#Content|Увеличение выборки]]

Решая всю ту же задачу вырожденности хотелось бы иметь метод на случай, если объектов меньше, чем признаков, и из-за этого матрица $X^TX$ имеет низкий ранг. Действительно хотелось бы, чтобы объектов было больше чем признаков и ,если это не так, искусственно увеличить количество объектов
 
&nbsp;

### [[AIM ML#Content|Устойчивая регрессия]]

Линейная регрессия как алгоритм может сильно страдать от выбросов, потому что каждый отдельный выброс существенно сдвигает модель (в реальности выбросы в ту или иную сторону друг друга компенсируют, но полагаться на выбросы дело сомнительное). 

Можно считать ошибку на каждом объекте с разным весом, тем самым придавая важность одним ошибкам и игнорируя другие. Такая весовая схема немного изменяет нашу задачу оптимизации (добавляет один новый множитель $v_i$)
$$\sum v_i(y_i - w^Tx_i)^2 = \sum(\sqrt{v_i}y_i - w^T\sqrt{v_i}x_i)^2$$
Получается обычная задача линейной регрессии, просто как бы для новой выборки, где вместо $x_i$ имеем $\sqrt{v_i}x_i$, а вместо $y_i$ имеем $\sqrt{v_i}y_i$

Таким образом, можно свести эту весовую схему к деформации данных. Причем если веса целые, то вместо этого можно просто делать дубликаты, а если веса находятся на $[0, 1]$, то можно брать следующий объект выборки с соответствующей вероятностью

Настройкой весов и, как следствие, борьбой с чувствительностью к выбросам, занимается *устойчивая регрессия (Robust regression)*. Это итеративный алгоритм, в котором изначально инициализируются веса $v = (\frac{1}{m}, \frac{1}{m}, \frac{1}{m}, \dots)$, а затем проходит цикл

1. Настроить алгоритм, учитывая веса объектов (градиентным спуском, чем-то еще, в общем сделать `fit`)
2. Выделить ошибки на обучении $e_i = a(x_i) - y_i$
3. Пересчитать веса объектов по формуле $v_i = \exp(-e_i^2)$

Смысл в том, что если ошибка на объекте очень большая, то это наверняка выброс, и в следующий раз мы будем учитывать его меньше

&nbsp;

### [[AIM ML#Content|RANSAC]]

*RANdom SAmple Consensus* это некогда популярный алгоритм, у которого очень простая геометрическая реализация. Случайно выбираем некоторое базовое подмножество выборки, и проводим соревнование: несколько раз строим по подмножеству модель, выбираем точки, хорошо предсказанные моделью (как правило просто берем объекты с ошибкой не больше $M$, что красиво визуализируется в виде трубки вокруг предсказанной прямой). Пополняем хорошо предсказанными точками базовое множество. В итоге выбираем модель с минимальной ошибкой.

Получается алгоритм, который в зависимости от выбранных в начале точек может как заточиться на определенный кластер объектов, так и начать предсказывать общую тенденцию. Считается, что RANSAC тем лучше, чем больше в нем точек

![[Pasted image 20220825012645.png]]

При этом сама регрессия может быть любой, она тоже является параметром алгоритма, просто линейная регрессия – это параметр по умолчанию

&nbsp;

### [[AIM ML#Content|Проекционная матрица]]

Пусть у нас есть какая-то линейная зависимость с точностью до шума
$$y = Xw^* + \varepsilon$$где $w^*$ это коэфициенты, дающие идеальный ответ на задачу

Мы можем поставить нашу задачу так – пытаемся приблизить зависимость с точностью до шума
$$y \approx a = Xw$$
Значит оценить шум мы можем величиной
$$\hat \varepsilon = y - a$$
Зная явно оптимальное решение задачи, сделаем выкладку для оценки шума
$$\hat \varepsilon = y - a = y - Xw = y - X(X^TX)^{-1}y = (1 - H)y =$$$$(1 - H)(Xw^* + \varepsilon) = Xw^* - HXw^* + (1 - H)\varepsilon = (1-H)\varepsilon$$Здесь за $H$ мы обозначили всю матрицу, появляющуюся при решении оптимизационной, задачи, а в последнем переходе мы пользуемся тем, что первые два слагаемых равны друг другу, что ясно, если раскрыть $H$

Получается, что мы выразили оценку шума через истинный шум. Матрица $H$ называется *projection matrix*, или *Hat matrix*. Красивейший факт заключается в том, что $$a = Hy$$то есть ответы нашего алгоритма получаются из вектора истинных значений  путем действия именно этой матрицы

Диагональные элементы этой матрицы имеют красивую форму записи
$$h_{ii} = \frac{1}{m}+\frac{(x_i - \bar x)^2}{\sum\limits_k (x_k - \bar x)^2}$$
И этот элемент пригождается, если мы хотим понять, насколько один объект выборки влияет на результат регрессии (то есть оценить, насколько регрессия с этим объектом и без него различаются). Для этого считается известное в эконометрике *расстояние Кука* как раз с использованием диагональных элементов hat matrix
$$D_j = const\cdot\frac{h_{jj}}{(1 - h_{jj})^2}\hat \varepsilon^2_j$$
Это расстояние может показать нам, где наша модель чувствительнее к выбросам (там, где расстояние больше)

![[Pasted image 20220825133311.png]] Например выброс для объекта с меткой  $0.5$ в 4 раза менее существенен, чем с меткой $1$

Почему же она названа проекционной? Как раз потому что именно ею надо подействовать на истинную зависимость, чтобы спроецировать ее на нашу. К слову, наша зависимость неизбежно будет линейной комбинацией векторов из $X$. Это понятно как из природы алгоритма, так и из матричной формы записи. Отсюда ответ нашего алгоритма будет лежать в линейной оболочке векторов $X$

![[Pasted image 20220825133758.png]]

Можно доказать, что эта проекция ортогональна, и что это кратчайшее растояние (красная линия как раз символизирует квадрат невязки, который мы минимизировали)

&nbsp;

### [[AIM ML#Content|Линейная регрессия и SVD]]

Рассмотрим гребневую регрессию. Оптимальные веса находятся в явном виде $$w = (X^TX + {\color{Red} \lambda I})^{-1}X^Ty$$Воспользуемся для матрицы $X$ сингулярным разложением вида $X = U\Lambda V^T$
$$w = (V\Lambda^TU^TU\Lambda V^T + {\color{Red} \lambda I})^{-1}V\Lambda^TU^Ty = $$
$$= (V(\Lambda^T\Lambda + {\color{Red} \lambda I})V^T)^{-1}V\Lambda^TU^Ty =$$
$$= V\;\;\;{\color{RubineRed}(\Lambda^T\Lambda + \lambda I)^{-1}\Lambda^T}\;\;\;U^Ty = $$Выделенное выражение, покуда все матрицы диагональны, является диагональной матрицей с элементами $\frac{\lambda_i}{(\lambda_i^2 + \lambda)}$. Теперь если записать всё через столбцы матриц $U$ и $V$ получим
$$\sum\limits_j\frac{\lambda_j\cdot u_j^Ty}{\lambda_j^2 + \lambda}v_j$$
Получили, что веса $w$ выражаются через сумму, которая является, если относиться ко всей дроби как к коэфициенту, линейной комбинацией столбцов матрицы $V$. Cами коэффициенты являются скалярным произведением столбцов $U$ на целевой вектор с точностью до константы. В каком-то смысле это скалярное произведение отвечает за корреляцию (то есть чем больше столец коррелирует с целевым значением, тем больший вклад он вносит)

Тут же виден явный эффект от регуляризации – при больших $\lambda$ дробь зануляется (и можно посчитать, что число обусловленности при этом стремится к единице)

&nbsp;

### [[AIM ML#Content|Поиск ортогонального соответствия]]

Не самый популярный алгоритм, но упомянуть стоит. Идея в том, чтобы итерационно наращивать пространство признаков. Будем хранить $J$ – множество номеров используемых признаков (изначальнно $J = \emptyset$). На каждой итерации решаем оптимизационную задачу
$$\|y - X_{J \cup \{j\}}w\| \rightarrow \min\limits_{w,j}$$То есть находим такой признак, при добавлении которого задача решается лучше всего. Критерием останова будет либо достаточная точность алгоритма, либо достаточное количество введенных признаков

Эту идею наращивания можно применять и для других семейств алгоритмов

Обычно вместо того, чтобы решать задачу оптимизации, удаляют все дубликаты признаков, нормируют их, и заменяют исходную задачу следующим
$$j = \arg\max(X[,j]^T(y - X_Jw))$$
То есть выбираем такой признак, который лучше всего коррелирует с текущей ошибкой (так мы сможем лучше ее убрать, вычитая этот признак с нужным коэффициентом). Далее добавляем его в множество признаков, пересчитываем коэффициенты и идем дальше

&nbsp;

### [[AIM ML#Content|Бинарная классификация и сигмоида]]

Пусть $X = \mathbb{R}^n,\;Y = \{0, 1\}$. Чтобы решать задачу классификации линейной моделью, можем получать вероятность принадлежности классу 1. Но любая линейная функция из $\mathbb{R}^n$ будет отображаться на $\mathbb{R}$, поэтому нужно привести действительную прямую к отрезку  $[0, 1]$ с помощью некоторой деформации $\sigma$

То есть для объекта $x = (x_1, x_2, \dots)$ ответом алгортима будет $\sigma(w^Tx)$.

Самой популярной функцией деформации является *сигмоида* $$\sigma(z) = \frac{1}{1 + e^{-z}}$$
Также существует вариант, использующий интеграл с экспонентой из нормального распределения. Это называется *Normal Cumulative Distribution Function*, или *CDF*
	$$\Phi(z) = \frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{z}exp(-\frac{t^2}{2})dt$$

![[Pasted image 20220826113351.png]]

В целом подошел бы любой нормированный интеграл от интегрируемой функции, но именно сигмоида выбрана не спроста. У нее есть ряд хороших свойств. Запишем вероятность принадлежности классу 1. Если $z = w^Tx$, то
$${\color{goldenrod}P(Y = 1\;|\;x) = \sigma(z)} = \frac{1}{1+e^{-z}} = \frac{e^z}{1 + e^{z}} = 1 - \frac{1}{1 + e^{z}} = {\color{goldenrod} 1 - \sigma({-z})}$$
Получается, что сигмоиду можно записывать в таком симметричном виде. Тогда
вероятность принадлежности классу 0, при учете что $P(Y = 1\;|\;x) + P(Y = 0\;|\;x) = 1$, можно вычислить
$${\color{goldenrod} P(Y = 0\;|\;x)} = 1 - P(Y = 1\;|\;x) = 1 - \sigma(z) = 1 - 1 + \sigma(-z) = { \color{goldenrod}\sigma(-z)}$$
А ее производная красиво выражается через саму функцию
$${\color{goldenrod} \sigma'(z)} = \frac{-(-e^{-z})}{(1+e^{-z})^2} = \frac{1}{1 + e^{-z}}(1 - \frac{1}{1 + e^{-z}}) = {\color{goldenrod} \sigma(z)(1 - \sigma(z))}$$
- - -

Но это просто красивые свойства. сигмоида берется из вероятностных соображений.
Представим, что оба наших класса нормально распределены

![[Pasted image 20220826115857.png]]

Оказывается, что явным оптимальным решением в каждой точке является именно сигмоида! Это следует из просто формулы Байеса, в которую подставлены формулы нормальных многомерных распределений, причем оптимальность сигмоиды верна *и для нелинейного случая*. Получается, что сигмоида заточена именнно на нормальные распределения, и учитывая ее популярность, такое допущение является частой практикой. 

- - -
Как обучать сигмоиду? Ну, мы ведь трактуем получаемые значения как вероятности? Тогда давайте применять метод максимального правдоподобия
$$L(w_0, w_1, w_2, \dots) = \prod\limits_{y_i=1}p(x_i)\prod\limits_{y_i=0}(1 - p(x_i)) \rightarrow \max$$Опять таки, выпуклая задача оптимизации. Значит есть единственное решение. Логарифмируем
$$\log\prod\frac{1}{1 + e^{-z}} + \log\prod\frac{1}{1 + e^{+z}}\rightarrow \max$$
Если переобозначить метки через замену $y' = 2y - 1$, то есть перевести $\{0, 1\}$ в $\{-1, 1\}$, формула схлопывается в очень короткую запись
$$-\sum{\color{goldenrod}\log(1 + exp(-y'_iz_i))}\rightarrow \max$$
Выражение под суммой называется *логистической функцией ошибки*. Чтобы максимизировать записанное выше выражение, вычислим градиент и будем по нему спускаться
$$\nabla L = \sum\sigma(-y'_iw^Tx_i)y'_ix_i$$
Соответственно, если мы предпочтем стохастический градиентный спуск, будем спускаться только по одну слагаемому из суммы

&nbsp;

### [[AIM ML#Content|Многоклассовая логистическая регрессия]]

Мы грамотно описали задачу классификации, потому что в таком виде ее удобно обобщить. Пусть теперь для каждого класса есть своя функция (своё линейное выражение) $w_j^Tx$, и мы хотим перевести значения этой функции в вероятности принадлежности классам (то есть нормировать значения функции так, чтобы они были неотрицательны и их сумма равнялась единице)

Для этого перехода существует преобразование *softmax*$$P(y =k\;|\;x) = \frac{\exp(w_{0k} + w_{1k}x_1 + w_{2k}x_2 + \dots)}{\sum\limits_i\exp(w_{0i} + w_{1i}x_1 + w_{2i}x_2 + \dots)}$$
Берем экспоненту от нашего выражения и делим на сумму экспонент

&nbsp;

### [[AIM ML#Content|Итоги]]

Линейные модели просты и надежны, популярны, быстры, интерпретируемы, поддерживают экстраполявцию и их, в прицнипе, можно обощить до нелинейных.

Но с другой стороны вряд ли в данных будет именно линейная зависимость, алгоритм страдает из-за выбросов и неоднородности признаков и возникают проблемы, если признаки линейно зависят друг от друга

- - -

&nbsp;

## [[AIM ML#Content|Линейная классификация]]

### [[AIM ML#Content|Бинарная классификация]]

Пусть теперь метки это $\{1;-1\}$. Хотим сделать линейную модель. В бинарном случае это очень удобно – просто смотрим на знак нашей линейной функции
$$a(x) = sign(w^Tx + b) \in [1;-1]$$(свободный член иногда будем заносить внутрь $w^Tx$, иногда писать отдельно, это не так критично)

Как можем выбирать гиперплоскость? Логично выбирать из всех гиперплоскостей ту, у которой наименьшая ошибка. Для этого можно использовать тривиальную сумму всех *0-1 loss* $$L(y_i, a(x_i)) = \begin{cases} 1,\;a(x_i) \neq y_i\\0,\;a(x_i) = y_i\end{cases}$$
То есть просто посчитать число ошибок. Но на практике так никто никогда не делает, потому что функция не дифференцируема. Более того, ее производная это везде ноль, и никаким градиентным спуском мы не найдем оптимум. К тому же в ней мало информации, потому что мы даже не знаем, насколько сильна каждая из ошибок (точка была почти на границе или очень далеко от нее)

Что же нам делать? Введем понятие *зазор (margin)*  $$z_i=y_iw^Tx_i$$То есть не просто берем ошибку, а умножаем на значение (получаемая величина пропорциональна расстоянию до гиперплоскости, поэтому получаем действительно степень ошибки)

С помощью зазоров мы можем перписать наш 0-1 Loss $$L(y_i, a(x_i))=I({\color{goldenrod} y_iw^Tx_i} < 0)$$
Но еще раз – это плохая функция. Тогда может быть вместо того, чтобы минимизировать сумму плохих функций мы ограничим их сверху хорошей функцией и будем минимизировать ее? Такие функции называются *суррогатными функциями*. Вот примеры нескольких гладких функций от зазора, улучшаяющих свойства 0-1 Loss

![[Pasted image 20220830195028.png]]

*Перспетрон, SVM, логистическая регрессия* минимизируют как раз выпуклые аппроксимации 0-1 Loss.

&nbsp;

### [[AIM ML#Content|Персептронный алгоритм]]

В *персептроне* в качестве суррогатной функции выбран $f(z) = \max(-z, 0)$, то есть если все правильно, то ошибка нулевая, иначе она пропорциональна расстоянию. Одной формулой, мы пытаемся решить следующую задачу оптимизации$$\sum\limits_i \max(0,\;-y_i(w^Tx_i)) \rightarrow \min$$Если мы будем применять *SGD* к нашему алгоритму, то новая позиция будет пересчитываться по следующей формуле $$w \leftarrow w + \eta\begin{cases}0,\;\;\;\;\;\;\;\;\;sgn(w^Tx) = y_i\\
+x_i,\;\;\;\;w^Tx \leq 0;\; y_i = +1\\
-x_i,\;\;\;\;w^Tx \geq 0;\; y_i = -1\end{cases}$$Есть *теорема Новикова*, которая доказывает, что этот процесс сходится

Если сравнивать персептронный алгоритм с логистической регрессией, можно понять что алгоритмы делают по сути одно и то же. Тогда как логистическая регрессия совершает переход $$w \leftarrow w + \eta\sigma(-y_iw^Tx_i)y_ix_i$$Персептронный алгоритм делает следующее $$w \leftarrow w + \eta \;\mathbb{I}(-y_iw^Tx_i \geq 0)y_ix_i$$То есть логика действия алгоритмов одна и та же. В одном случае мы смотрим на само наличие ошибки, а в другом на ее масштаб, и пересчитываем нашу весовую модель в соответствии этой ошибки

&nbsp;

### [[AIM ML#Content|SVM]]

*Support Vector Machine*, или *Метод опорных векторов* – самый популярный алгоритм 90-ых. 

Допустим мы решаем задачу линейным методом (обощается на нелинейные). И даже пусть классы линейно разделимы. Тогда есть куча вариантов, какую гиперплоскость выбирать.

![[Pasted image 20220902143032.png]]

Какая лучше? Наверное на этом рисунке последняя. Потому что в трёх других при малейшей неточности в данных, если точка на самом деле находится в немного другом месте, её метка уже будет неверна. Наверное наша гиперплоскость должна стараться держаться подальше от объектов, чтобы быть устойчивой к ошибкам.

Как это формализовать? Можем как бы *"расширять"* нашу гиперплоскость до *епсилон-коридора* во все стороны с одинаковой скоростью, и считать, что объекты выборки это твердые препятствия, которые и называются *опорными векторами*, и при соприкосновении с ними нужно подвинуть плоскость. Таким образом мы как бы надуваем наш ответ, двигая его в такое положение, в котором он находится дальше всего от точек выборки.

![[Pasted image 20220902143621.png]]

Если подумать, то мы ищем кратчайшее расстояние между *выпуклыми оболочками классов*, и в ответ в качестве плоскости даем серединный перпендикуляр к этому расстоянию

Теперь формализуем эту идею: пусть у нас есть выборка $\{x_i, y_i\}$ и метки классов $y_i$ лежат в $\{-1;1\}$. Мы хотим разделить классы плоскостью, то есть давать в ответ знак относительно этой гиперплоскости, который и будет определять класс нашего объекта (здесь мы оставляем свободный член отдельно) $$a(x) = sgn(w^Tx + b)$$
Мы хотим, чтобы по итогу метка класса совпадала с ответом алгоритма, то есть
$$w^Tx + b\;\begin{cases}\;\geq+1,\;y_i = +1\\\;\leq -1,\;y_i = - 1\end{cases}$$(отделить знак плоскости единицей можем, если нормируем данные)

Или, что то же самое (опять таки из нормировки)$$y_i(w^Tx_i + b) \geq 1 \Leftrightarrow \min|w^Tx_i + b| = 1$$
Мы хотим максимизировать зазор, то есть расстояние между двумя ближайшими объектами. Из школьной формулы известно, что $$\rho(x_i, w^Tx + b) = \frac{|w^Tx_i + b|}{\|w\|}$$
Хотим, чтобы минимальное такое расстояние было максимально, то есть решить задачу
$$\min\frac{|w^Tx + b|}{\|w\|} = \frac{1}{\|w\|} \rightarrow \max$$При нашей нормировке. То есть, чтобы расстояние между объектами было максимальным, нужно, чтобы норма $w$ была минимальна... Да это же, по сути, *регуляризация*! Раньше мы выводили ее на пальцах, а теперь у нее есть *геометрический смысл*. Она отвечает как раз-таки за этот зазор


>[!info]- Обобщение
Вообще, если обощить, то есть целое семейство алгоритмов, пользующееся такими вот *score function*, то есть функциями, оценивающими наш ответ. В общем виде это выглядит так $$a(x) = f(b(x))$$Здесь $|b(x_i)|$ – наша уверенность в ответе.
$y_ib(x_i)$ – насколько уверенность оправдала ожидания,
$f(x)$ деформирующая функция

>[!quote]- Оптимизация, двойственная задача
>
Раз нам надо минимизировать веса, давайте решать эту оптимизационную задачу с соответствующими ограничениями. Для простоты будем минимизировать квадрат нормы, и поделим его на два
$$\frac{\|w\|^2}{2} \rightarrow \min$$$$y_i(w^Tx_i + b) \geq 1$$
> [!quote]+ Приведение к двойственной задаче
> Вспоминаем оптимизацию с ограничениями и *условия Куна - Таккера*.  Записываем задачу через *Лагранжиан*$$\min\limits_{w,b}\max\limits_{\alpha} L(w,b,\alpha)$$
> Этот Лагранжиан будет иметь следующий вид $$L(w, b, \alpha) = \frac{w^Tw}{2} + \sum\limits_i\alpha_i(1 - y_i(w^Tx_i + b))$$
> Тупо берем производные этого Лагранжиана и приравниваем к нулю
> $$L'_w = 0 \Leftrightarrow w = \sum\limits_i \alpha_iy_ix_i$$
> $$L'_b = 0 \Leftrightarrow \sum\limits_i \alpha_iy_i = 0$$
> Выразили $w$, значит можем подставить в изначальную формулу и не забыть про второе условие
> $$\max\sum\limits_i\alpha_i - \frac{1}{2}\sum\limits_j\sum\limits_i\alpha_i\alpha_jy_iy_jx_i^Tx_j$$
> $$\sum\limits_i\alpha_iy_i = 0$$
> Получили двойственную задачу. Она имеет не единственное решение, но ее плюс в том, что опорные вектора в ней это ровно те объекты, у которых ненулевой $\alpha_i$, а так как $w = \sum\limits_i \alpha_iy_ix_i$, получается что мы можем выразить итоговый вектор весов только через эти опорные вектора! (действительно, если убрать из выборки все объекты, кроме граничных, то гиперплоскость не изменится, и это очень крутая оптимизация. 
> 
> Получается, что этот алгоритм во-первых строит весовую схему, во-вторых благодаря $\alpha_i$ явно показывает, как ее выразить через *некоторые* из наших признаков, а в-третьих утверждает, что остальные объекты нам не нужны. Снова получаем тот же эффект, что и в *boundary trees*, когда из выборки остаются только граничные величины
>
>- - -
>
>Как в итоге делать алгоритм SVM? Строится вспомогательная матрица $H = \|y_iy_jx_i^Tx_j\|_{m \times m}$. Далее решаем задачу оптимизации $-\frac{1}{2}\alpha^TH\alpha + I\alpha \rightarrow \max$ при условиях $\alpha \geq 0$ и $y^T\alpha = 0$
>
>Решение выглядит следующим образом $$w = \sum\limits_i\alpha_iy_ix_i$$ $$b = \frac{1}{|S|}\sum\limits_{i \in S}(y_i - w^Tx_i)$$где $S = \{i \in \{1,2,\dots,m\}\;|\;\alpha_i > 0\}$
>
>- - -
>
>Зачем приводить задачу к двойственной? Во-первых ради сокращения размерности, во-вторых задачи становится понятнее с точки зрения оптимизации, и на нее есть много солверов.
>
>А еще так как двойственная задача работает не с изначальными признаками, а с матрицей $H$, состоящей из скалярных произведений признаковых описаний, но не содержащей признаков в чистом виде, можно делать несколько трюков. Например, есть задачи, где мы не можем напрямую взять значение признака, но можем посчитать какое-то взаимодействие между признаками, которое можно принять за скалярное произведение

- - -

В итоге получаем нашумевший алгоритм, который разделяет линейно разделимую выборку, причем делает это с максимальным зазором. Однако он чувствителен к шуму и машстабу данных, поэтому данные нужно предварительно чистить и нормировать.

![[Pasted image 20220903165525.png]]

Но линейно разделимые данные это какая-то невероятная редкость. Поэтому на практике нужно либо стремиться к тому, чтобы ошибок было меньше, а не ноль, либо переходить в другое признаковое пространство, изменяя природу решения

![[Pasted image 20220903165758.png]]

Для первого варианта (допускаем ошибки) мы можем стремится к ситуации, где все неверные срабатывания находятся хотя бы на сильно далеко. Можно задаться, например, серединой класса (например медианным расстоянием) или какой-то его частью

![[Pasted image 20230602143646.png]]

Тогда к минимизируемому члену регуляризации мы добавим сумму всех выбросов, будем стремится уменьшить и её.

&nbsp;

## [[AIM ML#Content|Нелинейные методы]]

Очевидно, что линейные методы не всемогущи — они улавливают только линейные зависимости, чувствительны к выбросам и неоднородным признакам. Для начала давайте дадим им шанс и посмотрим, как можно попробовать исправить недостатки линейных моделей.

### [[AIM ML#Content|Деформация. Полиномиальная модель]]

Если зависимость сложнее чем линейная, мы можем попробовать определить её также через линейную модель, но в этот раз подбирая веса не для признаков (составляя тем самым линейную функцию от признаков), а для нелинейной (например, полиномиальной) функции от признака. Можно смотреть на это так: мы пополняем признаковое пространство $\{x_1, x_2, \dots, x_n\}$ признаками $\{x_1x_2, x_1x_3 \dots, x_{n-1}x_n\}, \{x_1^2x_2, x_1^2x_3 \dots, x_{n-1}^2x_n\}, \{x_1x_2x_3, x_1x_2x_4 \dots, x_{n-2}x_{n-1}x_n\} \dots$. Короче говоря, произвольными *мономами* (это могут быть как степени одного признака, так и попарные произведения или более сложные структуры)

>[!danger] Проблемы с переобучением
>Тут надо быть осторожным — очень легко переборщить со степенью, сделать модель слишком сложной и переобучить её.

Проблемой является еще и разрастающееся число признаков. При попарных или, еще хуже, тройных произведениях, происходит комбинаторный взрыв, увеличивающий число признаков неприлично сильно. А мы прекрасно знаем, что для эффективного обучения их должно быть существенно меньше чем число объектов.

![[Pasted image 20230616195921.png]]

>[!info]- А целевой признак?
>Можно также деформировать целевой признак. Например, наложить на него корень или логарифм (и предсказывать, соответственно, корень или логарифм). Тогда природа может стать линейной, а восстановить по корню или логарифму изначальную величину проще простого

Разумеется, полиномиальная модель это лишь частный случай изменения признакового пространства. Иногда его можно вообще переиначить.

Как вариант, можно сравнивать признак со множеством порогов $\{\theta_i\}$ и задаваться задачей аппроксимировать целевую функцию линейной комбинацией таких вот порогов $I(x < \theta_i)$