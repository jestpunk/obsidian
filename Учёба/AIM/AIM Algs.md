# Алгоритмы
#prog #course #ozon #datascience | [[prog]] [[course]] [[ozon]] [[datascience]]

Курс Рубцова, семестры ЭВМ и Кормен

> [!bug]- Шпаргалка
> - $\Theta(n)$ - нижняя и верхняя границы, точная оценка
> - $O(n)$ - верхняя граница, "меньше либо равно"
> - $\Omega(n)$ - нижняя граница, "больше либо равно"
- - -


&nbsp;

## Рекурсия и итерация
### Алгоритм Евклида
>[!abstract]- Описание
Поиск НОД двух чисел x, y.
&nbsp;
Длина входа в битовой модели:
$$\lceil \log_2x \rceil + \lceil \log_2y \rceil = n$$
&nbsp;
Пусть $x \geq y$
> ---
> Если $y = 0$, то ответ х
> Если $y \neq 0$, то 
> $$НОД(x, y) =  НОД(x-y, y)$$
> Более того $$НОД(x, y) = НОД(y, x\%y)$$
>
> >[!quote]- Доказательство
> > Деление x на у есть представление x в виде
> > $$x = ky + b$$
> > &nbsp;
> > Пусть d = НОД(x,y). Тогда поделим оба этих числа на d
> > $$dx' = kdy' + b \Rightarrow b = d(x' - ky')$$
> > То есть d в том числе делитель остатка b
> 
> Написанное есть рекурсивный алгоритм относительно функции НОД

>[!tip]- Алгоритм
> 1) Приводим полученную пару чисел $x, y$, где $x > y$, к паре $x\%y, y$
> 2) Если одно из чисел 0, ответом будет другое число

>[!done]- Оценка сложности
> Очевидно, что остаток при делении x на y меньше чем x/2, а значит число уменьшается минимум в два раза, то есть длина входа на каждом шаге уменьшается хотя бы на 1. Алгоритм линеен
> &nbsp;
> **Сложность:** *$O(n)$*

>[!quote]- Код 
> ```C
> int euclid(int x, int y)
> {
> 	if(x == 0)
> 		return y;
> 	if(y == 0)
> 		return x;
> 	if(x >= y)
> 		return euclid(x % y, y);
> 	return euclid(x, y % x);
> }
> ```

 &nbsp;
### Итерация
Рекурсию можно свести к итерации. Например, алгоритм Евклида
 ```C
int iter_euclid(int x, int y)
{
	while(y > 0)
	{
		int d = x % y;
		x = y;
		y = d;
	}
	return x;
}
 ```
 
 Компьютер автоматически переводит функцию из рекурсивной в итеративную, храня значения в стеке. 
 
 При этом стек нужен **только** тогда, когда после рекурсивного вызова есть хвост, то есть надо запоминать значения, чтобы потом вернуться на уровень выше и делать какие-то действия. Поэтому для усорения нужно по возможности сначала делать действия, а вызывать рекурсию в самом конце
 &nbsp;
 
 ### Расширенный алгоритм Евклида
>[!abstract]- Описание
> Нужно решить в целых числах (найти частное решение)
> $$ax + by = c$$
> Пусть $a \geq b$
> - - -
> $$by = c - ax$$
> Поделим с остатком
> $$y =  c' -a'x + \frac{c''-a''x}{b}$$
> $y,c',a',x$ - целые, значит и $\frac{c''-a''x}{b} = t$ - целое. Значит
> $$bt + a''x = c''$$
> Привели линейное уровнение к другому с меньшими коэффициентами,
> где в роли $x,y$ выступают $t,x$, и по новой паре решений однозначно восстанавливается старая
> > [!warning]- Важно
> > Коэффициенты $a,b$ в расширенном алгоритме Евклида изменяются так же, как числа в обычном алгоритме Евклида



>[!tip]- Алгоритм
>```Python
>pass
>```

>[!done]- Оценка сложности
>```Python
>pass
>```

>[!quote]- Код 
>```Python
>pass
>```
- - -

&nbsp;
## Разделяй и властвуй

### Алгоритм Карацубы
умножаем числа, уменьшая их длину вдвое
>[!abstract]- Описание
Перемножение двух битовых чисел $\overline{ab}$ и $\overline{cd}$ (оба числа длинны степени двойки, что верно для всех стандартов целых чисел в компьютере).
> - Первый алгоритм, который был быстрее квадратичного.
> - Иногда приписывается Гауссу, что неверно
> - - -
> $$\overline{ab} \cdot \overline{cd} = (a\cdot 2^{n/2} + b)(c\cdot 2^{n/2} + d) = ac\cdot 2^n + bd + 2^{n/2}(ad+bc)$$
> Получили разложение, при котором одна задача длины n разложилась на 4 задачи длины n/2, но это квадратичная сложность!
> 
> Но $ad + bc = (a+b)(c+d) - ac - bd$. Два последних перемножения мы и так планировали считать, поэтому скобка с двумя умножениями превращается в выражение с одним умножением, то есть задач на самом деле 3. То есть реккурентное соотношение имеет вид
> $$T(n) = 3T(\frac{n}{2}) + cn$$

>[!tip]- Алгоритм
> 1) Побитово разбить числа на половины
> 2) Рекурсивно поситать 3 необходимых умножения 

>[!done]- Оценка  сложности
Как было описано выше, реккурента имеет вид
$$T(n) = 3T(\frac{n}{2}) + cn$$
Расписывая количество операция на каждом уровне дерева рекурсий, получим сумму
$$\sum\limits_{i=0}^{h-1}(\frac{3}{2}^i)cn + dn^{log_23}$$
$$\sum\limits_{i=0}^{h-1}(\frac{3}{2}^i)cn = \frac{\frac{3}{2}^h - 1}{\frac{3}{2} - 1}cn =  \frac{3^h - 2^h}{2^{h-1}}cn = \frac{n^{log_23} - n}{2n}cn = \frac{cn^{log_23}-cn}{\frac{1}{2}} = \Theta(n^{log_23})$$
>
>**Сложность:** *$\Theta(n^{log_23})$*


>[!quote]- Код
> Пример с основанием 10
> ```python
> def karat(x,y):
>     if len(str(x)) == 1 or len(str(y)) == 1:
>         return x*y
>     else:
>         m = max(len(str(x)),len(str(y)))
>         m2 = m // 2
> 
>         a = x // 10**(m2)
>         b = x % 10**(m2)
>         c = y // 10**(m2)
>         d = y % 10**(m2)
> 
>         z0 = karat(b,d)
>         z1 = karat((a+b),(c+d))
>         z2 = karat(a,c)
> 
>         return (z2 * 10**(2*m2)) + ((z1 - z2 - z0) * 10**(m2)) + (z0)
> ```

&nbsp;
### Мастер Теорема

>[!note]- Мастер теорема	
> Для реккуренты следующего вида
> $$T(n) = aT(\frac{n}{b}) + f(n)$$
> 1) Если $f(n) = O(n^c)$ и $c < log_ba$ $$T(n) = \Theta(n^{log_ba})$$
> 2) Если $\exists\; k \geq 0 : f(n) = \Theta(n^clog^kn$) и $c = log_ba$ $$T(n) = \Theta(n^clog^{k+1}n)$$
> 3) Если $f(n) = \Omega(n^c)$ и $c > log_ba$, а еще $af(\frac{n}{b})\leq kf(n)$ для > некоторого $k<1$ и больших n $$T(n) = \Theta(f(n))$$

>[!quote]- Доказательство
>В основном аналогично оценке сложности в алгоритме Карацуба
- - -
&nbsp;

## Сортировки
Чтобы найти максимальный элемент массива, нужно пройтись всего один раз. Чтобы найти второй максимум, нужно либо хранить в памяти два элемента, лиюо пройтись два раза. Продолжая рассуждения, чтобы найти медиану массива, нужно либо потратить квадратичное время, либо задействовать линейное количество памяти.

К счастью мы знаем, что эта логика не самая оптимальная и можно, например, отсортировать массив за $O(nlogn)$ и взять средний элемент. Покажем алгоритм, который находит медиану за линейное время

Определим k-ую порядковую статистику через $a_{(k)}$

&nbsp;

### Поиск порядковой статистики (медиана за линейное время)
ищем медиану медиан, делаем по ней партишн, ищем в одном из подмассивов
>[!abstract]- Описание
> Алгоритм находит k-ую порядковую статистику в массиве а.
> 
> - Вход: (a, k)
> - Выход: $a_{(k)}$
> - - -
> - Разобьем весь массив на блоки по **5** элементов и отсортируем блоки. Теперь в центре каждого блока находится его медиана.
> - Ищем медиану $m*$ среди этих медиан. Переставим элементы так, чтобы $m*$ находилась на своем $l+1$-ом месте, чтобы левее нее было  *l* меньших элементов, а справа *r* больших (сделаем *partition*)
> - Если $l+1=k$, то ответ, иначе если $l+1 > k$ ищем медиану в $a[:l+1]$, иначе ищем в $a[l+2:]$
> - - -
> При этом каждая из  $m*$ больше (и меньше) как минимум $\frac{3}{10}$ всех элементов массива (очевидно)
> >[!quote]- Доказательство
> >$m*$ больше половины остальных медиан блоков. Но каждая из этих медиан больше двух элементов в своем блоке (всего их 5), поэтому медиана $m*$ больше чем 3 из 5 значений в половине блоков 

^c6fbb6

>[!tip]- Алгоритм
> 1. Разбиваем массив a на группы по 5 элементов и сортируем их
> 2. Ищем медиану среди получившихся медиан $m*$
> 3. Приводим массив к виду $[\{x: x<m*\},\; m*, \;\{x: x>m*\}]$
> (делаем *partition* массива относительно $m*$)
> 4. Если в новом массиве $m*$ стоит на k-ом месте, это ответ. Иначе продолжаем поиск в одном из подмассивов

>[!done]- Оценка сложности
> - Сортировка всех блоков:  **$O(n)$** (т.к. блоки размера 5 и их сортировка константна по времени)
> - Найти медиану медиан: **$T(\frac{n}{5})$**
> - Реорганизация массива: **$O(n)$** (заполянем новый массив, кидая маленькие элементы в начало, а большие в конец)
> - Так как максимальная длина любого подмассива в худшем случае $\frac{7n}{10}$ ([[AIM Algs#^c6fbb6| см.тут]]), Оценкой сверху на рекурсивный вызов будет **$T(\frac{7n}{10})$**
> 
> Итого имеем неравенство **$T(n) \leq T(\frac{n}{5}) + T(\frac{7n}{10}) + cn$**
> Докажем, что его правая часть не превсоходит $dn$ для некоторого $d$
> > [!quote]-  Доказательство
> > Переход:
> >  Если $T(n) \leq dn$, то $T(n) \leq T(\frac{n}{5}) + T(\frac{7n}{10}) + cn \leq \frac{dn}{5} + \frac{7nd}{10} + cn \leq dn$ 
> > Достаточно взять $d \geq 10c$
> > &nbsp;
> > База:
> > Возьмем достаточно большое d, чтобы оно превосходило константное $T(1)$
 

>[!quote]- Код 
> ```Python
> def k_choise(a: list[int], k: int) -> a_k: int:
> 	if len(a) < 5:
> 		b = a.sorted()
> 		return b[k]
> 	ms = []
> 	for i in range(5, len(n), 5):
> 		block = a[i-5: i]
> 		ms.append(k_choise(block, 2))
> 	mstar = k_choise(m_star, len(floor(m_star//2))
> 	new_a = partitioned(a, mstar)
> 	l = new_a.index(mstar)
> 	if new_a[k] < mstar:
> 		return k_choise(new_a[:l], k)
> 	elif new_a[k] == mstar:
> 		return mstar
> 	else:
> 		return k_choise([l+1:], k-l+1)
> ```

&nbsp;
### Partition 
>[!abstract]- Описание
 На вход подается массив и его опорный элемент  $x$ (*pivot*). Необходимо переделать массив таким образом, чтобы все элементы, меньшие опорного, располагались левее него, а все большие правее. Функция возвращает номер опорного элемента в новом массиве 
> 
> Чтобы не возиться с передачей элемента, будем вместо этого ставить его в конец массива и считать что $x$ - последний элемент массива
> > [!quote]-  Доказательство корректности
> > Допишем слева к массиву $-\infty$. Всегда выполняется следующий инвариант:
> > 
> > *Слева от красного указателя находятся только элементы, меньшие опорного. Между красным и синем указателем находятся элементы, большие опорного. *
> > 
> > Очевидно, что когда синий маркер уходит за конец массива, а красный остается в массиве, при данном инварианте происходит *partition*

>[!tip]- Алгоритм
> 1. Ставим "красный" указатель в начало массива. Двигаем его вправо до тех пор, пока не найдем элемент, больший опорного. Ставим "синий" указатель сразу за красным. 
> 2. Двигаем синий указатель вправо до тех пор, пока не найдем элемент, меньший опорного
> 3. Меняем местами элементы под указателями. Сдвигаем оба указателя вправо 
> 4. Если синий указатель уходит за массив (то есть на шаге 3 мы поменяли последний элемент с каким-то из середины массива), прекращаем работу. Иначе см. пункт 2

>[!done]- Оценка сложности
> На каждом шаге синий указатель сдвигается хотя бы на один вправо. Значит, алгоритм выйдет за границы массива за линейное время
**Сложность:** *$O(n)$*

>[!quote]- Код
>```Python
> def partition(a):
>     pivot = a[-1]
>     red = 0
>     while(red < len(a) and a[red] < pivot):
>         red += 1
>     blue = red + 1
>     while(blue < len(a)):
>         while(blue < len(a) and a[blue] > pivot):
>             blue += 1
>         tmp = a[red]
>         a[red] = a[blue]
>         a[blue] = tmp
>         red += 1
>         blue += 1
>     return red-1
> ```

&nbsp;
### Быстрая сортировка (quicksort) 
Партишн элемента, потому партишн получившихся подмассивов
>[!abstract]- Описание
Просто постоянно выполняем *partition*, после чего рекурсивно выполняем *partition* для левых и правых подмассивов 

>[!tip]- Алгоритм
> 1. Выполняем  `partition` в границах от `l` до `r`. Его результат - `k` 
> 2. Если левый подмассив длины больше 1, применяем сортировку к нему
> 3. Если правый подмассив длины больше 1, применяем сортировку к нему

>[!done]- Оценка сложности
> Если получается добиться сбалансированных разбиений, алгоритм так же быстр, как сортировка слиянием. Иначе он может быть так же медленен, как сортировка вставками
> 
> В худшем случае, если каждый раз мы разбиваем нашу задачу на две подзадачи размеров $n-1$ и $0$, реккурента имеет вид $$T(n) = T(n-1) + \Theta(n)$$
> Что соответствует сложности *$T(n) = \Theta(n^2)$*
> В лучшем случае, если каждый раз мы разбиваем задачу на две подзадачи размеров $\frac{n-1}{2}$, реккурента имеет вид $$T(n) = 2T(\frac{n-1}{2}) + \Theta(1)$$
> Что соответствует сложности *$T(n) = \Theta(n^2)$*

>[!quote]- Код 
>Немного изменим `partition`, чтобы он также принимал индексы
>```Python
>def partition(a, l, r):
>    pivot = a[r]
>    red = 0
>    while(red <= r and a[red] < pivot):
>        red += 1
>    blue = red + 1
>    while(blue <= r):
>        while(blue <= r and a[blue] > pivot):
>            blue += 1
>        tmp = a[red]
>        a[red] = a[blue]
>        a[blue] = tmp
>        red += 1
>        blue += 1
>    print(a)
>    return red-1
>        
>def qsort(a, l, r):
>    k = partition(a, l, r)
>    print(f"a = {a}, k = {k}")
>    if(k-l) > 1:
>        print(f"go to {a[l:k]}")
>        qsort(a, l, k-1)
>    if(r-k) > 1:
>        print(f"go to {a[k+1:r+1]}")
>        qsort(a, k+1, r)
>```

&nbsp;
### Быстрая сортировка (вероятностый алгоритм)
партишн *случайного* элемента, потом партишн *случайного* элемента подмассивов

>[!abstract]- Описание
Та же быстрая сортировка, только опорный элемент каждый раз выбирается случайно

>[!tip]- Алгоритм
> 1. Выполняем `partition` в границах от `l` до `r` *относительно случайного элемента*. Его результат - `k` 
> 2. Если левый подмассив длины больше 1, применяем сортировку к нему
> 3. Если правый подмассив длины больше 1, применяем сортировку к нему

>[!done]- Оценка сложности
Оценим матожидание числа сравнений.
Пусть $C$ - число сравнений. Это случайная величина, то есть функция. Множество перестановок однозначно соотносится с порядком выбора опорных элементов, поэтому перестановки подходят в качестве вероятностного пространства для этой задачи.
> 
> Тогда $C$ - функция из пространства перестановок в целые числа, причем она считается как $$C(p) = \sum\limits_{i<j}I_{ij}(p)$$ где $I_{ij}(p)$ индикатор того, сравнивались ли $a_i$ и $a_j$ для перестановки $p$. Из линейности матожидания очевидно, что $$\textbf{E}(C) = \sum\limits_{i<j}\textbf{E}(I_{ij}(p))$$
> Какая же вероятность, что два числа сравнятся? Элементы сравниваются, когда один из них опорный. Очевидно следующее: если опорным становится элемент, находящийся между $a[i]$ и $a[j]$, то $a[i]$ и $a[j]$ больше никогда не будут сравнены (центральный элемент разделит их при `partition`)
>
> Значит $a[i]$ и $a[j]$ сравнятся, когда какой-то из них стал опорным до того, как опорным стал какой-то из элементов между ними. Из равномерности вероятность сравнения равняется $\frac{2}{j-i+1}$. Тогда
> $$\textbf{E}(C) = \sum\limits_{i<j}\textbf{E}(I_{ij}(p)) = \sum\limits_{i<j}\textbf{E}(\frac{2}{j-i+1}) = \sum\limits_{j=2}^{n}\sum\limits_{i=1}^{j-1}(\frac{2}{j-i+1}) = \sum\limits_{j=2}^{n}\sum\limits_{k=1}^{j-1}(\frac{2}{k+1}) = 
\sum\limits_{j=2}^{n}\sum\limits_{k=2}^{j}(\frac{2}{k})$$
> Последняя сумма есть ни что иное как $n\Theta(logn)$
>
> **Сложность**:  *$\Theta(nlogn)$*


>[!quote]- Код
>```Python
>pass
>``` 

&nbsp;
### Сортировка вставками (insertion sort) #stable 
Понемногу делаем массив отсортированным, вставляя элементы в нужные места
>[!abstract]- Описание
Начниаем строить внутри нашего массива $A$ отсортированный подмассив $a$. Пусть для начала в нем только первый элемент. Далее если следующий за $a$ элемент больше чем $a[-1]$, значит его можно добавить без нарушения отсортированности, то есть теперь к $a$ добавился новый элемент.
>
> В противном случае ищем место в отсортированном массиве (можно воспользоваться бинарным поиском) для нового элемента и *вставляем* его (сдвигаем вправо все элементы $a$, находящиеся правее)
> В конце концов $a$ за $n$ шагов разрастется до всего массива, тем самым отсвортировав его

>[!tip]- Алгоритм
> 1. Заводим метку `p` и приравнием к 0 (конец отсортированного массива)
> 2. Если `a[i] > a[p]`, то `p++`
> 3. Иначе производим `insertion` `a[i]` в массив `a[:p+1]`, `p++`
> 4. Если `p >= len(a)`, заканчиваем работу

> [!done]- Оценка сложности
> В худшем случае, для обратно отсортированного массива нужно на каждом шаге двигать весь $a$, что занимает квадратичное время
>
> **Сложность**: *$O(n^2)$*


>[!quote]- Код 
> ```Python
> def insertion(a, position):
>     elem = a[position]
>     ind = 0
>     while(a[ind] < elem):
>         ind += 1
>     if(ind >= position):
>         return
>     for i in range(position, ind, -1):
>         a[i] = a[i-1]
>     a[ind] = elem
>     return ind
>     
> def insertion_sort(a):
>     for edge in range(len(a)-1):
>         if(a[edge] > a[edge + 1]):
>             insertion(a, edge + 1)
>     return
> ```

&nbsp;
### Сортировка слиянием (Merge sort) #stable 
Разбиваем на половины и сортируем их
>[!abstract]- Описание 
> Считаем длину массива степенью двойки
> > [!question]- Почему имеем право?
> > Высота дерева рекурсии $$h = \lceil log_2 n \rceil$$
> > Отcюда $$2^{h-1} < n \leq 2^{h}$$
> > Асимптотика у левой и правой части равны, а время работы алгоритма монотонно зависит от длины входа 
> 
> Разбиваем на половины и сортируем их

>[!tip]- Алгоритм
> 1) Если длина массива больше одного разбить массив на два равных
> 2) Рекурсивно отсортировать половины
> 3) **Слить** половины
> 
> Слияние :
> 1) Ставим указатели $l, r$ на начало каждого из двух массивов.
> 2) Берем минимум из двух элементов, перемещаем вперед указатель, который указывал на меньший элемент

>[!done]- Оценка сложности
> - На самом верхнем (нулевом) уровне проиходит слияние, это $cn$ операций.
> - На первом подуровне у нас два процесса, каждый по $\frac{cn}{2}$. Итого на уровне снова $cn$ операций.
> $\dots$
> - На k-м подуровне у нас $2^k$ процессов, каждый по $\frac{cn}{2^k}$. Итого на уровне снова $cn$ операций.
> - На последнем подуровне у нас $2^h = 2^{log_2n} = n$ процессов, каждый работает константное время $d$.
>
>**Сложность:** *$\Theta(cnh) = \Theta(cn\log_2n) = \Theta(nlogn)$*


>[!quote]- Код 
> ```Python
>def merge(a: list, b: list) -> list:
>	i, j = 0, 0
>	res = []
>	while i < len(a) and j < len(b):
>		if a[i] < b[i] :
>			res.append(a[i])
>			i += 1
>		else:
>			res.append(b[j])
>			j += 1
>	return res + a[i:] + b[j:]
 >    
 >def merge_sort(a: list) -> list:
    >    if len(a) <= 1:
    >        return a
    >    ll = len(a) // 2
    >    return merge(merge_sort(a[:ll]), merge_sort(a[ll:]))
> ```


    

- - -
&nbsp;
## Нижние оценки
### Угадай число
Рассмотрим игру: ведущий загадал число $x$ в пределах от $1$ до $N$. Мы можем задавать любые вопросы общего типа (да/нет).  Нужно максимально $x$ за наименьшее число вопросов

Сразу хочется сделать бинарный поиск, где мы гарантированно находим $x$ за $log_2n$ вопросов. Это действительно лучший алгоритм, но пока что мы доказали верхнюю оценку -- нам больше не надо искать алгоритм, работающий дольше чем за $log_2n$, но все еще надо искать более быстрый алгоритм

Докажем, что при любой стратегии игры мы не сможем гарантированно найти $x$ быстрее чем за $log_2n$ запросов. Есть несколько способов доказательства таких оценок

> [!quote]- Доказательство разрешающими деревьями
> Обозначим за $S$ изначальное множество, $S_1$ множество, на которое мы сужаемся при ответе "да" на вопрос,  $S_0$ множество, на которое мы сужаемся при ответе "нет" на вопрос. Далее для множества $S_{0}$ обозначим за $S_{00}$ и $S_{01}$ множества, на которые мы сужаемся при тех же ответах и так далее.
>
> В итоге дерево возможых действий представляет из себя структуру, где из $S$ идут $S_0$ и $S_1$, из $S_0$ идут $S_{01}$ и $S_{00}$ и так далее. Получается полное бинарное дерево, причем в каждом из листьев записан тот или иной ответ (в зависимости от загаданного числа)
> 
> Всего листьев у нас $2^h$. Это дерево не может быть слишком маленьким -- количество ответов не меньше чем количество всех возможных загаданных чисел (иначе если один лист соответствует двум загаданным числам, алгоритм их не различает): $$2^h \geq N \Rightarrow h \geq \lceil \log_2(N) \rceil$$
> 
> Высота разрешающего дерева это количество (максимальное) вопросов, которые мы задаем, то есть как раз нижняя оценка для алгоритма. Получается, что в случае такой игры она не лучше чем  $log_2(N)$, а значит это и есть лучший алгоритм

> [!quote]- Доказательство стратегией противника
> Зная, что алгоритм представляет собой древесную структуру, каждый раз будем выбирать такой ответ на вопрос, который приводит к минимальному уменьшению множества. Ясно, что худший ответ на вопрос уменьшит наше множество не более чем в 2 раза, то есть сузим множество до 1 элемента при худшей игре противника мы не быстрее чем за $log_2n$, а эту оценку мы как раз подкрепили примером

> [! warning]- про адаптивный поиск
> **Адаптивный поиск** -- это когда мы задаем каждый новый вопрос, учитывая предыдущие ответы.
> **Неадаптивный поиск** -- когда не учитываем (сразу задаем все повросы, а потом даем по ним ответ)
> 
> Ясно, что любая неадаптивная стратегия включена в множество адаптивных и работает не быстрее адаптивной. Поэтому *всякая нижняя оценка адаптивного поиска есть нижняя оценка для неадаптивного поиска*

 &nbsp;
### Теорема о нижней оценке сортировок сравнением

> [!note]- Теорема о нижней оценке сортировок сравнением
В худшем случае сортировка сравнениями выполняет $\Omega(nlogn)$ сравнений

> [!quote]- Доказательство
> Каждая сортировка сравнениями запрашивает сравнения в каком-то порядке (причем делает это, как правило, адаптивно, опираясь на предыдущие ответы) в зависимости от текущей перестановки элементов массива. Получается разрешающее бинарное дерево, у которого количество листьев никак не меньше количества перестановок элементов массива (алгоритм должен уметь всячески переставить объекты), то есть $$2^h \geq n! \Rightarrow h \geq log(n!)$$
> При этом для логарифма верно следующее
> $$\begin{cases}
	> log(n!) \leq log(n^n) \leq nlog(n) \\
	> log(n!) \geq log(\frac{n}{2}^{\frac{n}{2}}) \geq \frac{n}{2}(logn - 1)) = \Theta(nlogn)
> \end{cases}$$
> Иными словами, *$log(n!) = \Theta(nlogn)$*

 &nbsp;
### Поиск по монотонной функции
 Теперь займемся следующей задачей: пусть есть монотонная функция 
 $$f: \{1,\dots, n\} \rightarrow \{1,\dots, m\}$$
 Необходимо спрашивая, какое значения функция принимает в той или иной точке, за наименьшее время найти $x: f(x) = y$, то есть понять, гду функция принимает значение $y$
 
 Эта задача уже не бинарная, потому что в данной модели у нас задаются вопросы на которые не 2, а целых m ответов. Однако понятно, что ответ будет также $log_2n$
 
 > [!quote]- Доказательство стратегией противника
 > Аккуратно построим стратегию противника: Во-первых если $y = 1$ или $y = m$, ответ можно получить за один вопрос. Во всех остальных случаях противник действует следующим образом: если мы хотим узнать значение функции в точке $x$, противник хочет направить нас в больший отрезок, при этом сказав меньше информации. Тогда если слева больше неизвестных точек, то ответом будет $m$, иначе $1$. 
 > 
 > В такой стратегии у нас возникает ситуация, очень похожая на бинарный поиск в игре "угадай число", сложность так же *$log_2n$*

&nbsp;
### Подведение итогов
Для нижней оценки работы алгоритмов есть два основных подхода -- либо представить алгоритм как древесную структуру (в той модели, в которой мы работаем) и оценить количество листьев из соображения, что у алгоритма должна быть возможность дойти до всех необходимых исходов, либо оценить стратегию противника, который выдает нам ответы, которые меньше всего продвигают нас к решению задачи  
- - -
&nbsp;
## Сортировки за линию
Во-первых очевидно, что это не просто сортировки сравнением. Происходит либо tradeoff по памяти, либо накладываются дополнительные ограничения

### Сортировка подсчётом (counting sort) 
Считаем количество элементов и переписываем массив
>[!abstract]- Описание
Необходимо отсортировать массив $a = [a_1, a_2, \dots, a_n]$, состоящий из чисел не превосходящих $k$.
Заводим дополнительный массив  `C` длины $k$, состоящий из нулей, и будем хранить в `C[i]` количество чисел $i$ в первоначальном массиве. После этого будет тривиально восстановить отсортированный массив по `С`

>[!tip]- Алгоритм
> 1. Создаем массив `C`
> 1. Проходимся по массиву `A`, прибавляем 1 к `C[a[i]]`
> 2. Записываем `i` в `A` `C[i]` раз

>[!done]- Оценка сложности
Создание массива `C` -- *$\Theta(k)$*
Заполнение массива `C` -- *$\Theta(n)$*
Запись в массив `A` -- *$\Theta(n)$*
> 
> **Сложность**: *$\Theta(n + k)$*
> (как правило $k = O(n)$, тогда сложность равна *$\Theta(n)$*)
> **Память**: *$\Theta(n)$*


>[!quote]- Код 
>```Python
>def count_sort(a, k):
>   c = [0]*len(k)
>   for elem in a:
>       c[elem] += 1
>   i = 0
>   j = 0
>   while i < len(a):
>       if(c[j] > 0):
>           for k in range(c[j]):
>               a[i] = j
>               i += 1
>       j += 1
>   return a
>```

&nbsp;
### Модификация сортировки подсчётом #stable
Считаем количество элементов, не меньших данных, и заполняем массив
>[!abstract]- Описание
> После заполнения массива `C` будем модифицировать его так, чтобы в `C[i]` хранилось не количество элементов `i`, количество элементов не превышающих `i`. Для этого сделаем следующее
> ```Python
> 
> for i in range(1, len(c)):
> 	c[i] += c[i-1]
> ```
> Теперь нам осталось положить в новый массив каждый элемент на свое место. То есть на тот номер, сколько элементов не превосходит данный, или на $C[A[i]]$
> 
> *Зачем так делать?*
> Теперь алгоритм стал устойчивым, потому что мы вставляем элементы в `B` в том же порядке, в котором они находились в `A`

>[!tip]- Алгоритм
> 1. Проходимся по массиву `A`, заполняем `C`
> 2. Модифицируем массив `C`, прибавляя к каждому элементу предыдущий
> 3. Заполняем новый массив `B`, ставя `A[i]` на место под номером `C[A[i]]`
> 4. Декрементируем `C[A[i]]`

>[!done]- Оценка сложности
та же

>[!quote]- Код 
> ```Python
> 
> def count_sort(a, k):
>     c = [0]*k
>     print(c)
>     for elem in a:
>         c[elem] += 1
>     for i in range(1, len(c)):
>         c[i] += c[i-1]
>     b = [0]*len(a)
>     print(c)
>     for i in range(len(a)-1, -1, -1):
>         b[c[a[i]]-1] = a[i]
>         c[a[i]] -= 1
>     return b
> ```

&nbsp;
### Поразрядная сортировка (Radix sort) #stable  
Последовательно сортируем по каждому разряду, начиная с меньшего
>[!abstract]- Описание
> Будем смотреть на запись числа в какой-то системе счисления (допустим, двоичная)
> 
> Пусть все числа не превосходят `d`, в нашей системе счисления умещаются в `m` разрядов и записаны следующим образом
> $$
> \begin{cases}
> a[0] = \overline{a_{m}^{0}a_{m-1}^{0}\dots a_{1}^{0}a_{0}^{0}} \\
> a[1] = \overline{a_{m}^{1}a_{m-1}^{1}\dots a_{1}^{1}a_{0}^{1}} \\
> \dots \\
> a[n] = \overline{a_{m}^{n}a_{m-1}^{n}\dots a_{1}^{n}a_{0}^{n}} \\
> \end{cases}
> $$
> Теперь давайте сортировать по столбцам. Сначала по одному, затем по следующему...
> 
> Если пойдем слева направо, то есть от старших  разрядов к младшим, то возникнет путаница и для получения отсортированного массива нужно будет дополнительно перераспределять числа определенным образом (нужно запоминать границы групп чисел с одним значением разряда, и перекидывать последующие результаты между границами этих групп)
> 
> К счастью, если сортировать начиная с младших разрядов, то всё супер. Звучит логично -- сначала разбираемся с единицами, чтобы число с девяткой на конце стояло раньше числа с единицой на конце. И в последствии ставим раньше те числа, где больше сотен или тысяч 
> 
> Отличная новость в том, что сортировка *устойчива*, что вполне очевидно

>[!tip]- Алгоритм
> 1. Сортируем числа по нулевому разряду
> 2. Сортируем числа по первому разряду
> 3. Сортируем числа по второму разряду
> 4. ...

>[!done]- Оценка сложности
> Сколько стоит каждая сортировка? Применяя описанную выше сортировку подсчетом, получим *$\Theta(n+d)$* 
> Таких сортировок у нас будет столько, сколько есть разрядов у наших чисел в $X$-ичной системе счисления, то есть *$log_X(max(A[i]))$*
> 
> **Сложность**: *$\Theta(m(n+d)) = \Theta(log_X(max(A[i])))(n+d)$*
> **Память**: *$\Theta(n)$*

>[!quote]- Код 
>```Python
>pass
>```
- - -

&nbsp;
## Простейшие структуры данных
### Абстрактный тип данных VS Структура данных
*Абстрактный тип данных* -- описание структуры данных на абстрактном уровне. Например, стек -- это нечто, у чего есть `push`, добавляющий элемент,  `pop`, убирающий элемент, `empty`, проверяющий на пустоту. То есть это вербальное описание, в котором нету ни слова про реализацию в языке программирования

По сути абстрактный тип данных позволяет нам пользоваться написанными кем-то структурами данных, знать их интерфейс и вообще не задумываться, как оно работает под капотом. Это про теорию
&nbsp;
*Структура данных* -- непосредственная реализация абстрактного типа данных. Параллельно ли вычислять ответ? Как оптимальнее реализовать метод? Как хранить данные?

Это тот код, который мы запускаем. Две структуры данных могут реализовывать один и тот же тип данных, но делать это соверенно по-разному (именно поэтому твой стек будет работать в 5 раз медленнее библиотечного)

&nbsp;
### Стек
>[!abstract]- Описание
>*LIFO* -- добавляем и забираем элементы сверху. Логичнее всего реализовывать списком, потому что не нужно обращаться к какому-то участку памяти, кроме головы

>[!tip]- Набор методов
> - `push(x)`  *$O(1)$*
> - `pop()`  *$O(1)$*
> - `empty()`  *$O(1)$*


> [!quote]- Код 
>```Python
>pass
>```

&nbsp;
### Очередь
>[!abstract]- Описание
>*FIFO* -- добавляем в один конец, забираем из другого. Так же логичнее всего реализовывать списком, но теперь держим указатель на оба конца

>[!tip]- Набор методов
> - `push(x)`  *$O(1)$*
> - `pop()`  *$O(1)$*
> - `empty()`  *$O(1)$*

>[!quote]- Код 
>```Python
>pass
>```

&nbsp;
### Связный список
>[!abstract]- Описание
> Однонаправленный содержит только указатели на следующие элементы.
> Двунаправленный еще и на предыдущие

>[!tip]- Набор методов
> - `delete(x*)` *$O(1)$*
> - `insert(x*)` *$O(1)$*
> - `search(x)` *$O(n)$*
> - `head()` *$O(1)$*
> - `tail()` *$O(1)$*

>[!quote]- Код 
>```Python
>pass
>```

&nbsp;
### Массив
>[!abstract]- Описание
> Цельный кусок памяти, на которой выделяется памяти с запасом, но в случае переполнения придется выделять больший блок памяти и долго копировать массив. Зато мгновенно имеем доступ по индексу

>[!tip]- Набор методов
> - `delete(a[k])` *$O(n-k+1)$*
> - `insert(a[k])` *$O(n-k+1)$*
> - `search(x)` *$O(1)$*

>[!quote]- Код 
>```Python
>pass
>```


&nbsp;
### Дерево
>[!abstract]- Описание
> Структура, у которой есть `tree_node` корень, а у каждого корня указатели на детей и, возможно, предка
>
> > [!tip]- Хранение дерева в массиве
> > Можно хранить элементы `k`-ичного дерева в плоском виде в массиве. Тогда родитель `a[i]` это `a[i//k]`, а его дети это `a[i*k], a[i*k+1],... a[i*k+k-1]`
> > 
> > Все необходимые операции выполняются за *$O(1)$*, а память хранится цельным блоком

>[!tip]- Набор методов
>```Python
>pass
>```


>[!quote]- Код 
>```Python
>pass
>```

&nbsp;
### Куча (heap)
>[!abstract]- Описание
> Почти полное бинарное дерево (неполнота может быть только на последнем уровне и только справа), обладающее *максимальным свойством*.
> 
> *Максимальное свойство* -- это когда каждый элемент максимум в своем поддереве (то есть элементы чем выше, тем больше)


>[!tip]- Набор методов
> - `add(x, priority)` *$O(logn)$*
> - `extract_max()` *$O(logn)$*
> - [ `change_priority(x, new_priority)` *$O(logn)$* ]
> 
> Добавляем элемент снизу справа (чтобы сохранить почти полноту). Если он больше предка, рекурсивно поднимаем его вниз, а предка опускаем. Очевидно, что это займет *$O(logn)$* операций
> 
> При удалении максимума у дерева убирается корень, и чтобы не поломать структуру на его место ставится крайний нижний правый элемент. Теперь чтобы разрешить максимальное свойство, нужно, наоборот, спускаться сверху вниз, меняя его местами с максимальным ребенком, пока он больше чем этот элемент. Аналогично, *$O(logn)$* операций
> 
> При смене приоритета коллизия может возникнуть в одном месте (там, где приоритет поменялся). Если элемент стал меньшего какого-то из детей, то разрешаем коллизию вниз, как мы делали. Иначе если элемент стал больше родителя, разрешаем коллизию вверх

>[!quote]- Код 
>```Python
>pass
>```

&nbsp;
### Двоичное дерево поиска
>[!abstract]- Описание
Все ключи уникальны. Левое поддерево каждого элемента $x$ содержит все элементы, меньшие $x$ и наоборот

>[!tip]- Набор методов
> - `add(x)` *$O(h)$*
> - `search(x)` *$O(h)$*
> - `delete(x)` *$O(h)$*
> 
> Высота дерева $h$ может достигать $n$ в случае бамбука, поэтому при добавлении и удалении дерево надо поворачивать и ребалансировать. Для этого мы пользуемся красно-черным деревом

>[!quote]- Код 
>```Python
>pass
>```

&nbsp;
### Красно-черное дерево
>[!abstract]- Описание
> 1. Каждая вершина либо красная, либо черная
> 2. Каждый пустой ребенок виртуально считается черным
> 3. Корень черный
> 4. В любом пути от корня до листа одинаковое количество черных вершин
> 5. Оба ребенка красной вершины черные
> 
>  Красно черное дерево сбалансировано, поэтому *$h = \Theta(log(n))$*
>  За всем остальным простое бинарное дерево


>[!tip]- Набор методов
>```Python
>pass
>```

>[!quote]- Код 
>```Python
>pass
>```

- - -

&nbsp;
## Хеширование
Хочется уметь обращаться к объектам за $O(1)$, но не хранить для каждого объекта уникальный идентификатор (если объектов потенциально миллиард, то у нас выделено на идентификаторы миллиард ячеек памяти). 

Для этого разобьем все объекты на $m$ классов эквивалентноси по какому-то параметру (например, по первой букве имени или по трем последним цифрам). Тогда для поиска каждого элемента нужно будет вызвать поиск в той структуре данных, которая соответствует значению хеша, в ней элементов будет в $m$ раз меньше, что существенно ускорит процедуру

При этом в худшем случае порядок поиска не изменится, однако на практике при логичных допущениях можно считать время поиска константным

### Коллизия

*Коллизия* -- это когда два элемента имеют один и тот же хэш. Неизбежное явление, но хотелось бы такого поменьше, чтобы все элементы легли как можно равномернее среди всех значений хеша. Определим семейство *универсальных хеш-функций* $H$ - всех тех функций, у которых $P(f(x) = f(y)) = \frac{1}{M}$ для любых $x,y$

- - -

Решим задачу для IP-адресов. Их может быть сколь угодно много, но всего компьютеров в нашей сети порядка 255. Каждый IP-адрес -- это 4 байта $$b_1\;b_2\;b_3\;b_4$$
Попробуем уложить значения хеш-функции в один байт. Возьмем и умножим каждый байт адреса на какое-то значение (по сути сделаем скалярное произведение на некоторый вектор) 
$$a_1b_1 + a_2b_2 + a_3b_3 + a_4b_4$$
Но это число может быть довольно большим, поэтому возьмем его по модулю $M$
$$a_1b_1 + a_2b_2 + a_3b_3 + a_4b_4\;(mod M)$$
В качестве $M$ возьмем не 255, а ближайшее простое число (257)

Итого: хеш-функция задается числами $a_1\;a_2\;a_3\;a_4$, каждое из которых меньше какого-то простого $M$. Докажем, что $\mathbb{P}(\sum a_ix_i = \sum a_iy_i) = \frac{1}{M}$, то есть что эта функция есть универсальная хеш-функция
>[!quote]- Доказательство
> $x$ и $y$  разные числа, а значит отличаются хотя бы в одном разряде. Пусть они отличаются в 4-ом байте. Тогда перепишем равенство заметив, что $x_4 - y_4 \neq 0$
> $$
> a_4(x_4 - y_4) = \sum\limits_{i=1}^3a_i(y_i - x_i)
> $$
> Представим, что значение в правой части зафиксировано и равно $R$. Сколько $a_4$ подходит нам для того, чтобы равенство было справедливо? Ровно одно, потому что относительно $a_4$ это линейное уравнение по модулю $M$ и его решение есть
> $$a \equiv_{M} (x_4 - y_4)^{-1}R$$
> Мы доказали, что $\mathbb{P}(a_4 \equiv R\;|\;a_1, a_2, a_3) = \frac{1}{M}$. Это условная вероятность. Давайте возьмем полную вероятность
> $$
> \sum\limits_{i=1}^3\mathbb{P}(a_4 \equiv R\;|\;a_1, a_2, a_3) * \mathbb{P}(a_1, a_2, a_3) = \frac{1}{M} * 1 = \frac{1}{M}
> $$

- - - 

Вычислим матожидание коллизии (длину цепочки).

Пусть $C_i$ - случайная величина, равная длине $i$-ой цепочке в нашем хеше. Заметим, что если $I_j^i$ -- случаная величина, показывающая, попал ли $j$-ый элемент в $i$-ый ящик. Очевидно, что $\mathbb{E}(I_j^i) = \frac{1}{M}$ и что
$$C_i = I_1^i + I_2^i + \dots + I_n^i \Rightarrow$$
$$\Rightarrow \mathbb{E}(C_i) = \mathbb{E}(I_1^i) + \mathbb{E}(I_2^i) + \dots + \mathbb{E}(I_n^i) = \frac{n}{M} = \alpha$$
Число $\alpha$ очень полезно, его называют *коэффициентом заполнения*

## Графы

Рассмотрим графы поподробнее -- будем считать, что вершины $V$ просто пронумерованы: $V = \{1,2,\dots n\}$

Как хранить граф? Самый просто способ -- это матрица смежности. Да, мы за константное время находим ребро, но это **квадратичная** трата памяти. Если граф *плотных* графов (много ребер) этот способ хорош, но не для *разреженных* (ребер мало)

Поэтому используем *списки смежности* -- у каждой вершины есть список вершин, в которые можно из нее пойти. Теперь памяти мы тратим $O(|V|)$, но узнаём, есть ли ребро за $O(|E|)$

Далее нам пригодится ориентированный ациклический граф, будем называть его *DAG*

Рассмотрим алгоритм обхода графа – поиск в глубину

### Поиск в глубину 

>[!abstract]- Описание
> Пусть дан граф $G(V,E)$. Надо обойти все вершины графа, начиная с некоторой вершины $x$
> - - -
> Пусть у каждой вершины $u$ есть время открытия $d[u]$ и время закрытия $f[u]$. Начиная обход, обозначим $d[x]$ за `1`. Далее открываем вершины $c$, смежную с $x$, которая еще не была открыта и открываем ее (увеличиваем счетчик шага, устанавливаем $d[c]$ равное увеличенному шагу), после этого запускаем поиск в глубину из только что открытой вершины $c$
> 
> Если у нас нет не открытых вершин, смежных с данной, то мы закрываем данную вершину (увеличиваем счетчик шага, устанавливаем $f[x]$ равное этому шагу)
> 
> Есть шанс, что мы начнем в *стоке*, то есть в листе, или в какой-то неподходящей вершине, и не откроем весь граф. Тогда после завершения поиска надо проверить, остались ли неоткрытые вершины, и если остались запустить поиск из любой такой вершины

>[!tip]- Алгоритм
>1. Создать глобальные массивы $d$, $f$, счетчик времени $t$
>2. Запустить глобальный алгоритм
>
>  **Глобальный алгоритм над графом**
>  1. Создать очередь, состоящую из всех вершин графа
>  2. Пока очередь не опустеет, вытаскивать вершину и, если она не открыта, запускать локальный алгоритм
> 
> **Локальный алгоритм над вершиной**
> 1. Устанавливаем время открытия вершины,
> 2. Запускаем локальный алгоритм для всех неоткрытых детей этой вершины
> 3. Закрываем вершину

>[!done]- Оценка сложности
>
>По каждому ребру мы "проходим" не более двух раз (если по ребру происходил рекурсивный вызов, то мы пройдемся туда и обязательно вернемся назад). Всех остальных операций у нас $O(1)$. Получается, для каждой вершины мы делаем не более $O(|E|)$ операций. Получается квадратичный алгоритм? Нет, он линейный
>
>*Исходящая степень вершины* $d_+v$ -- это количество ее детей. Из теории графов очевидно, что 
>$$
>\sum d_+v_i = |E|
>$$
>Для каждой вершины мы делаем не $O(|V|)$ операций, а $O(d_+(v))$, что в сумме по всем вершинам дает нам сложность переходов порядка *$O(|E|)$*. А еще в глобальном алгоритме мы работаем с очередью из всех вершин, поэтому итоговая сложность ***$O(|V| + |E|)$***

>[!quote]- Код
> ```Python
> 
> t = 0
> d[u] = []
> f[u] = []
> for i in range(len(V)):
> 	d[u].append(0)
> 	f[u].append(0)
> 
> def dfs(u): # локальнйы алгоритм для каждой вершины
> 	d[u] = ++t;
> 	for v in adj(u) # список смежности
> 		if d[v] == 0: # если вершина не открыта
> 			dfs(v)
> 	f[u] = ++t;
> 	
> def DFS(G): # глобальный алгоритм для графа
>     q = queue(V) # очередь на основе V
>     while not q.is_empty():
>         u = q.extract() # извлекаем вершину
>         if(d[u] == 0):
>             dfs(u) 
> ```

Заметим, что при поиске в глубину всегда получается лес. В случае, если мы правильно подобрали вершину, строится *остовное дерево*.

- - -

Для дальнейших рассуждений введем классификацию ребёр, которая полностью разбивает их на 4 класса

*рёбра дерева* -- рёбра, которые лежат в дереве обхода в глубину

*прямые рёбра* --  ведущие от предка дерева к потомку
> [! question]- Как опеределить?
> $u \rightarrow v$ – прямое ребро, если $$f[u] > f[v]\;\;\text{и}\;\;d[u] < d[v]$$

*обратные рёбра* -- ведущие от потомка дерева к предку
> [! question]- Как опеределить?
> $u \rightarrow v$ – обратное ребро, если $$f[u] < f[v]\;\;\text{и}\;\;d[u] > d[v]$$

*перекрёстные рёбра* -- все остальные
> [! question]- Как опеределить?
> $u \rightarrow v$ – перекрестное ребро, если $$f[u] < d[v]\;\;\text{или}\;\;f[v] < d[u]$$
- - -

Заметим, что если мы сопоставим каждому шагу открытия открывающую скобку, а каждому шагу закрытия закрывающую, то хронология открытия-закрытия вершин превращается в *правильную скобочную последовательность*. Это еще пригодится нам

Напомним, что *Эйлеров цикл* в графе это цикл, содержащий все рёбра по одному разу

Введем две леммы

> [!note]- Лемма 1

> [!quote]-  Доказательство

 &nbsp;
> [!note]- Лемма 2
> Пусть вершина $u$ поиском в глубину была открыта первой в своей компоненте сильной связности. Тогда для любой другой вершины $v$ той же компоненты $$d[u] < d[v] < f[v] < f[u]$$

> [!quote]-  Доказательство
> ```Python
> 
> pass
> ```

- - -

### Топологическая сортировка

Каждый *DAG* в каком-то смысле задаёт отношение порядка. Это легко понять, представив структуру пакетов в линукс – у пакетов есть зависимости, один зависит от одного, другой зависит от многих, но есть такой пакет, который не зависит ни от кого и с которого эта цепочка начиналась. После того, как мы обновили пакет, хорошей идеей будет обновить все зависящие от него пакеты, пройдясь по графу. Для быстрого доступа к этой функции граф обычно хранят так, чтобы "следующая" вершина графа зависела от предыдущей. Этим и занимается тологическая сортировка.  

>[!abstract]- Описание
>Составим такой порядок на вершинах DAG, что все рёбра будут идти от элементов с меньшим номером к элементам с больим номером (заодно увидим, что это всегда для DAG возможно)
>
> - - -
>
> Для этого начинаем поиск  вглубину из случайной вершины. Из леммы 1 (и здравого смысла) напрямую следует, что первая закрытая вершина будет стоком (из нее не будет выходить вершин). Далее будут закрыты вершины, которые либо сами стоки, либо чьи ребра ведут в стоки и так далее по цепочке. 
> 
> Таким образом, нам подходит тот порядок вершин, который получается при обходе в глубину, поэтому можно просто печатать или закидывать в стек вершины, как только мы выходим из них в DFS





>[!tip]- Алгоритм
>1. Красим все вершины  в белый.
>2. Для каждой вершины выполняем локальный алгоритм
> 
> **Локальный алгоритм**
> 1. Если вершина черная, то ничего делать не нужно
> 2. Если вершина серая, найден цикл, поэтому сортировки быть не может
> 3. Если вершина белая, то красим ее в серый, запускаем локальный алгоритм для всех ее детей. После чего красим вершину в черный и помещаем в начало отсортированного списка

>[!done]- Оценка сложности

>[!quote]- Код 
