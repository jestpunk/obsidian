# DL
#ml #ozon #prog #course | [[ml]] [[ozon]] [[prog]] [[course]]
Курс Дьяконова
- - -

&nbsp;

## Нейронные сети

Рассмотрим то, что мы будем называть *нейрон* – это структура, получающая на вход признаки $(x_1, x_2, \dots,x_n)$ и фиктивный признак 1 с какими-то весами $(w_1, w_2, \dots, w_n, b)$, аккумулирующая линейную комбинацию признаков, домноженных на веса, и передающая ее дальше через *функцию активации* $\sigma$.

![[Pasted image 20220922002326.png]]

Таким образом, если функция активации тождественна, то это просто *линейная регрессия*. Если она *сигмоида*, то мы получаем *логистическую регрессию*, возвращающую вероятность принадлежности к классу, а если мы задаемся каким-то пороговым значением, то получаем *линейный классификатор*. Нейрон получился отличным обобщением линейных моделей.

**Функции активации**

1) Тождественная функция $$f(z) = z;$$$$f'(z) = 1$$
2) Пороговая функция $$f(z) = \mathbb{I}(z > 0);$$$$f'(z) = 0$$
3) Сигмоида $$\sigma(z) = \frac{1}{1 + e^{-z}}\in(0,1);$$$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
4) Гиперболический тангенс (по сути сигмоида со сдвигом)$$tanh(z) = \frac{2}{1+e^{-2z}} + 1 \in (-1,1)$$$$tanh'(z) = 1 - tanh^2(z)$$
5) softmax (для множества вводов)$$softmax(z_1, z_2,\dots,z_n) = \frac{(exp(z_1), exp(z_2), \dots, exp(z_n))}{(exp(z_1)+ exp(z_2)+ \dots+ exp(z_n)}$$Плюс этой функции в том, что она переводит все числа в положительные в сумме дающие 1, то есть в вероятности, и сохраняет между числами отношение порядка (максимум остается максимумом)

С точки зрения булевой алгебры логики, нейрон может реализовать логическое *И* и логическое *ИЛИ* (просто разделив булев куб соответствующей плоскостью), а так же логическое *НЕ*. Значит, с помощью каскада нейронов можно реализовать любую булеву функцию

Теперь сделаем уже *нейронную сеть*. Пусть у нас есть два признака, и мы подаём их на вход двум нейронам. Они получают линейную модель и пускают дальше сигнал от 0 до 1 (скажем, сглаженый сигмоидой). Оба сигнала получает конечный нейрон, который либо отдаёт результат уже своей линейной модели, и тогда мы решаем задачу регрессии, либо так же сглаживает его, и тогда это будет классификация.

![[Pasted image 20220922134534.png]]

Разумеется, на первый слой можно было наложить и больше нейронов, тогда понятно как обобщалась бы итоговая функция

Через матрицы это тоже записывается вполне симпатично
$$\sigma
\begin{pmatrix}
	\begin{bmatrix}
		w_{31} & w_{32} & b
	\end{bmatrix}
	\begin{bmatrix}
		\sigma
		\begin{pmatrix}
			\begin{bmatrix}
				w_{11} & w_{12} & b_1\\
				w_{21} & w_{22} & b_2
			\end{bmatrix}
			\begin{pmatrix}
				x_1\\
				x_2\\
				1
			\end{pmatrix}
		\end{pmatrix}\\
		1
	\end{bmatrix}
\end{pmatrix}$$

Существует развязывающая нам руки *теорема Хорника*
>[!info]- Теорема Хорника
>Любую непрерывную функцию можно сколь угодно приблизить нейросетью глубины 2, у которой на скрытом слое сигмоидная функция активации и линейная функция на выходе

То есть получается, что нагромоздив много нейронов (линейных моделей) мы можем строить модели любой сложности, но
1) может потребоваться много нейронов
2) веса могут расти экспоненциально
3) такое сложно обучить

Поэтому нечего ограничиваться пусть и покрывающими все функции, но не самыми удобными двухслойными сетями – пойдем в глубину

![[Pasted image 20220922140005.png]]

Получаем ориентированный граф. Такой граф называется *граф вычислений*. Если в нём нет циклов, он называется *Feedforward network*, или *сеть прямого распространения*. Связь в такой сети каждый с каждым, то есть все нейроны одного слоя отдают сигналы во все нейроны следующего

Стоит понимать, что нейросеть это, по сути, последовательное, слой за слоем, преобразование пространства. Каждый следующий слой не видит исходные признаки, он работает в уже немного другом признаковом пространстве.

Чтобы записать нейросеть в совсем красивом функциональном виде, откажемся от фиктивного единичного признака. Мы можем считать, что он подаётся на вход вместе с остальными признаками, но даже если этого не делать, при необходимости нейросеть сама ссимулирует этот свободный член и научиться его воспроизводить. Короче говоря, без него формула будет выглядить вообще супер красиво
$$\varphi_k(W_k\cdot\dots\varphi_2(W_2\cdot\varphi_1(W_1\cdot x)))$$

&nbsp;

### Обучение нейросети
Точно так же как и раьше – минимизируем эмпирический риск с регуляризацией
$$\frac{1}{n}\sum\limits_i L(a(x_i\;|\;w), y_i) + \lambda R(w) \rightarrow \min\limits_w$$
Из-за того, что в хорошей глубокой сети будет очень много слагаемых, логичнее всего оптимизировать эту сумму стохастическим *градиентным спуском*

Напоминаем, для начала мы можем случайно инициализировать веса (заметим, что нужно их именно рандомизировать, потому что если мы возьмем, например, все нулевые, то может быть ситуация, когда все нейроны в слое будут изменяться синхронно)
$$w^{(0)}\sim N(0,\sigma^2)$$а затем итеративно выбирать случайное слагаемое, считать по нему градиент и адаптировать веса с опеределенным learning rate

Есть вещи с которыми нужно быть аккуратными. Например, если мы решаем задачу классификации, и на выходе у нас softmax, то в качестве функции потерь можно увидеть кросс-энтропию
$$L((a_1,\dots,a_n),y) = -\log\frac{exp(a_y)}{\sum exp(a_j)}$$То есть если на выход мы выдали значения для классов $a_1,a_2,\dots$, а правильный класс $y$, то берем экспоненту от ответа на этот класс (от вероятности принадлежности к этому классу) и делим на сумму остальных под логарифмом.

Проблема в том, что наши ответы могут быть, скажем 100, 10 и 1, и тогда эксопненты от этих ответов будут очень сильно отличаться, вызывать *переполнение* и так далее... Поэтому можно, например, сокращать всю дробь на самую большую степень логарифма. Теперь появился риск зануления, но да и бог с ним, потому что зануляемые классы действительно на порядок менее значимы (в 100 раз) чем интересующие нас

&nbsp;


### Обратное распространение ошибки
Хотим решать задачу градиентным спуском, а значит считать производную от лосса. Но там стоит сложная функция, и если мы вспомним, что вывод нейросети это композиция нескольких функций, изменяющих наше признаковое пространство, производная легко раскладывается на произведение производных$$L = \sum(loss(y_i - f(x_i,w))) \sim L(f_k(\dots f_2(f_1(w))))$$ Как выполнен алгоритм оптимизации – сначала идет *прямое распространение*, когда мы идем от начала к концу, считаем ответ и лосс$$x,w \rightarrow L(w,x,f_k(\dots),\dots,f_1(\dots))$$
А затем идет *обратное распространение*, или *backpropagation*, где мы вычисляем градиент. А для этого, как мы выяснили, нужно знать градиенты предыдущих функций. Значит перерасчет идет как бы из конца в начало. По сути это комбинирование SGD и дифференцирования сложных функций (функция, еще раз, сложная, из-за вложенности нейронных слоёв)

На картинке всё совсем наглядно. Для каждой отдельной вершины мы получаем частичное произведение сложной производной, и для вычисления той или иной частной производной пускаем обратный сигнал дальше в нужную вершину, домножая его на локальную, свою собственную производную

![[Pasted image 20220922201031.png]]

Теперь о размерностях – градиент имеет смысл, если у нас на вход подаётся вектор, а на выход число. Если же и на входе, и на выходе вектор, то надо перемножать не градиенты, а якобианы (матрицы частных производных)

&nbsp;

### ReLU
Есть такая проблема с сигмоидой – её производная почти обнуляется, стоит отойти совсем наделеко от старта. И это проблема, потому что при градиентном спуске мы будем шагать на всё более незначительный шаг. Это называется *градиентное затухание*. Помочь с этим, заменив сигмоиду, нам может популярная функия ReLU

ReLU расшифровывается как *Rectified Linear Unit* – очень сложное название для очень простой формулы $$ReLU(x) = \max(0, x)$$
В принципе функция активации пусть и грубо, делает примерно то же самое что и сигмоида. Как минимум градиент на ней никогда не убывает, считается **намного** быстрее чем постоянное вычисление экспоненты.

Однако тот факт, что на половине значений функция зануляет градиент тоже может быть опасен. Есть такой термин, как *мёртвые нейроны*. Это нейроны, выдающие ноль на любом объекте. ReLU способствует их появлению, и хоть вероятность возникновению мёртвого нейрона мала, и с обучением они устраняются, в глубоких сетях это может стать проблемой (её решают на уровне построения архитектуры нейросети).

Если дорабатывать идеи ReLU, можно обратиться к *LeakyReLU*$$LeakyReLU(x) = \max(0.1\cdot x,\;x)$$
Или к *Exponential Linear Unit* aka *ELU*$$ELU(x) = \begin{cases}x,\;\;\;\;\;\;\;\;\;\;\;\;\;\;x\geq0\\\alpha(e^x-1),\;\;x<0\end{cases}$$
И скалированной версии *ELU* – *Scaled Exponential Linear Unit* aka *SELU*
$$SELU(x) = \lambda ELU(x)$$


![[Pasted image 20220924224427.png]]

Еще есть функция *swish*$$swish(x) = x\cdot\sigma(\alpha x)$$
![[Pasted image 20220924230659.png]]

На глубоких сетях она показывает качества чуточку (~1%) лучше ReLU
- - -

&nbsp;

## Фишечки
### Нормировка

Для начала данные было бы неплохо *стандартизировать*, то есть вычесть из них эмпирическое среднее и поделить на корень из оценки дисперсии
$$X \rightarrow \frac{X - \mu}{\sqrt{\sigma^2}}$$
Еще можно применить к данным модификацию *метода главных компонент* – *PCA*

![[Pasted image 20220926172006.png]]

&nbsp;

### Иницилиация весов
Несколько советов. Во-первых, надо нарушить симметричность, задать веса *рандомно*, чтобы нейроны были разные. Во-вторых, нельзя допускать *"насыщенность"* нейрона, то есть ситуации когда его выход всегда близок к 1, или наоборот его смерти, когда он всегда нулевой. Для этого надо проследить, чтобы входы на все слои имели одинаковую *дисперсию* (иначе шаги между разными слоями при градиентном спуске будут как бы разного масштаба). При этом смещения нужно подавать нулевые. Если что, нейронка сама их обучит быть какими надо

Чтобы не придумывать велосипед, есть *инициализация Ксавьера*
$$w_{ij}^{(k)} \sim U(-\sqrt{\frac{6}{n_{in}^{(k)} + n_{out}^{(k)}} }, +\sqrt{\frac{6}{n_{in}^{(k)} + n_{out}^{(k)}}})$$
Где $n_{in}^{(k)}$ и $n_{out}^{(k)}$ – количество входных и выходных связей для $k$-го слоя

Идея инициализации Ксавьера следующая – пусть функция активации нечетная, а в нуле идентична тождественной. Тогда если захотеть, чтобы от слоя к слою сохранялась дисперсия значений и градиентов, получим нерешаемую систему уравнений. Однако усредняя значения решений каждого уравнения и выбирая из всех подходящих распределений равномерное (могли взять и нормальное) и нормируя его как надо, получаем эту формулу.

Отсюда понятно, что инициализация Ксавьера не подходит для ReLU, потому что она локально в нуле не выглядит как тождественная

Если же все-таки брать нормальное распределение вместо равномерного, получим *нормальную инициализацию Ксавьера*. Ограничение в поставленное задачи стоит только на матожидание и дисперсию распределения
$$w_{ij}^{(k)}\sim N(0, \frac{2}{n_{in}^{(k)} + n_{out}^{(k)}})$$ 
Еще есть *инициализация Кеминга*
	$$w_{ij}^{(k)} \sim U(-\sqrt{\frac{3}{n_{in}^{(k)}}}, +\sqrt{\frac{3}{n_{in}^{(k)}}})$$
&nbsp;

### Ранняя остановка

Как только качество начало ухудшаться, останавливаем обучение модели. Это было и в GDB, но здесь это прям существенно важный инструмент

&nbsp;

### Мини-батчи
На самом деле нейросети обучаются немного хитрее того что мы говорили. Выбирается не просто случайный объект и по нему делается спуск, а целая группа. Формально если эта группа $I$, градиентный спуск выглядит так
$$w^{(t+1)} = w^{(t)}-\frac{\eta}{|I|}\sum\limits_{i\in I}\nabla(l(a(x_i\;|\;w^{(t)}), y) + \lambda R(w^{(t)}))$$

Понятно, чем это лучше. Градиент не такой случайный. Но самое главное – при обучении на GPU при выборе одного объекта программа будет *однопоточная*. Грех не нагрузить остальные потоки, подключив еще элементы в батч. Можно делать *нормировку по батчам*, а также специально организовывать батчи (например, чтобы они содержали представителей всех классов)

При этом важный эффект – **Увеличение размера батча приводит к тем же результатам, что и уменьшение темпа обучения**. Поэтому не стоит удивляться более медленной сходимости. Если ты купил себе новую видеокарту нужно не только радостно увеличить размер батча из-за увеличения потоков, а еще увеличить темп обучения

Эксперты полагают, что размер батча надо делать очень небольшим, в пределах 32.
Как минимум это связано с тем, что на самом деле нам нужно найти не просто минимум функции, а *плоский* минимум (если он будет острым, значит рядом с ним значения сильно больше. Если вдруг наша выборка хоть немного отличается от реальных данных, качество резко упадет. А плоский минимум стабилен даже при отличающейся выборке). Вот как раз плоские минимумы SGD находит лучше обычного градиентного спуска

К слову, один проход по всем батчам это *эпоха*. И делается именно проход по всем батчам, а не выбор случайного (взяв батч один раз, больше его элементы мы не трогаем до следующей эпохи), что идет вразрез с философией SGD, однако качество становится лучше. После эпохи мы, конечно же, делаем новые батчи. Об этом не стоит забывать, если придется реализовывать алгоритм руками – одни и те же батчи из эпохи в эпоху это плохо

&nbsp;

###  Продвинутая оптимизация

В чём проблема батчей? В том, что каждый батч смотрит не на глобальный минимум, а куда-то в другую сторону. Для частичного решения этой проблемы можно сделать градиент *инертным*, то есть учитывать не только направление батча, а еще и инерцию, и брать их векторную сумму. Саму инерцию, понятное дело, нужно перерасчитывать после появления нового батча

Тогда на каждом шаге веса имеют следующую запись
$$w^{(t+1)} = w^{(t)} - \eta m^{(t+1)}$$
Вектор инерции $m$ задаётся либо стандартным путём
$$m^{(t+1)} = \rho m^{(t)} = \nabla L^{(t)}(w^{(t)})$$
Либо *методом Нестерова*
$$m^{(t+1)} = \rho m^{(t)} + \nabla L^{(t)}(w^{(t)} - \eta m^{(t)})$$

&nbsp;

Следующая существующая эвристика – *Adagrad*, где мы делим градиент на корень из накопленной суммы квадратов градиентов на прошлых шагах. То есть чем больше была производная по компоненте, тем меньше и меньше мы делаем по ней шаг
$$w_i^{(t+1)} = w_i^{(t)} - \frac{\eta}{\sqrt{v_i^{(t+1)}+\varepsilon}}\nabla_iL^{(t)}(w^{}(t))$$
$$v_i^{(t+1)} = v_i^{(t)} + (\nabla_iL^{(t)}(w^{(t)}))^2$$

&nbsp;

Если сделать это умнее, то наверное стоит добавить гиперпараметр – то, на сколько значим для нас текущий градиент по отношению ко всем предыдущим. Получаем метод *RMSprop* aka *Root Mean Squared Propagation*

$$w_i^{(t+1)} = w_i^{(t)} - \frac{\eta}{\sqrt{v_i^{(t+1)}+\varepsilon}}\nabla_iL^{(t)}(w^{}(t))$$
$$v_i^{(t+1)} = \beta v_i^{(t)} + (1 - \beta)(\nabla_iL^{(t)}(w^{(t)}))^2$$


&nbsp;

Также есть метод оптимизации *Adam* aka *Adaptive Moment Estimation*. Это RMSprop + инерция. Тут мы накапливаем и инерцию, и сумму квадратов градиентов

$$m^{(t+1)}_i = \alpha m_i^{(t)} + (1 - \alpha)\nabla_iL^{(t)}(w^{(t)})$$$$v^{(t+1)}_i = \beta v_i^{(t)} + (1 - \beta)(\nabla_iL^{(t)}(w^{(t)}))^2$$
$$w_i^{(t+1)} = w_i^{(t)}-\frac{\eta}{\sqrt{v_i^{(t+1)} + \varepsilon}}m_i^{(t)}$$

И еще есть целая куча обощений и модификаций... Подытоживая, важно знать только одно – подбор алгоритма **очень** важен, и не менее важен подбор гиперпараметров в нем (а они есть во всех рассмотренных алгоритмах оптимизации). На практике почти всегда хорош Adam. Если он по какой-то причине не заработал, можно попробовать SGD + momentum

![[Pasted image 20220926212656.png]]Обзор методов

&nbsp;

### Зашумление градиента
Сейчас так почти никто не делает, но вообще есть удивительно простой способ улучшить качество обучения – зашумить градиент$$g^{(t)} \rightarrow g^{(t)} + N(0,\alpha)$$
Идея примитивная – не просто бухаемся в минимум, а благодаря шуму отступаем туда-сюда проверяем понемногу альтернативы

&nbsp;

### Регуляризация aka Weight decay
В нейросетях тоже есть регуляризация. Правда часто она называется термином *weight decay*. Формула следующая
$$w^{(t+1)} = (1 - \alpha)w^{(t)} - \eta\nabla L^{(t)})(w^{(t)})$$То есть при $\alpha = 0$ имеем SGD, а при $\alpha = \frac{1}{2}$ видим что веса на каждом шаге уменьшаются вдвое.  Реально получается decay. Звучит странно, но на самом деле не просто так мы заговорили про *регуляризацию* – если сделать L2 регуляризацию с параметром $\alpha$, то метод градиентного спуска как раз превратиться в этот decay

&nbsp;

### Max-norm регуляризация
Зачем регуляризовать, если можно тупо ограничивать. Для каждого нейрона можно ограничить сверху норму весов$$\|w_{neuron}\|\leq c$$
Если норма превышает константу, *проецируем* ее так, чтобы не превышала (тут вопрос, не очень понятно что это за проекция)

&nbsp;

### Dropout
Дропаут – это когда мы выбрасываем нейроны из нейросети. Формально это значит, что теперь нейрон всегда выдаёт константный ноль. Зададимся числом $p$ – вероятностью сделать дропаут каждому нейрону. После этого проводим обучение кастрированной сети. На следующей итерации делаем новый дропаут и удаляем другие нейроны. Получается, что мы обучаем не всю сеть сразу, а её подсети.

Формально, обучение превращается в следующую модель
$$y = f(Wx)\cdot m$$
$$m \sim Bernoulli(1 - p)$$


В полносвязных НС принято обнулять все слои, кроме последнего, потому что последний слой это уже ответы, и убирать часть ответов это как-то неправильно

![[Pasted image 20220926232318.png]]

Тест мы будем делать по всей сети, однако результат нужно уменьшать, потому что нейроны, обученные на дропауте, привыкают к маленькой порции входящих сигналов, и при большом количестве входящих узлов будут выдавать большие значения. Поэтому умножаем на соответствующую долю
$$y = (1 - p)f(x)$$
Бонусом получаем оценку на *дисперсию* ответа (смотрим как его шатает в зависимости от выбора нейронов для дропаута)

Почему это работает? Согласно *байесовской теории*, дропаут связан с регуляризацией, агументацией и много чем ещё хорошим. Также дропаут уменьшает *созависимость* совместных нейронов (функционал нейронов не концентрируется в одном месте, а размазывается по сети). А еще есть прямая аналогия с *ансамблированием* (учим кучу мелких моделей, а в конце объединяем их в одну большую модель). Наконец, сеть становится более *робастной*, мы не просто доверяем одному нейрону, а стараемся смотреть на картину широко (потому что "важный" нейрон может быть исключён)

&nbsp;

Иногда вместо дропаута используется *инвертированный дропаут*. Это когда мы вместо того чтобы умножать тест на $(1-p)$, делим на него трейн.

Тогда обучение выглядит так$$y = \frac{1}{1-p}f(Wx)\cdot m$$
А тест так $$y = f(x)$$
То есть во время тестирования ничего специфического не делаем. Например, если случайная величина $m$ убивает половину нейронов ($p = \frac{1}{2}$), то мы увеличиваем выходной сигнал в два раза. Удобно, потому что на тесте не надо ни о чём думать

&nbsp;

Еще одна гигачад альтернатива – *standout*. Идея в том, чтобы подбирать вероятность дропаута отдельной нейросетью исходя из получившейся нейросети$$m \sim Bernoulli(g(Wx))$$
&nbsp;

*DropConnect* – это когда мы убираем не нейроны целиком, а отдельные связи. Математически это эквивалентно домножению на матрицу

![[Pasted image 20220927002419.png]]

$$y = f((W\cdot M)x)$$
&nbsp;

*Гауссовский dropout* – это когда мы не убиваем нейрон (домножая его на 0 или 1, как в распределении Бернулли), а умножаем на коэффициент, распределенный нормально вокруг единицы

![[Pasted image 20220927110818.png]]

Всё это алгоритмы с *неявной регуляризацией*. Мы, дёргая значения весов, ухудшаем обучение, но за счёт этого делаем нашу сеть более робастной и подготовленной к суровым настоящим данным. Тем самым мы избегаем переобучения, и ничего страшного если качество от этого пострадает (метод 1 соседа даёт 100% качества)

&nbsp;

### Обрезка градиента  aka Gradient clipping

Если мы уже начали спускаться в яму, но "попали на утёс", есть шанс улететь очень далеко от точки минимума

![[Pasted image 20220927111739.png]]

Как вариант – укоротить градиент при сохранении направления
$$g^{new} = \frac{\min(\theta, \|g\|)}{\|g\|}g$$
Лучше этим не пренебрегать и пользоваться. Одна строчка в коде, а в итоге решается потенциально неприятная ситуация. При этом клипировать нужно обязательно до корректировки весов и после обратного хода. Первое нужно, чтобы корректировать уже по клипированному градиенту, а второе чтобы было что клипировать

&nbsp;

### Батч-нормализация
Есть проблемка – когда мы корректируем какой-то вес нейрона, мы делаем это, ориентируясь на текущую ситуацию и распределение значений. Однако за эпоху поменяются в том числе нейроны, от которых зависит наш недавно измененный, и получится, что только что сделанные изменения нейрона отражают уже не актуальное состояние нейросети.

Это изменение называется *covariative shift*, и мы хотели бы, чтобы оно было не таким существенным

Как изменить тот факт, что у нас смещаются отношения между объектами? Можно нормировать батч, то есть если $\{x_i\}$ – набор значений нейрона на батче, а $\mu$ и $\sigma^2$  эмпирические среднее и дисперсия, то мы нормируем батч по формуле$$x_i^{new} = \frac{x_i - \mu}{\sqrt{\sigma^2 + \varepsilon}}$$
Правда теперь мы снова накосячили, потому что теперь все значения тусуются около нуля. Ну не беда, будем сдвигать полученный результат линейной функцией с подходящими параметрами
$$x^{new} \rightarrow \alpha x^{new} + \beta$$
Как понять что они подходящие? Засунем их в ту же нейросеть и будем обучать их в том числе! 

Батч-норму нужно использовать в каждом слое, если уж взялся. Иногда можно нормировать до функции активации, иногда после. Это тонкий момент, от которого тоже может зависеть качество, и правильного ответа здесь нет. Профессиональная фишечка – Если слой линейный, а потом идет батч-норма, то делать этот слой нужно без смещения (всё равно батч-норма его смещает)

**Плюсы:**
- Может (в разы) увеличить скорость обучения
- Интересно, что можно убрать дропаут. Они даже могут друг с другом спорить в процессе обучения, так что либо-либо
- Делается некоторая регуляризация
- Можно использовать более глубокие сети и больший шаг обучения
- Меньше чувствительность к инициализации (например, можно не нормироват

&nbsp;
### Аугментации

*Аугментация* есть ни что иное как расширение множества данных. Как правило из существующих данных искусственно моделируем новые данные.

Очевидный пример на картинках: если мы отразим собачку по вертикали, повернем, обрежем, зашумим, изменим контраст, собачка останется собачкой. 
Также можно зашумлять музыку (надо уметь распознавать речь даже на фоне музыки). В тексте можно переставлять слова, делать замену синониму.

Правда стоит понимать, что не все преобразования допустимы. Например, для цифр отражения будут совершенно греховными – "6" превратится в "9"

Аугментация это круто, потому что нейросеть становится устойчивой к нашим преобразованиям. 

Аугментация может быть

- **Простая** – естественная, такая как поворот
- **Агрессивная** – порча объета (накладывание маски, объекта другого класса)
- **Креативная** – симуляция, GANы

Может быть *online* и *offline* аугментация (либо заранее сохраняем на диск увеличенный датасет, либо аугментируем прямо во время воркфлоу)

&nbsp;

*Mixup* – очень простая статья, тем не менее ставшая популярной. Идея в том, чтобы по двум объектам брать их взвешенную линейную комбинацию, то есть для объектов $x_{1},x_{2}$ мы имеем
$$
\begin{cases}
x_{new} = \lambda x_{1}+(1 - \lambda)x_{2}\\
y_{new} = \lambda y_{1}+ (1-\lambda)y_{2}
\end{cases}
$$
Физический смысл в том, что если мы наложили на картинку и собаку, и кота, то нейросеть тоже должна видеть и кота и собаку. Еще есть *cutout*, который отрезает кусок изображения, чтобы нейросеть тренировалась не только по одному элементу картинки. *Cutmix* объединяет эти идеи

![[Pasted image 20221026022101.png]]


Более хитрым способом является *attentive cutmix*. Это когда мы имеем картинку котёнка, учимся его определять, а потом смотрим, по каким конкретно кускам изображения котёнок был распознан (как это делать – позднее). Теперь берем и вставляем эти $k$ важных "кошачьих" кусков в картинку собаки и придаём новой картинке желаемую метку – $\frac{k}{n}$ за кошку и $\frac{n-k}{n}$ за собаку

![[Pasted image 20221026022730.png]]


&nbsp;

*GridMask* это свежая, но простая идея – накладываем маску из черных квадратиков, получаем улучшение

![[Pasted image 20221026023133.png]]












