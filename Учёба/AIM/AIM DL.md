---
banner: "https://losslandscape.com/wp-content/uploads/2019/11/edge-horizon.jpg"
banner_y: 0.78929
---
# DL
#ml #ozon #prog #course | [[ml]] [[ozon]] [[prog]] [[course]]
Курс Дьяконова
- - -

&nbsp;

## Нейронные сети

Рассмотрим то, что мы будем называть *нейрон* – это структура, получающая на вход признаки $(x_1, x_2, \dots,x_n)$ и фиктивный признак 1 с какими-то весами $(w_1, w_2, \dots, w_n, b)$, аккумулирующая линейную комбинацию признаков, домноженных на веса, и передающая ее дальше через *функцию активации* $\sigma$.

![[Pasted image 20220922002326.png]]

Таким образом, если функция активации тождественна, то это просто *линейная регрессия*. Если она *сигмоида*, то мы получаем *логистическую регрессию*, возвращающую вероятность принадлежности к классу, а если мы задаемся каким-то пороговым значением, то получаем *линейный классификатор*. Нейрон получился отличным обобщением линейных моделей.

**Функции активации**

1) Тождественная функция $$f(z) = z;$$$$f'(z) = 1$$
2) Пороговая функция $$f(z) = \mathbb{I}(z > 0);$$$$f'(z) = 0$$
3) Сигмоида $$\sigma(z) = \frac{1}{1 + e^{-z}}\in(0,1);$$$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
4) Гиперболический тангенс (по сути сигмоида со сдвигом)$$tanh(z) = \frac{2}{1+e^{-2z}} + 1 \in (-1,1)$$$$tanh'(z) = 1 - tanh^2(z)$$
5) softmax (для множества вводов)$$softmax(z_1, z_2,\dots,z_n) = \frac{(exp(z_1), exp(z_2), \dots, exp(z_n))}{(exp(z_1)+ exp(z_2)+ \dots+ exp(z_n)}$$Плюс этой функции в том, что она переводит все числа в положительные в сумме дающие 1, то есть в вероятности, и сохраняет между числами отношение порядка (максимум остается максимумом)

С точки зрения булевой алгебры логики, нейрон может реализовать логическое *И* и логическое *ИЛИ* (просто разделив булев куб соответствующей плоскостью), а так же логическое *НЕ*. Значит, с помощью каскада нейронов можно реализовать любую булеву функцию

Теперь сделаем уже *нейронную сеть*. Пусть у нас есть два признака, и мы подаём их на вход двум нейронам. Они получают линейную модель и пускают дальше сигнал от 0 до 1 (скажем, сглаженый сигмоидой). Оба сигнала получает конечный нейрон, который либо отдаёт результат уже своей линейной модели, и тогда мы решаем задачу регрессии, либо так же сглаживает его, и тогда это будет классификация.

![[Pasted image 20220922134534.png]]

Разумеется, на первый слой можно было наложить и больше нейронов, тогда понятно как обобщалась бы итоговая функция

Через матрицы это тоже записывается вполне симпатично
$$\sigma
\begin{pmatrix}
	\begin{bmatrix}
		w_{31} & w_{32} & b
	\end{bmatrix}
	\begin{bmatrix}
		\sigma
		\begin{pmatrix}
			\begin{bmatrix}
				w_{11} & w_{12} & b_1\\
				w_{21} & w_{22} & b_2
			\end{bmatrix}
			\begin{pmatrix}
				x_1\\
				x_2\\
				1
			\end{pmatrix}
		\end{pmatrix}\\
		1
	\end{bmatrix}
\end{pmatrix}$$

Существует развязывающая нам руки *теорема Хорника*
>[!info]- Теорема Хорника
>Любую непрерывную функцию можно сколь угодно приблизить нейросетью глубины 2, у которой на скрытом слое сигмоидная функция активации и линейная функция на выходе

То есть получается, что нагромоздив много нейронов (линейных моделей) мы можем строить модели любой сложности, но
1) может потребоваться много нейронов
2) веса могут расти экспоненциально
3) такое сложно обучить

Поэтому нечего ограничиваться пусть и покрывающими все функции, но не самыми удобными двухслойными сетями – пойдем в глубину

![[Pasted image 20220922140005.png]]

Получаем ориентированный граф. Такой граф называется *граф вычислений*. Если в нём нет циклов, он называется *Feedforward network*, или *сеть прямого распространения*. Связь в такой сети каждый с каждым, то есть все нейроны одного слоя отдают сигналы во все нейроны следующего

Стоит понимать, что нейросеть это, по сути, последовательное, слой за слоем, преобразование пространства. Каждый следующий слой не видит исходные признаки, он работает в уже немного другом признаковом пространстве.

Чтобы записать нейросеть в совсем красивом функциональном виде, откажемся от фиктивного единичного признака. Мы можем считать, что он подаётся на вход вместе с остальными признаками, но даже если этого не делать, при необходимости нейросеть сама ссимулирует этот свободный член и научиться его воспроизводить. Короче говоря, без него формула будет выглядить вообще супер красиво
$$\varphi_k(W_k\cdot\dots\varphi_2(W_2\cdot\varphi_1(W_1\cdot x)))$$

&nbsp;

### Обучение нейросети
Точно так же как и раьше – минимизируем эмпирический риск с регуляризацией
$$\frac{1}{n}\sum\limits_i L(a(x_i\;|\;w), y_i) + \lambda R(w) \rightarrow \min\limits_w$$
Из-за того, что в хорошей глубокой сети будет очень много слагаемых, логичнее всего оптимизировать эту сумму стохастическим *градиентным спуском*

Напоминаем, для начала мы можем случайно инициализировать веса (заметим, что нужно их именно рандомизировать, потому что если мы возьмем, например, все нулевые, то может быть ситуация, когда все нейроны в слое будут изменяться синхронно)
$$w^{(0)}\sim N(0,\sigma^2)$$а затем итеративно выбирать случайное слагаемое, считать по нему градиент и адаптировать веса с опеределенным learning rate

Есть вещи с которыми нужно быть аккуратными. Например, если мы решаем задачу классификации, и на выходе у нас softmax, то в качестве функции потерь можно увидеть кросс-энтропию
$$L((a_1,\dots,a_n),y) = -\log\frac{exp(a_y)}{\sum exp(a_j)}$$То есть если на выход мы выдали значения для классов $a_1,a_2,\dots$, а правильный класс $y$, то берем экспоненту от ответа на этот класс (от вероятности принадлежности к этому классу) и делим на сумму остальных под логарифмом.

Проблема в том, что наши ответы могут быть, скажем 100, 10 и 1, и тогда эксопненты от этих ответов будут очень сильно отличаться, вызывать *переполнение* и так далее... Поэтому можно, например, сокращать всю дробь на самую большую степень логарифма. Теперь появился риск зануления, но да и бог с ним, потому что зануляемые классы действительно на порядок менее значимы (в 100 раз) чем интересующие нас

&nbsp;


### Обратное распространение ошибки
Хотим решать задачу градиентным спуском, а значит считать производную от лосса. Но там стоит сложная функция, и если мы вспомним, что вывод нейросети это композиция нескольких функций, изменяющих наше признаковое пространство, производная легко раскладывается на произведение производных$$L = \sum(loss(y_i - f(x_i,w))) \sim L(f_k(\dots f_2(f_1(w))))$$ Как выполнен алгоритм оптимизации – сначала идет *прямое распространение*, когда мы идем от начала к концу, считаем ответ и лосс$$x,w \rightarrow L(w,x,f_k(\dots),\dots,f_1(\dots))$$
А затем идет *обратное распространение*, или *backpropagation*, где мы вычисляем градиент. А для этого, как мы выяснили, нужно знать градиенты предыдущих функций. Значит перерасчет идет как бы из конца в начало. По сути это комбинирование SGD и дифференцирования сложных функций (функция, еще раз, сложная, из-за вложенности нейронных слоёв)

На картинке всё совсем наглядно. Для каждой отдельной вершины мы получаем частичное произведение сложной производной, и для вычисления той или иной частной производной пускаем обратный сигнал дальше в нужную вершину, домножая его на локальную, свою собственную производную

![[Pasted image 20220922201031.png]]

Теперь о размерностях – градиент имеет смысл, если у нас на вход подаётся вектор, а на выход число. Если же и на входе, и на выходе вектор, то надо перемножать не градиенты, а якобианы (матрицы частных производных)

&nbsp;

### ReLU
Есть такая проблема с сигмоидой – её производная почти обнуляется, стоит отойти совсем наделеко от старта. И это проблема, потому что при градиентном спуске мы будем шагать на всё более незначительный шаг. Это называется *градиентное затухание*. Помочь с этим, заменив сигмоиду, нам может популярная функия ReLU

ReLU расшифровывается как *Rectified Linear Unit* – очень сложное название для очень простой формулы $$ReLU(x) = \max(0, x)$$
В принципе функция активации пусть и грубо, делает примерно то же самое что и сигмоида. Как минимум градиент на ней никогда не убывает, считается **намного** быстрее чем постоянное вычисление экспоненты.

Однако тот факт, что на половине значений функция зануляет градиент тоже может быть опасен. Есть такой термин, как *мёртвые нейроны*. Это нейроны, выдающие ноль на любом объекте. ReLU способствует их появлению, и хоть вероятность возникновению мёртвого нейрона мала, и с обучением они устраняются, в глубоких сетях это может стать проблемой (её решают на уровне построения архитектуры нейросети).

Если дорабатывать идеи ReLU, можно обратиться к *LeakyReLU*$$LeakyReLU(x) = \max(0.1\cdot x,\;x)$$
Или к *Exponential Linear Unit* aka *ELU*$$ELU(x) = \begin{cases}x,\;\;\;\;\;\;\;\;\;\;\;\;\;\;x\geq0\\\alpha(e^x-1),\;\;x<0\end{cases}$$
И скалированной версии *ELU* – *Scaled Exponential Linear Unit* aka *SELU*
$$SELU(x) = \lambda ELU(x)$$


![[Pasted image 20220924224427.png]]

Еще есть функция *swish*$$swish(x) = x\cdot\sigma(\alpha x)$$
![[Pasted image 20220924230659.png]]

На глубоких сетях она показывает качества чуточку (~1%) лучше ReLU
- - -

&nbsp;

## Фишечки
### Нормировка

Для начала данные было бы неплохо *стандартизировать*, то есть вычесть из них эмпирическое среднее и поделить на корень из оценки дисперсии
$$X \rightarrow \frac{X - \mu}{\sqrt{\sigma^2}}$$
Еще можно применить к данным модификацию *метода главных компонент* – *PCA*

![[Pasted image 20220926172006.png]]

&nbsp;

### Иницилиация весов
Несколько советов. Во-первых, надо нарушить симметричность, задать веса *рандомно*, чтобы нейроны были разные. Во-вторых, нельзя допускать *"насыщенность"* нейрона, то есть ситуации когда его выход всегда близок к 1, или наоборот его смерти, когда он всегда нулевой. Для этого надо проследить, чтобы входы на все слои имели одинаковую *дисперсию* (иначе шаги между разными слоями при градиентном спуске будут как бы разного масштаба). При этом смещения нужно подавать нулевые. Если что, нейронка сама их обучит быть какими надо

Чтобы не придумывать велосипед, есть *инициализация Ксавьера*
$$w_{ij}^{(k)} \sim U(-\sqrt{\frac{6}{n_{in}^{(k)} + n_{out}^{(k)}} }, +\sqrt{\frac{6}{n_{in}^{(k)} + n_{out}^{(k)}}})$$
Где $n_{in}^{(k)}$ и $n_{out}^{(k)}$ – количество входных и выходных связей для $k$-го слоя

Идея инициализации Ксавьера следующая – пусть функция активации нечетная, а в нуле идентична тождественной. Тогда если захотеть, чтобы от слоя к слою сохранялась дисперсия значений и градиентов, получим нерешаемую систему уравнений. Однако усредняя значения решений каждого уравнения и выбирая из всех подходящих распределений равномерное (могли взять и нормальное) и нормируя его как надо, получаем эту формулу.

Отсюда понятно, что инициализация Ксавьера не подходит для ReLU, потому что она локально в нуле не выглядит как тождественная

Если же все-таки брать нормальное распределение вместо равномерного, получим *нормальную инициализацию Ксавьера*. Ограничение в поставленное задачи стоит только на матожидание и дисперсию распределения
$$w_{ij}^{(k)}\sim N(0, \frac{2}{n_{in}^{(k)} + n_{out}^{(k)}})$$ 
Еще есть *инициализация Кеминга*
	$$w_{ij}^{(k)} \sim U(-\sqrt{\frac{3}{n_{in}^{(k)}}}, +\sqrt{\frac{3}{n_{in}^{(k)}}})$$
&nbsp;

### Ранняя остановка

Как только качество начало ухудшаться, останавливаем обучение модели. Это было и в GDB, но здесь это прям существенно важный инструмент

&nbsp;

### Мини-батчи
На самом деле нейросети обучаются немного хитрее того что мы говорили. Выбирается не просто случайный объект и по нему делается спуск, а целая группа. Формально если эта группа $I$, градиентный спуск выглядит так
$$w^{(t+1)} = w^{(t)}-\frac{\eta}{|I|}\sum\limits_{i\in I}\nabla(l(a(x_i\;|\;w^{(t)}), y) + \lambda R(w^{(t)}))$$

Понятно, чем это лучше. Градиент не такой случайный. Но самое главное – при обучении на GPU при выборе одного объекта программа будет *однопоточная*. Грех не нагрузить остальные потоки, подключив еще элементы в батч. Можно делать *нормировку по батчам*, а также специально организовывать батчи (например, чтобы они содержали представителей всех классов)

При этом важный эффект – **Увеличение размера батча приводит к тем же результатам, что и уменьшение темпа обучения**. Поэтому не стоит удивляться более медленной сходимости. Если ты купил себе новую видеокарту нужно не только радостно увеличить размер батча из-за увеличения потоков, а еще увеличить темп обучения

Эксперты полагают, что размер батча надо делать очень небольшим, в пределах 32.
Как минимум это связано с тем, что на самом деле нам нужно найти не просто минимум функции, а *плоский* минимум (если он будет острым, значит рядом с ним значения сильно больше. Если вдруг наша выборка хоть немного отличается от реальных данных, качество резко упадет. А плоский минимум стабилен даже при отличающейся выборке). Вот как раз плоские минимумы SGD находит лучше обычного градиентного спуска

К слову, один проход по всем батчам это *эпоха*. И делается именно проход по всем батчам, а не выбор случайного (взяв батч один раз, больше его элементы мы не трогаем до следующей эпохи), что идет вразрез с философией SGD, однако качество становится лучше. После эпохи мы, конечно же, делаем новые батчи. Об этом не стоит забывать, если придется реализовывать алгоритм руками – одни и те же батчи из эпохи в эпоху это плохо

&nbsp;

###  Продвинутая оптимизация

В чём проблема батчей? В том, что каждый батч смотрит не на глобальный минимум, а куда-то в другую сторону. Для частичного решения этой проблемы можно сделать градиент *инертным*, то есть учитывать не только направление батча, а еще и инерцию, и брать их векторную сумму. Саму инерцию, понятное дело, нужно перерасчитывать после появления нового батча

Тогда на каждом шаге веса имеют следующую запись
$$w^{(t+1)} = w^{(t)} - \eta m^{(t+1)}$$
Вектор инерции $m$ задаётся либо стандартным путём
$$m^{(t+1)} = \rho m^{(t)} = \nabla L^{(t)}(w^{(t)})$$
Либо *методом Нестерова*
$$m^{(t+1)} = \rho m^{(t)} + \nabla L^{(t)}(w^{(t)} - \eta m^{(t)})$$

&nbsp;

Следующая существующая эвристика – *Adagrad*, где мы делим градиент на корень из накопленной суммы квадратов градиентов на прошлых шагах. То есть чем больше была производная по компоненте, тем меньше и меньше мы делаем по ней шаг
$$w_i^{(t+1)} = w_i^{(t)} - \frac{\eta}{\sqrt{v_i^{(t+1)}+\varepsilon}}\nabla_iL^{(t)}(w^{}(t))$$
$$v_i^{(t+1)} = v_i^{(t)} + (\nabla_iL^{(t)}(w^{(t)}))^2$$

&nbsp;

Если сделать это умнее, то наверное стоит добавить гиперпараметр – то, на сколько значим для нас текущий градиент по отношению ко всем предыдущим. Получаем метод *RMSprop* aka *Root Mean Squared Propagation*

$$w_i^{(t+1)} = w_i^{(t)} - \frac{\eta}{\sqrt{v_i^{(t+1)}+\varepsilon}}\nabla_iL^{(t)}(w^{}(t))$$
$$v_i^{(t+1)} = \beta v_i^{(t)} + (1 - \beta)(\nabla_iL^{(t)}(w^{(t)}))^2$$


&nbsp;

Также есть метод оптимизации *Adam* aka *Adaptive Moment Estimation*. Это RMSprop + инерция. Тут мы накапливаем и инерцию, и сумму квадратов градиентов

$$m^{(t+1)}_i = \alpha m_i^{(t)} + (1 - \alpha)\nabla_iL^{(t)}(w^{(t)})$$$$v^{(t+1)}_i = \beta v_i^{(t)} + (1 - \beta)(\nabla_iL^{(t)}(w^{(t)}))^2$$
$$w_i^{(t+1)} = w_i^{(t)}-\frac{\eta}{\sqrt{v_i^{(t+1)} + \varepsilon}}m_i^{(t)}$$

И еще есть целая куча обощений и модификаций... Подытоживая, важно знать только одно – подбор алгоритма **очень** важен, и не менее важен подбор гиперпараметров в нем (а они есть во всех рассмотренных алгоритмах оптимизации). На практике почти всегда хорош Adam. Если он по какой-то причине не заработал, можно попробовать SGD + momentum

![[Pasted image 20220926212656.png]]Обзор методов

&nbsp;

### Зашумление градиента
Сейчас так почти никто не делает, но вообще есть удивительно простой способ улучшить качество обучения – зашумить градиент$$g^{(t)} \rightarrow g^{(t)} + N(0,\alpha)$$
Идея примитивная – не просто бухаемся в минимум, а благодаря шуму отступаем туда-сюда проверяем понемногу альтернативы

&nbsp;

### Регуляризация aka Weight decay
В нейросетях тоже есть регуляризация. Правда часто она называется термином *weight decay*. Формула следующая
$$w^{(t+1)} = (1 - \alpha)w^{(t)} - \eta\nabla L^{(t)})(w^{(t)})$$То есть при $\alpha = 0$ имеем SGD, а при $\alpha = \frac{1}{2}$ видим что веса на каждом шаге уменьшаются вдвое.  Реально получается decay. Звучит странно, но на самом деле не просто так мы заговорили про *регуляризацию* – если сделать L2 регуляризацию с параметром $\alpha$, то метод градиентного спуска как раз превратиться в этот decay

&nbsp;

### Max-norm регуляризация
Зачем регуляризовать, если можно тупо ограничивать. Для каждого нейрона можно ограничить сверху норму весов$$\|w_{neuron}\|\leq c$$
Если норма превышает константу, *проецируем* ее так, чтобы не превышала (тут вопрос, не очень понятно что это за проекция)

&nbsp;

### Dropout
Дропаут – это когда мы выбрасываем нейроны из нейросети. Формально это значит, что теперь нейрон всегда выдаёт константный ноль. Зададимся числом $p$ – вероятностью сделать дропаут каждому нейрону. После этого проводим обучение кастрированной сети. На следующей итерации делаем новый дропаут и удаляем другие нейроны. Получается, что мы обучаем не всю сеть сразу, а её подсети.

Формально, обучение превращается в следующую модель
$$y = f(Wx)\cdot m$$
$$m \sim Bernoulli(1 - p)$$


В полносвязных НС принято обнулять все слои, кроме последнего, потому что последний слой это уже ответы, и убирать часть ответов это как-то неправильно

![[Pasted image 20220926232318.png]]

Тест мы будем делать по всей сети, однако результат нужно уменьшать, потому что нейроны, обученные на дропауте, привыкают к маленькой порции входящих сигналов, и при большом количестве входящих узлов будут выдавать большие значения. Поэтому умножаем на соответствующую долю
$$y = (1 - p)f(x)$$
Бонусом получаем оценку на *дисперсию* ответа (смотрим как его шатает в зависимости от выбора нейронов для дропаута)

Почему это работает? Согласно *байесовской теории*, дропаут связан с регуляризацией, агументацией и много чем ещё хорошим. Также дропаут уменьшает *созависимость* совместных нейронов (функционал нейронов не концентрируется в одном месте, а размазывается по сети). А еще есть прямая аналогия с *ансамблированием* (учим кучу мелких моделей, а в конце объединяем их в одну большую модель). Наконец, сеть становится более *робастной*, мы не просто доверяем одному нейрону, а стараемся смотреть на картину широко (потому что "важный" нейрон может быть исключён)

&nbsp;

Иногда вместо дропаута используется *инвертированный дропаут*. Это когда мы вместо того чтобы умножать тест на $(1-p)$, делим на него трейн.

Тогда обучение выглядит так$$y = \frac{1}{1-p}f(Wx)\cdot m$$
А тест так $$y = f(x)$$
То есть во время тестирования ничего специфического не делаем. Например, если случайная величина $m$ убивает половину нейронов ($p = \frac{1}{2}$), то мы увеличиваем выходной сигнал в два раза. Удобно, потому что на тесте не надо ни о чём думать

&nbsp;

Еще одна гигачад альтернатива – *standout*. Идея в том, чтобы подбирать вероятность дропаута отдельной нейросетью исходя из получившейся нейросети$$m \sim Bernoulli(g(Wx))$$
&nbsp;

*DropConnect* – это когда мы убираем не нейроны целиком, а отдельные связи. Математически это эквивалентно домножению на матрицу

![[Pasted image 20220927002419.png]]

$$y = f((W\cdot M)x)$$
&nbsp;

*Гауссовский dropout* – это когда мы не убиваем нейрон (домножая его на 0 или 1, как в распределении Бернулли), а умножаем на коэффициент, распределенный нормально вокруг единицы

![[Pasted image 20220927110818.png]]

Всё это алгоритмы с *неявной регуляризацией*. Мы, дёргая значения весов, ухудшаем обучение, но за счёт этого делаем нашу сеть более робастной и подготовленной к суровым настоящим данным. Тем самым мы избегаем переобучения, и ничего страшного если качество от этого пострадает (метод 1 соседа даёт 100% качества)

&nbsp;

### Обрезка градиента  aka Gradient clipping

Если мы уже начали спускаться в яму, но "попали на утёс", есть шанс улететь очень далеко от точки минимума

![[Pasted image 20220927111739.png]]

Как вариант – укоротить градиент при сохранении направления
$$g^{new} = \frac{\min(\theta, \|g\|)}{\|g\|}g$$
Лучше этим не пренебрегать и пользоваться. Одна строчка в коде, а в итоге решается потенциально неприятная ситуация. При этом клипировать нужно обязательно до корректировки весов и после обратного хода. Первое нужно, чтобы корректировать уже по клипированному градиенту, а второе чтобы было что клипировать

&nbsp;

### Батч-нормализация
Есть проблемка – когда мы корректируем какой-то вес нейрона, мы делаем это, ориентируясь на текущую ситуацию и распределение значений. Однако за эпоху поменяются в том числе нейроны, от которых зависит наш недавно измененный, и получится, что только что сделанные изменения нейрона отражают уже не актуальное состояние нейросети.

Это изменение называется *covariative shift*, и мы хотели бы, чтобы оно было не таким существенным

Как изменить тот факт, что у нас смещаются отношения между объектами? Можно нормировать батч, то есть если $\{x_i\}$ – набор значений нейрона на батче, а $\mu$ и $\sigma^2$  эмпирические среднее и дисперсия, то мы нормируем батч по формуле$$x_i^{new} = \frac{x_i - \mu}{\sqrt{\sigma^2 + \varepsilon}}$$
Правда теперь мы снова накосячили, потому что теперь все значения тусуются около нуля. Ну не беда, будем сдвигать полученный результат линейной функцией с подходящими параметрами
$$x^{new} \rightarrow \alpha x^{new} + \beta$$
Как понять что они подходящие? Засунем их в ту же нейросеть и будем обучать их в том числе! 

Батч-норму нужно использовать в каждом слое, если уж взялся. Иногда можно нормировать до функции активации, иногда после. Это тонкий момент, от которого тоже может зависеть качество, и правильного ответа здесь нет. Профессиональная фишечка – Если слой линейный, а потом идет батч-норма, то делать этот слой нужно без смещения (всё равно батч-норма его смещает)

**Плюсы:**
- Может (в разы) увеличить скорость обучения
- Интересно, что можно убрать дропаут. Они даже могут друг с другом спорить в процессе обучения, так что либо-либо
- Делается некоторая регуляризация
- Можно использовать более глубокие сети и больший шаг обучения
- Меньше чувствительность к инициализации (например, можно не нормироват

&nbsp;
### Аугментации

*Аугментация* есть ни что иное как расширение множества данных. Как правило из существующих данных искусственно моделируем новые данные.

Очевидный пример на картинках: если мы отразим собачку по вертикали, повернем, обрежем, зашумим, изменим контраст, собачка останется собачкой. 
Также можно зашумлять музыку (надо уметь распознавать речь даже на фоне музыки). В тексте можно переставлять слова, делать замену синониму.

Правда стоит понимать, что не все преобразования допустимы. Например, для цифр отражения будут совершенно греховными – "6" превратится в "9"

Аугментация это круто, потому что нейросеть становится устойчивой к нашим преобразованиям. 

Аугментация может быть

- **Простая** – естественная, такая как поворот
- **Агрессивная** – порча объета (накладывание маски, объекта другого класса)
- **Креативная** – симуляция, GANы

Может быть *online* и *offline* аугментация (либо заранее сохраняем на диск увеличенный датасет, либо аугментируем прямо во время воркфлоу)

&nbsp;

*Mixup* – очень простая статья, тем не менее ставшая популярной. Идея в том, чтобы по двум объектам брать их взвешенную линейную комбинацию, то есть для объектов $x_{1},x_{2}$ мы имеем
$$
\begin{cases}
x_{new} = \lambda x_{1}+(1 - \lambda)x_{2}\\
y_{new} = \lambda y_{1}+ (1-\lambda)y_{2}
\end{cases}
$$
Физический смысл в том, что если мы наложили на картинку и собаку, и кота, то нейросеть тоже должна видеть и кота и собаку. Еще есть *cutout*, который отрезает кусок изображения, чтобы нейросеть тренировалась не только по одному элементу картинки. *Cutmix* объединяет эти идеи

![[Pasted image 20221026022101.png]]


Более хитрым способом является *attentive cutmix*. Это когда мы имеем картинку котёнка, учимся его определять, а потом смотрим, по каким конкретно кускам изображения котёнок был распознан (как это делать – позднее). Теперь берем и вставляем эти $k$ важных "кошачьих" кусков в картинку собаки и придаём новой картинке желаемую метку – $\frac{k}{n}$ за кошку и $\frac{n-k}{n}$ за собаку

![[Pasted image 20221026022730.png]]


&nbsp;

*GridMask* это свежая, но простая идея – накладываем маску из черных квадратиков, получаем улучшение

![[Pasted image 20221026023133.png]]

&nbsp;

### Ансамбль нейросетей

Не очень популярный подход. Потому что обучить несколько нейросетей это очень тяжело с вычислительной точки зрения. Как правило применение нескольких сеток и усреднение результатов дает +2 процента.

Гораздо чаще применяется *shapshot ansemble*. Алгоритм позволяет делать "ансамблирование" обучая всего одну сеть. Для этого мы сохраняем значения в локальных минимумах (для этого нужно устроить обучение так, чтобы ходить по нескольким минимумам). В конце обучения говорим, что каждое состояние сети в минимумах есть отдельная нейросеть, и усредняем ответ по ним.

*SWA* aka stochastic weight averaging – отечественная разработка, в который мы не просто обновляем веса, а раз в $N$ итераций сохраняем их и усредняем с уже сохраненными тем же способом весами. На данных из статьи давало очень сильный прирост качества

&nbsp;

### Transfer Learning

Идея в том, чтобы брать для решения своей задачи уже натренированную нейросеть, и дообучать только последние слои, отвечающие как раз за высокоуровевные выводы. Тогда остальные слои, работающие с примитивами (условно, в детекции изображений эти слои ищут общие паттерны фигур: углы, закругления и т.д.) мы *заморозим*. Иногда можно не прям морозить, а просто понижать для них learning rate

![[Pasted image 20221026175621.png]]

>[!done]+ Советы от Дьяконова
>- начинайте с простых архитектур и методов (оптимизации)
>
>- начинайте с небольшого набора данных (для начальных экспериментов, попробуйте переобучиться на батче)
>
>- выбирайте правильную архитектуру (классификация изображений – CNN,
последовательности – LSTM/GRU/Transformer и т.п.)
>
>- Добавление параметров $\Rightarrow$ усложнение сети (больше времени на обучение, риск переобучения)
>
>- Используйте средства борьбы с переобучением (см. выше)
>
>- Если данных слишком много средства могут не понадобиться. Если можно –
собирайте данные!
>
>- Используйте уже натренированные модели (не геройствуйте – берите проверенное)
>
>- Learning rate часто самый важный параметр – лучше уменьшать (рекомендуют Adam)
>
>- Есть методы настройки параметров лучше, чем structured (grid) search
>
>- Визуализируйте!
>
>- Смотрите на несколько метрик (обязательно на интерпретируемые)
>
>- 0.1 / 1% Rule – веса должны меняться на 1% от своих значений
>
>- Правильно инициализируйте (например при дисбалансе в последнем слое надо так, чтобы выдавалась малая вероятность)
>
>- если что-то не получается можно понизить размерность (по-умному (PCA и т.п.) или просто уменьшить картинки)
>
>- если что-то не получается можно упростить / усложнить сеть
>
>- меняйте сеть поэтапно (не вносите более одного изменения)
>
>- осторожней со смешиванием техник (например, dropout и BN)
>
>- тренируйте дольше
>	«One time I accidentally left a model training during the winter break and when I got back in January it was SOTA»
>
>- не доверяйте встроенным программам понижения темпа

- - -

&nbsp;

## Диагностика проблем

**1**  Можно приближать градиент по формуле $$f'(x) = \frac{f(x+\varepsilon) - f(x - \varepsilon)}{2\varepsilon}$$
Это позволяет нам сравнить градиент от обратного распространения с двумя результатами прямого распространения

&nbsp;

**2** Признаки можно визуализировать, и хорошо было бы, чтобы признаки были некореллированны и имели большую дисперсию

![[Pasted image 20221026150820.png]]

Как мы видим в плохом случае, у нас есть почти одинаковые нейроны и вообще мертвые (черные столбцы). Нехорошо

Для *свёрток* (об этом дальше) нужно стремиться к нешумной неодинаковой и структурированной картинке

![[Pasted image 20221026151002.png]]

&nbsp;

**3** Надо убедиться что сеть обучается на маленьком количестве объектов и тупо его запоминает (то есть сильно переобучается). Если даже это не так, есть повод задуматься

&nbsp;

**4** Можно проверить, насыщены ли нейроны заранее при инициализации? То есть хорошо ли мы инициализировали нейроны. Если нет, то бахаем нормировку

&nbsp;

**5** Есть такое нестрогое правило, что веса должны меняться примерно на 0.1% за итерацию. Поэтому если вес изменяется в два раза за эпоху, то вам есть о чем задуматься

&nbsp;

**6** Смотрим на динамику ошибки, по этому графику можно очень многое понять

![[Pasted image 20221026151555.png]]

Между train и test должен быть зазор. Если он слишком большой, то это переобучение и нужно усилить регуляризацию. Если он маленький, то нужно усложнить модель

&nbsp;

**7** настраиваем learning rate. Например, постепенно снижая его. Для этого есть, например, *lr finder*, который уменьшает lr и смотрит на качество

![[Pasted image 20221026152732.png]]

Еще есть программаторы learning rate. Например, *StepLR*, который показывает, через сколько шагов и как мы будем менять lr

> [!error]+ Типичные ошибки от Дьяконова
>
>- не перевести в режим `train / eval`
>
>- забыть `zero_grad()` до `backward()`
>
>- сделать `softmax` для функции, ожидающей логиты (оценки)
	>(`nn.CrossEntropyLoss = nn.LogSoftmax + nn.NLLLoss` – не нужен softmax)
>
>- размерности, по которым делаются операции (`nn.Softmax(dim=-1)`)
>
>- не создавайте лишнего в forward pass
	>(например, тогда `nn.Embedding` будет каждый раз новым)
>
>- не тестировать с разными параметрами
	>(например, `batch_size`, т.к. ориентацию матриц можно перепутать, если матрица квадратная, то ошибки не будет. Если в датасете матрица 5x5, лучше проверить на 5х4 и в будущем не добавлять параметр величины 5)
>
>- не оформлять последовательность модулей в `nn.ModuleList / nn.Sequential` (если это внутри другого модуля, то их параметры автоматически не определятся как параметры обёртки
>
>- вызывать .to(device) внутри инициализации НС
>
>- пренебрегать правильной инициализацией в глубоких сетях

- - -

&nbsp;

## Свёрточные нейронные сети
Начнем с понимания изображения. Что это такое? Матрица целочисленных яркостей в случае черно-белого изображения. В случае цветного изображения у нас добавляется дополнительная размерность в виде каналов. Получается размер `Channels x H x W`

Линейный подход к обучению на картинках такой : вытягиваем картинку и в случае линейного слоя делаем линейные комбинации для каждого слоя

![[Pasted image 20221026195857.png]]

Из плюсов такого метода – можно на основе получившихся строчек для каждого из классов понять, как выглядят эти классы для нейросети, получить интерпретацию. Для MNIST получаем очень залипательную карту внимания для каждого класса

![[Pasted image 20221026200322.png]]

Но на самом деле такой подход не очень хорош, хотя бы потому что теряет информацию близости почти всех пикселей. Если вдоль изображения была рамка (довольно простой паттерн), при растягивании она превратится в очень неочевидный набор пикселей, который очень сложно подвязать друг к другу

Из-за этого мы не можем выделять паттерны и, например, учимся детектить изображения только в том месте где их видели и на фоне того же цвета. Модель довольно примитивна и вдобавок имеет какое-то непозволительное число параметров. А мы хотим и поменьше параметров, и устойчивость к сдвигу.

В *свёрточной сети* сначала идут многократные свёртка $\rightarrow$ нелинейность $\rightarrow$ пуллинг, а потом обычная, привычная полносвязка. Свёрточные сети подходят для обработки любых "равномерных сигналов" – и звуков, и видеорядов

![[Pasted image 20221027002721.png]]
Супер наглядная иллюстрации работы свёрточной нейросети

&nbsp;

### Определение

С точки зрения математики свёртка это следующая операция 
$$
(f \ast g)(x) = \int\limits_{\mathbb{R^n}}f(y)g(x-y) dy 
$$

Или в дискретном случае

$$
(f\ast g)(i) = \sum\limits_{j}f(j)g(i - j) 
$$
Вот пример одномерной свёртки для сигнала (картинки, звука) $I = (i_{1}, i_{2}, \dots, i_{n})$ и *ядра свёртки* (как правило гораздо более короткого) $K = (k_{1}, \dots, k_{r})$

![[Pasted image 20221026201442.png]]

В случае, если размерности ответа, свёртки и вектора не согласованы, можно расширить вектор. Основные способы это либо заполнение константой, лиюо отражение краёв, либо цикличное продолжение. Такое продолжение называется *отступ* aka *padding*. Это иногда нужно, чтобы не уменьшать результат свёртки

![[Pasted image 20221026201823.png]]

Следующий параметр – *шаг* aka *stride*. Из названия понятно, что это. Усиливает эффект свёртки, но заметно уменьшает ответ

![[Pasted image 20221026201903.png]]

Также можно менять *расширение* aka *dilation*. Позволяет увеличить область действия свёртки и тоже в каком-то смысле усиливает эффект (напоминаю, что естественные данные непрерывны и соседние значения, как правило, похожи, а значит хочется смотреть не только на одинаковых соседей)

![[Pasted image 20221026202223.png]]

- - -

В случае *2D-свёртки* идея совершенно аналогична
$$
(I \ast K)_{xy} = \sum\sum K_{ij}I_{x+i-1,\;y+j-1}
$$
Например

$$
\begin{pmatrix}
1 & 2 & 3 \\
3 & 4 & 5 \\
1 & 0 & 2
\end{pmatrix}\ast\begin{pmatrix}
0 & 1 \\
1 & -1 \\
\end{pmatrix} = \begin{bmatrix}
\begin{pmatrix}
1 & 2 \\
3 & 4
\end{pmatrix}\ast\begin{pmatrix}
0 & 1 \\
1 & -1
\end{pmatrix} & \begin{pmatrix}
2 & 3 \\
4 & 5
\end{pmatrix}\ast \begin{pmatrix}
0 & 1 \\
1 & -1
\end{pmatrix} \\
\begin{pmatrix}
3 & 4 \\
1 & 0
\end{pmatrix}\ast\begin{pmatrix}
0 & 1 \\
1 & -1
\end{pmatrix} & \begin{pmatrix}
4 & 5 \\
0 & 2
\end{pmatrix}\ast\begin{pmatrix}
0 & 1 \\
1 & -1
\end{pmatrix}
\end{bmatrix} = \begin{pmatrix}
1 & 2 \\
5 & 3
\end{pmatrix}
$$

&nbsp;

### Применение

Хотим узнать наличие какого-то паттерна. Например, есть ли на картинке голова человека. Для этого можно пройтись изображением головы по всему нашему изображению и проделать свёртку. Там, где результат будет выше всего, большая корелляция с нашей целевой головой, а значит там, скорее всего, она и находится

Заметим, что свёртка есть суть фильтр. Ею можно добиваться усиления границ и четкости или наоборот размытия

![[Pasted image 20221026205928.png]]

Прелесть применения свёртки в глубоком обучении в том, что нам не нужно подбирать параметры матрицы свёртки – мы их просто обучим

На практике нам необходимо, чтобы ядро свёртки имело столько же каналов, сколько изображения. Проходя по изображению, будем получать в каждой точке численный результат для каждого из каналов. Результатом свёртки в этой точке будет сумма этих результатов по каналам.

То есть свёртка из многоканального изображения делает "одноканальную" матрицу

![[Pasted image 20221026210455.png]]
![[Pasted image 20221026210413.png]]

Если мы хотим применить сразу несколько свёрток, получим, соответственно, сразу несколько таких вот однослойных матриц

![[Pasted image 20221026210542.png]]

Каждый такой листок называется *feature map* aka *карта признаков*. Почему? Потому что каждая свёртка отвечает за что-то. Опять же, если внутри свёртки лежало изображение головы или фигуры Хаара, то мы получили бы карту совпадения с этим признаком каждой области нашего изображения.

Заметим, что размерность свёрточного слоя – $D \times C \times h \times w$, то есть это четырехмерный объект (при этом количество параметров всё равно небольшое относительно линейного метода)

Также заметим что свёрточный слой не связывает нейроны *каждый с каждым*. В свёртке нейрон зависит только от тех элементов, которые захватило ядро свёртки находясь в соответствующей точке. Это называется *разреженное взаимодействие* aka *sparse interaction* 

&nbsp;
### Поточечная свёртка

Есть так называемая *поточечная свёртка*, то есть размера $1\times1$. Зачем она нужна? Представим, что наше "изображение" имеет 3 канала "пикселей" (кавычки, потому что после нескольких свёрточных слоёв каждая точка будет относиться не к пикселю, а какому-то признаку объекта). Тогда беря свёртку в какой-то точке мы рассматриваем линейную комбинацию этих признаков-пикселей. Таким образом мы как бы усложняем нашу интерпретацию, и понятное дело что таких поточечных свёрток может быть несколько, поэтому опять же получаем столько каналов на выходе, сколько хотим

Поточечные свёртки, в силу того, что они не меняют размер изображения, часто используют, чтобы поменять число каналов (хочешь 10 каналов? примени 10 поточечных свёрток). В итоге получается довольно умная реализация

&nbsp;

### Пулинг

Некоторое изменение идеи свёртки. Теперь мы вместо скалярного произведения с набором параметров смотрим значение какой-то функции в каждом окне (например, среднего или максимума)

С помощью пулинга можно изменять размер изображения. Почему не свёрткой? Пулинг добавляет лишнюю нелинейность и работает быстрее свёртки.

&nbsp;

### Последние слои

После свёрток у нас на каждом шаге тензор преращается в тензор. Но как в конце должны получаться вектора для полносвязного конца сети? Есть два способа. Либо просто *вытянуть* результат в вектор и работать как привыкли, либо   для каждого канала сделать *пулинг* по **всему** каналу (то есть по всему "изображению") и результаты пулинга по каждому каналу (это опять может быть максимум, среднее и т.д.) объединить в вектор. И там и там теряется пространственная информация, но при пулинге она теряется полностью, а при вытягивании какие-то близости сохраняются

&nbsp;

### Spatial Separable Convolutions
Идея в том, чтобы *разложить* по возможности свёртку на две меньшего размера и применять их последовательно. Например при разложении
$$
\begin{pmatrix}
-1 & 0 & 1 \\
-2 & 0 & 2 \\
-1 & 0 & 1
\end{pmatrix} = \begin{pmatrix}
1 \\
2 \\
1
\end{pmatrix}\cdot \begin{pmatrix}
-1 & 0 & 1
\end{pmatrix}
$$
Количество обучаемых параметров падает с 9 до 6 (но не все свёртки представимы так)

&nbsp;

### Групповая свёртка
Идея в том, чтобы применять не одну свёртку ко всем признакам, а разбивать признаки на *группы* и к каждому применять свою свёртку (потенциал к параллелизации очевиден)

![[Pasted image 20221027160551.png]]
Эта идея появилась из *AlexNet*

>[!warning]- Кратность
>И входное, и выходное количество каналов должно быть кратным количеству групп, иначе pytorch не съест

&nbsp;

### Depth-wise свёртка
*depth-wise convolution* – это когда мы вместо того, чтобы применять в конкретной точке свёртки сразу ко всем слоям, применяем три одноканальных свёртки независимо к каждому слою. По сути это групповая свёртка для количества групп равного количеству каналов.

![[Pasted image 20221027161638.png]]

После этого можно пройтись поточечными свёртками и при желании получить результат той же размерности, что и при обычных свёртках.

Теперь каждой точке соответствует не $3\times{3}\times{3}\times c$ параметров, а $3 \times 3 \times 3$  для каждой точки + $3 \times c$ для поточечной свёртки. И настолько же драматично улучшается эффективность алгоритма.

Поэтому такие свёртки применяются для облегеченных сетей, где нужно уменьшать число параметров и время работы 

- - -
&nbsp;

##  Детекция объектов

![[Pasted image 20221125162523.png]]
Карта детекции объектов
### Классификация + локализация

Задача в том, чтобы определить котёнка на картинке и обвести его рамочкой. Архитектурно это значит, что помимо основной полносвязной "головки" нашей сети у нас будет дополнительная для определения границ объекта (они однозначно определяются двумя точками)

Понятно, что для этой задачи в обучающей выборке должны содержаться не только классы, но и границы рамки.

![[Pasted image 20221124004705.png]]

Какие у нас метрики качества? Очевидно, хочется сравнивать совпадение получившихся площадей относительно общей площади (по сути меру Джаккара), также называемую *Intersection over union*. Эту метрику сравнивают с некоторым порогом, после которого мы считаем что предсказали объект
$$
IoU = \frac{\mid A \cap B \mid}{\mid A \cup B\mid} \geq \alpha
$$

Далее обычно изображения упорядочивают по "уверенности" алгоритма в своём ответе, и начиная с самого вероятного котёнка считают *MAP*, усредняя его по всем классам

&nbsp;


### Детекция

Качественно более сложная задача. Это та же классификация + локализация, но теперь объектов может быть сколь угодно много. Теперь обучающая выборка подкрепляется списком того, в каком файле и с какой рамкой объект какого типа присутствует (понятно, что один и тот же файл может быть упомянут многократно)

Как нам с этим быть? Первая идея – *давайте генерировать на каждое изображение кучу прямоугольников и для каждого из них решать задачу локализации*. Костыльное решение, которое вызывает больше проблем, чем решает. Особенно больно работать с проблемой разного масштаба рамок (детектор настроен на картинки определенного размера)

![[Pasted image 20221124010528.png]]

Но есть и другие идеи. Например...


- - -
&nbsp;

## Двухстадийные детекторы

В чём идея?

- по картинке генерируем регионы (с помощью какой-нибудь *RPN* или *selective search*). Каждый из которых есть кандидат

- Каждый регион подготавливаем по формату (меняем размер, переводим в признаковое описание (по сути вытягиваем в вектор))

- Классифицируем

- Проводим регрессию для рамки, чтобы немного ее дотюнить и подогнать еще лучше

![[Pasted image 20221124011310.png]]

Почему *двухстадийные*? Потому что есть два чётких этапа – генерация регионов и предсказание по ним

Не забываем в конце сделать *NMS* (Non Maximum Supression), который убьёт дубликаты. Как оно работает? Для каждого класса опять-таки упорядочиваем рамки по убываниям уверенности. Идём по всем рамкам и считаем пересечение с самым "убедительным" представителем класса. Если пересечение довольно велико, считаем, что предсказали то же самое, и обнуляем менее уверенный в себе вариант. 

![[Pasted image 20221124011620.png]]

&nbsp;

### Генерация регионов. SS

Но как же нам, собственно, генерировать регионы? Можно с помощью *selective search* Это алгоритм, который кластеризует наше изображение (пиксели примерно одного цвета, лежащие рядом, засовываются в один класс). Далее в цикле, пока сегментов много, объединяем похожие сегменты в один, уменьшая их количество и создавая новые сегменты. Для всех, даже промежуточных сегментов, запускаем классификацию


![[Pasted image 20221124012043.png]]

&nbsp;

### R-CNN

*Region-CNN* – семейство, принадлежащее двухсадийным детекторам. Идея всё та же – генерируем регионы, каждый регион подгоняем по размерам и запихиваем в классификатор (уже архаичный SVM, регрессию), в конце проходимся NMS. Есть куча внетренних ньюансов. Например, как нам быть с предсказаниями рамок, которые не совсем удачно легли и накрыли объект (например, машину) не полностью? Тогда задаёмся порогом, начиная с которого считаем, что эта рамка должна угадывать машину, а значит и находиться там, где на обучающей выборке отмечена машина

Заметим, что задача детекции дисбалансна – почти все рамки будут иметь метку "фон", а значит с этим надо что-то делать (focal loss или специальное семплирование батчей)

У R-CNN очень долгое обучение: сначала надо дообучить обученную на ImageNet CNN, затем SVM, потом регрессию... Ну и наверное некруто, что для *каждого* региона мы запускаем свёрточную сеть

&nbsp;

### SPP-net

Это способ не вырезать регионы. Классически, мы для каждого региона брали и растягивали его до такого одного формата, который принимает нейросеть классификатор (и в том числе по этой причине ее нужно дообучать – она может быть ненатренирована на классификацию растянутых объектов)

А давайте сначала сделаем свёртки (всё равно они сохраняют пространственную близость, и вырезать можно и после свёрток). Но как нам вырезать не в явном виде, а сразу так, чтобы можно было скормить полносвязной сети? 

Так появляется SPP-слой. Пусть в нашем тензоре 256 каналов. Возьмем среднее по каждому каналу и получим 256 признаков. Мало? Разделим на 4 части и для каждой сделаем то же самое, получаем еще $4*256$ признаков. Мало? Делим на 16 частей и так далее... В итоге каждый признак представляет собой среднее по какой-то области какого-то канала – довольно полезный признак

Особенность архитектуры в том, что свёрточные слои используются один раз в самом начале. Дальше мы вытягиваем результат свёртки в вектор и работаем с ним, из-за чего сеть ускоряется в несколько порядков.

&nbsp;

### Fast R-CNN

Похожая идея. Генерируем регионы обычным R-CNN, и теперь разбиваем не многократно, создавая *пирамиду* из разбиений, а один раз на заранее заданное количество кусков (например, в статье было 7х7)

&nbsp;

### Faster R-CNN

Это полностью нейросетевое решение. Если в Fast R-CNN мы отказались от SVM в пользу регрессии, то теперь мы отказываемся еще и от selective search в пользу нейросети *Region Proposal Network* aka *RPN*

Обучение сети довольно нетривиальное. Нам надо обучить RPN часть, потом Fast-RCNN часть. Ошибкой считается сумма ошибок классификации и регресии.

При работе сети изображение проходит через RPN + CNN. После этого оставляется 6000 перспективных регионов, удаляются очень узкие или выходящие за изображение. После этого делается NMS с большим порогом для убийства дубликатов. Оставшиеся регионы прогоняем через RoI пулинг, регрессию и классификацию, и получаем результат, который окончательно прогоняем через NMS по каждому классу 

- - -
&nbsp;

## Одностадийные детекторы

Гениальная идея оптимизации задачи под нейросетевую архитектуру. Считаем, что ответ нам даётся в формате списка *фиксированной* длины, где каждый элемент списка это метка объекта + координаты рамки. Понятное дело, что если длина списка 10, а на экране только 1 объект, то мы заполняем оставшиеся 9 элементов рамками с классом "фон". Теперь, так как длина списка фиксирована, переводим его в *тензор*

![[Pasted image 20221125143208.png]]

Для того, чтобы сеть обучалась правильно и каждая рамка (каждая строка тензора) тренировалась на свою рамку существует понятие *якорей*. В каждой части изображения (скажем, около каждого угла), инциализируется свой набор рамок, и, например, при свёртке эти якоря будут смотреть на получившиеся в соотвествующем углу значения и крепиться к ним. якорей на каждую область может быть много, если мы хотим находить несколько объектов.

Как понимать, насколько хорошо якорь угадал объект? Как обычно, смотрим на пересечение с истинными метками. Если оно достаточно велико, считаем, что якорь предсказал именно этот класс и корректируем его так, чтобы он стремился "перейти" в истинную рамку.

&nbsp;

### YOLO