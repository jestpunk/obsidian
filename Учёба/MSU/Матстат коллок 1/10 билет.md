## 10 билет

### Оценки, состоятельность. Оценки по методу подстановки. Состоятельные оценки для математического ожидания и дисперсии. Метод моментов

&nbsp;


Рассмотрим другой подход к нахождению истинного распределения

Пусть у истинного распределения наших данных есть какой-то функционал $$\theta = \Phi(F)$$Это может быть дисперсия, третий момент, квантиль – всё что угодно

Наша практическая задача по выборке $X = (X_1,X_2,\dots,X_n)$ сделать *оценку* $\hat{\theta}$ этого параметра

>[!summary]- Замечание
>Всюду дальше $$\hat{\theta} = (\hat{\theta_1}, \hat{\theta_2},\dots)$$
>Чтобы не плодить целый вектор из оценок

Оценка *состоятельная*, если$$P(|\hat{\theta}_n-\theta|<\varepsilon)\xrightarrow[n \rightarrow\infty]{}0\;\;\;\Leftrightarrow\;\;\; \hat{\theta}_n\xrightarrow[]{P}\theta$$То есть с увеличением n вероятность того, что оценка будет далека от истины, крайне мала

Оценка *сильно состоятельная*, если $$\hat{\theta}_n\xrightarrow[]{\text{п.н.}}\theta$$


&nbsp;

### Метод подстановки
Метод заключается в том, чтобы взять $$\hat\theta = \Phi(\hat{F}_n)$$
То есть мы просто берём необходимый функционал от эмпирической функции распределения

>[!warning]+ Примеры 
> 1) $\Phi(x) = \int xd(F(x)) = \mathbb{E}X_i$. Тогда $$\widehat{\mathbb{E}X_i} = \int xd(\widehat{F_n}(x)) = \sum\frac{1}{n}X_i = \frac{\sum X_i}{n}$$Что, согласитесь, вполне логичная оценка для матожидания
>
> 3) $\Psi(x) = \int x^2d(F(x)) = \mathbb{E}X^2_i$. Аналогично $$\widehat{\mathbb{E}X^2_i} = \frac{\sum X^2_i}{n}$$



>[!info]+ Теорема
>Оценки по методу подстановки для $a = \mathbb{E}X$ и $\sigma^2 = DX$ имеют вид 
>1) $\widehat{a} = \frac{\sum X_i}{n} = \bar{X}$
>2) $\sigma^2 = \frac{1}{n}\sum(X_i - \bar{X})^2$
>
>И являются сильно состоятельными

>[!quote]- Доказательство
>
**1** Первое необходимое нам стремление следует напрямую из УЗБЧ
$$\widehat{a} = \frac{\sum X_i}{n}\xrightarrow[]{\text{п.н.}}a$$
&nbsp;
>
**2** Далее
$$\sigma^2 = \frac{1}{n}\sum(X_i^2 - 2\bar{X}X_i + \bar{X}^2) = \frac{1}{n}\sum X_i^2 - {\color{goldenrod}\frac{1}{n}}\cdot2\bar{X}{\color{goldenrod}\sum X_i} + \bar{X}^2 =$$ $$= \frac{1}{n}\sum X_i^2 - \bar{X}^2$$
Первое слагаемое сходится почти наверное к $\mathbb{E}X^2$, а второе к $(\mathbb{E}(X))^2$, а значит разность почти наверное сходится к дисперсии

&nbsp;

### Метод моментов
Пусть параметр (вектор параметров) $\theta$ лежит во всеобъемлющем множестве $\Theta$. Обозначим $$M(\theta) = \mathbb{E}_\theta(T(X_i)) = \int T(x)d(F(x, \theta))$$Где в роли $T(X_i)$ вполне может выступать и вектор $$T = (T_1,\;T_2,\dots,\;T_n)$$

Далее пусть 
1) $\forall X_i:\;\mathbb{E}_\theta(T(X_i)) < \infty$

3) $M(\theta)$ имеет непрерывную обратную функцию на множестве значений $\theta$

4) $(\frac{1}{n}\sum T_1(X_1),\;\frac{1}{n}\sum T_2(X_1),\dots,\frac{1}{n}\sum T_m(X_1)) \in \Theta$

>[!info]+ Теорема
>
Оценка по методу моментов  имеет вид
$$\widehat{\theta} = M^{-1}(\bar{g})$$
И в условиях **1-3** является состоятельной

>[!quote]- Доказательство
>
По УЗБЧ  $$\bar{g} \xrightarrow[]{\text{п.н.}}\mathbb{E}_\theta g(x) = M(\theta)$$
А так как обратная фунция непрерывна по условию, имеем $$M^{-1}(\bar{g}) \rightarrow M^{-1}(M(\theta)) = \theta$$
