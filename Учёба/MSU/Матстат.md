# Матстат
#ml #math #ozon #course | [[ml]] [[ozon]] [[math]] [[course]]
Курсы Баштовой и Панова
- - -
&nbsp;
## Параметрика (П)

$(\Omega, F, P)$ – *вероятностное пространство*

$\Omega$ – *sample space*, или *множество элементарных исходов*
F - сигма-алгебра (зачастую булеан, или *power set* $2^\Omega$)
P - вероятностная мера

- - -

$\xi$ - *случайная величина*, если для любого борелевского подмножества прямой прообраз этого множества лежит в сигма-алгебра, то есть $$\forall B \subset \mathbb{B(R)}:\;\;\xi^{-1}(B) \in F$$Зачем такая сложность с определением? Дело в том, что основной способ анализа с.в. заключается в *функции распределения*, которая определеятся так$$F_\xi(x) = P(\xi \leq x) = {\color{goldenrod}P(w\in\Omega:\xi(w) \leq x)}$$Думаю, выделенная запись не вызывает вопросов. Берем все элементы из sample space, для которых с.в. как функция меньше либо равна $x$. Но что значит, что функция меньше либо равна $x$? Это значит что мы берем те элементы области определения, при которых значение этой функции меньше либо равно $x$, а потом уже к этим элементам применяем функцию $P$, то есть$$P(w \in \Omega:\;\xi(w) \in (-\infty;\;x]) = P(w\in\Omega:\;w\in\xi^{-1}((-\infty;x]))$$
$(-\infty,x]$ – это элемент сигма-алгебры над $R$. А функция вероятности может принимать величины только из множества $F$.

Теперь понятно, зачем мы хотели, чтобы прообраз любого элемента сигма-алгебры у случайных величин лежал в $F$. Чтобы мы вообще могли рассматривать функцию распределения, которая есть вероятность, определенная только для элементов из $F$

- - -

Оценка параметра $\hat\theta(x_1, x_2, \dots,x_n)$ является *состоятельной*, если она сходится по вероятности куда надо $$\hat\theta(x_1, x_2, \dots,x_n) \xrightarrow{P}\theta$$

- - -
&nbsp;

## Введение (Б)

В результате эксперименов мы всегда получаем какой-то набор чисел, или реализацию какого-то случайного вектора $(X_1, X_2, \dots, X_n)$, которую мы называем  *данные*. Если вдруг все данные получены из одного распределения и независимо друго от друга (норсв), то данные являются *выборкой*.

При этом случайная величина есть функция из Колмогоровой тройки на выборочное пространство (и Борелевскую сигма-алгебру в нем) $$X: (\Omega, F, P) \rightarrow (\mathbb{X}^n, \mathbb{B}^n)$$с увеличением выборки получается,  будто мы меняем пространство, в которое отображаем функцию, нехорошо... Поэтому давайте считать, что  случайная величина отображается сразу в бесконечномерное пространство $$X: (\Omega, F, P) \rightarrow (\mathbb{X}^\infty, \mathbb{B}^\infty)$$
- - -

*Статистика* $T(x)$ – любая измеримая функция от выборки (сумма, порядковая статистика, произведение логарифмов...)

*Функция распределения случайной величины $\xi$ * : $F(y) = P(x \leq y)$. Эта функция монотонно возрастает от 0 до 1

В матстатистике мы хотим по разным данным предсказать распределение, которое имеет выборка, и первым робким шагом в это болото является *Эмпирическая функция распределения*. Задаётся она следующим нехитрым образом$$\hat{F}_n(y) = \frac{\#\{x_i: x_i < y\}}{n}$$То есть считаем, что вероятность быть меньше, чем $y$, это количество элементов выборки, меньших $y$, деленное на количество. Есть теорема, доказывающая, что эта оценка сходится к истинному распределению

&nbsp;
 >[!info]+ Теорема Гливенко-Кантелли
 >$$\sup\limits_y|\hat{F}_n(y) - F_n(y)| \xrightarrow{п.н.} 0$$
 >То есть максимальная дельта между функциями распределения почти наверное уменьшается до нуля 

>[!quote]- Доказательство
>
>**1** Докажем, что для каждого фиксированного $y$ $$\sup\limits_{y\in\mathbb{R}}|\hat{F}_n(y) - F(y)|\xrightarrow[]{\text{п.н.}}0$$
По УЗБЧ имеем$$\hat{F}_n(y)=\frac{\sum\mathbb{I}\{x_i\leq y\}]}{n}\xrightarrow[\text{УЗБЧ}]{\text{п.н.}}\mathbb{E}(\mathbb{I}\{x_i\leq y\})=F(y)$$
Доказано)
>
&nbsp;
>
**2** Разделим ось на конечное число частей таких, что
>
$$t_k=\sup\{y:\;F(y)<k\varepsilon\}\;\;\;\;k\leq\frac{1}{\varepsilon}+1$$ 
![[Pasted image 20221022015512.png]]
>
Тогда $$\begin{cases}F(t_{k+1}-0)\leq(k+1)\varepsilon\\F(t_k)\geq k\varepsilon\end{cases}\Rightarrow F(t_{k+1}-0)-F(t_k)\leq\varepsilon$$
Рассмотрим $$\sup\limits_{y\in[t_k;\;t_{k+1})}|\hat{F}_n(y)-F(y)| = \max\{\sup(\hat{F}_n(y)- F(y)),\;\sup(F(y) - \hat{F}_n(y))\}$$
Оценим левую часть
$$\sup\limits_{y\in[t_k;\;t_{k+1})}|\hat{F}_n(y)-F(y)| \leq \hat{F}_n(t_{k+1}-0) + F(t_{k+1}-0)-F(t_k) =$$ $$={\color{pink}\frac{\sum\mathbb{I}\{x_i<t_{k+1}\}}{n}-\mathbb{E}(\mathbb{I}\{x_i<t_{k+1}\})} + {\color{skyblue}F(t_{k+1}-0)-F(t_k)}$$
Розовое слагаемое по УЗБЧ стремится к нулю, а голубое $\leq\varepsilon$ по выкладкам выше. 
Мы доказали, что $$\forall\varepsilon:\;\;P(\exists n_0:\;\;\forall n>n_0\;\;\sup\limits_{y\in[t_k;\;t_{k+1})}|F_n(y)-F(y)|\leq\varepsilon)=1$$
А значит то же самое верно и для максимума по супремуму всех отрезков$$\forall\varepsilon:\;\;P(\exists n_0:\;\;\forall n>n_0\;\;\sup\limits_{y}|F_n(y)-F(y)|\leq\varepsilon)=1\Rightarrow$$$$\Rightarrow \forall j:\;\;P(\exists n_0:\;\;\forall n>n_0\;\;\sup\limits_{y}|F_n(y)-F(y)|\leq\frac{1}{j})=1$$
А значит можно пересечь и получить $$P(\forall j:\;\;\exists n_0:\;\;\forall n>n_0\;\;\sup\limits_{y}|F_n(y)-F(y)|\leq\frac{1}{j})=1\Rightarrow$$ $$\Rightarrow \sup\limits_y|\hat{F}_n(y)-F(y)|\xrightarrow[]{\text{п.н.}}0$$
Что и требовалось

&nbsp;

В этой теореме возникает выражение $\sup\limits_y|\hat{F}_n(y) - F_n(y)|$ которое, если подумать, тоже является случайной величиной, которую мы назовём *$D_n$*. Эта случайная величина, вроде как, должна зависеть от n и распределения, но выясняется, что зависимости от распределения нет, и можно находить ее хоть для равномерного распределения

Это распределение $D^u_n$, взятое от равномерной случайной величины (как мы выяснили это не важно) называется *распределением Колмогорова*. Следующая теорема формализует наши заключения

&nbsp;
>[!info]+ Теорема
>Если $F(x)$ непрерывна, топри всех n распределение $D_n$ не зависит от $F(x)$. В частности для равномерного случая формула выглядит так
>$$D_n = D_n^{u} = \sup\limits_{y \in (0,1)}|\frac{1}{n}\sum\mathbb{I}(x_i \leq y) - y|$$

>[!quote]- Доказательство
Для доказательства этой теоремы потребуется *обощенная обратная функция*, введем ее и несколько связанных лемм.
> 
> *Обобщенная обратная функция* $$F^{-1}(y) = \inf\limits_x\{F(x)\geq y\}$$то есть ее значение это первый такой х, значение которого не меньше y
>&nbsp;
>>[!summary]+ Лемма 1
>Если $F(x)$ непрерывна, то $$F(F^{-1}(y)) = y$$
>
>>[!summary]+ Лемма 2
>$$y \leq F(x)\;\Leftrightarrow\;F^{-1}(y) \leq x$$
>
>>[!summary]+ Лемма 3
> Если $X \sim F$, а $U$ – равномерное распределение, то $$F^{-1}(U) \stackrel{d}{=}X \sim F$$
>
$$D_n = \sup|\widehat{F}_n(x) - F(x)| = \sup\limits_X|\frac{1}{n}\sum\mathbb{I}(X_i\leq x) - F(x)| = $$ $$= \sup\limits_X|\frac{1}{n}\sum(F_0^{-1}(u_i)\leq x) - F(x)| = \sup\limits_X|\frac{1}{n}\sum\mathbb{I}(u_i \leq \underbrace{ F(x)}_y) - \underbrace{F(x)}_y|= $$ $$= \sup\limits_X|\frac{1}{n}\sum\mathbb{I}(u_i\leq y) - y| := D_n^u$$

>[!danger]- Основа моделирования выборки
>Лемма 3 из доказательства является основой для *моделирования* выборки из какого-то распределения. Сначала генерируем точки равномерно, а потом берем обобщенную обратную функцию нашего распределения и, применив к равномерной выборке, получим выборку уже из нужного нам распределения

&nbsp;

Заметим, что если $n \rightarrow \infty$, то существует асимптотический результат, выведенный (интересно кем) Колмогоровым и который называется *теорема Колмогорова* $$P(\sqrt{n}D_n \leq x) \xrightarrow[{n\rightarrow\infty}]{}K(x)$$ $K(x)$ – это, что бы вы думали, *функция Колмогорова*, записывающаяся в явном виде как$$K(x) = \sum\limits_{m=-\infty}^{\infty}(-1)^me^{-2m^2x^2}$$
Запишем результат еще раз, формализовав

>[!info] Теорема Колмогорова
Пусть $(X_1, \dots, X_n)$ – выборка, $F_0(x)$ – функция распределения выборки и она непрерывна. Тогда
>
>1) $P(\sqrt{n}D_n \leq x) \xrightarrow[n\rightarrow\infty]{}K(x)$ 
>2) Если функция распределения выборки при альтернативе $F_1(x) \neq F_0(x)$, то $$\sqrt{n}D_n \xrightarrow[n\rightarrow\infty]{p} \infty$$

&nbsp;

### Алгоритм проверки гипотез критерием Колмогорова

1) Еще до эксперимента находим $K_n(x),\;K(x)$

2) По полученной выборке находим такое $D_n$, что $$D_n = \sup\limits_X|\widehat{F}_n(x) -F_0(x)| = \max\limits_{i}\{|F_0(X_{(i)})-\frac{i-1}{n}|,\;|\frac{i}{n} - F_0(X_{(i)})|\}$$
3) Задаём желаемый уровень значимости $\alpha$

А дальше смотрим на размер выборки

>[!question]- $n \leq 20$ (маленькая выборка)
>Находим $z_\alpha^n$ такое, что $$1 - K_n(z_\alpha^n) = \alpha$$
>И отклонить гипотезу, если $D_n > z_\alpha^n$
>
>&nbsp;
>
>**или, что то же самое**
>Находим $p_{value} = 1 - K_n(D_n)$ и отклоняем гипотезу, если $$p_{value} < \alpha$$

>[!question]- $n > 20$ (большая выборка)
>Находим $z_\alpha$ такое, что $$1 - K(z_\alpha) = \alpha$$
>И отклонить гипотезу, если $\sqrt{n}D_n > z_\alpha$
>
>&nbsp;
>
>**или, что то же самое**
>Находим $p_{value} = 1 - K(\sqrt{n}D_n)$ и отклоняем гипотезу, если $$p_{value} < \alpha$$

- - -
&nbsp;
## Порядковые статистики, совместное распределение (Б)

Возьмем случайную величину $\xi_n = n\hat{F}_n(y) = \#\{x_i:x_i \leq y\}$ и оценим ее распределение. Так как эта величина является просто *ненормированным* эмпирическим распределением, она также дискретна, а значит ее можно записать через вероятность равенства какому-то значению.$$P(\xi_n = k) = C_n^k(F(y))^k(1 - F(y))^{n-k}$$Мы получили биномиальную схему, потому что по сути вероятность того, что наша $\xi_n$ равняется $k$, есть вероятность того, что какие-то $k$ элементов выборки находятся левее $y$, а остальные правее. Очень похоже на подбрасывание монетки

Поcчитаем *матожидание* и *дисперсию* полученной с.в.$$E(\xi_n) = nF(y) \;\;\;\;\Rightarrow\;\;\; E(\hat{F}_n(y)) = E(\xi_n/n) = F(y)$$
$$D(\xi_n) = nF(y)(1 - F(y))) \;\;\;\Rightarrow\;\;\;D(\hat{F}_n(y)) = \frac{F(y)(1 - F(y))}{n}$$

*Вариационный ряд* выборки $X_1, X_2, \dots, X_n$ есть ее отсортированная версия $X_{(1)}, X_{(2)}, \dots, X_{(n)}$.

$X_{(k)}$ называют *k-ой порядковой статистикой*

Забавно, что выборка, состоящая из обязательно независимых и одинаково распределенных величин, после сортировки превращается в набор зависимых и по-разному распределенных величин. Это можно понять интуитивно, но выведем явно формулы распределения для статистик

- Распределение максимума
$${\color{goldenrod}P(X_{(n)} \leq x)} = P(X_1 \leq x,\;X_2 \leq x \dots, X_n \leq x) = $$$$= P(X_1 \leq x)\cdot P(X_2\leq x)\dots P(X_n\leq x) = {\color{goldenrod}F(x)^n}$$
- Распределение минимума
$${\color{goldenrod}P(X_{(1)} \leq x)} = 1 - P(X_{(1)} > x) = $$$$1 - P(X_1 > x)\cdot P(X_2 > x) \dots P(X_n > x) = {\color{goldenrod}1 - (1 - F(x))^n}$$
- Распределение 2-ой статистики
$${\color{goldenrod}P(X_{(2)} \leq x)} = 1 - P(X_{(2)} > x) = {\color{goldenrod}1 - (1 - F(x))^n - nF(x)(1 - F(x))^{n - 1}}$$
Здесь мы разделили вероятность на два варианта – либо все значения больше $x$, либо первая порядковая статистика меньше... Появляется ветвление, которое хотелось бы обощить

- Распределение k-ой статистики
По сути нам подходят все те варианты, при которых какие-то $k$ элементов меньше $x$, а остальные больше, и количество подходящих вариантов растёт комбинаторно
$${ P(X_{(k)}) \leq x)} = { \sum\limits_{i=k}^{n}}C_n^iF(x)^i(1 - F(x))^{n-i}$$что очень похоже на распределение ненормированного эмпирического распределения. Это логично, потому что тут тоже возникает схема Бернулли – нам подходит хотя бы $k$ успехов, и мы суммируем все возможные варианты 

Найдем *совместное распределение* минимума и максимума
$${\color{goldenrod}F_{X_{(1)}X_{(n)}}(x, y)} = P(X_{(1)} \leq x,\;X_{(n)} \leq n) = {\color{grey}[P(AB) = P(A)- P(A\bar{B})]} =$$ $$= P(X_{(n)} \leq y) - P(X_{(1)} > x,\;X_{(n)} \leq y) = $$$$= F(y)^n - P(x \leq X_1 \leq y)\cdot P(x \leq X_2 \leq y)\dots =$$
$$= {\color{goldenrod}F(y)^n - (F(y) - F(x))^n}$$

- - -
&nbsp;

## Проверка гипотез (Б)

Имеются данные $\mathbb{X}$ – данные. Мы рассматриваем какое-то множество допустимых для этих данных распределений (все нормальные множества, все непрерывные, вообще все, какие-то два конкретных). Тогда введем *допустимое множество* функций распределения $F = \{\mathbb{F}\}$. Считаем, что $\mathbb{X} \sim \mathbb{F} \in F$

Разобъем допустимое множество на два подмножества $F = F_0 \sqcup F_1$. То есть разделим его по какому-то признаку (например положительное и отрицательное матожидание). По сути это разбиение формализует любой вопрос вида "да/нет", поэтому любой вопрос можно представить в виде такого разбиения.

Наша задача дать самый разумный ответ на этот вопрос. *Основная гипотеза* $H_0$ заключается в том, что распределение принадлежит семейству $F_0$. Если мы ее отвергаем, автоматически принимается *Альтернатива* $H_1:\;F \in F_1$

Мы должны построить в нашем выборочном пространстве $\mathbb{X}$ по тем или иным соображением *критическое множество* $\mathbb{X}^1$ такое, что если наши данные $x \in \mathbb{X}^1$, то отвергается гипотеза $H_0$ и принимается альтернатива

Соответствено, *критерий* – это правило, по которому мы отвергаем $H_0$, а *статистика критерия* $T(x)$ – та статистика, по которой мы определяем, лежит ли выборка в критическом множестве. Более формально
$$T(x) \in A_1 \Leftrightarrow x \in \mathbb{X}^1$$
Для этого нужно ввести *уровень значимости* $\alpha$ – вероятность, с которой мы готовы совершать ошибку первого рода (отклонить гипотезу, когда она была верна). То есть
$$\sup\limits_{F\in F_0}P(x\in \mathbb{X}^1_\alpha) \leq \alpha$$
Здесь мы приписали к критическому множеству альфу, явно показывая, что выбираем мы его, опираясь на значение критического множества. Дальше это часто будет опускаться.

Данный экстремум, посколько выбирается по всем распределениям из $F_0$, принято обозначать просто через $$P_0(x \in \mathbb{X}^1_\alpha) \leq \alpha$$
&nbsp;

### Алгоритм действий

0) Задаём уровень значимости $\alpha$

1) Формулируем гипотезу $H_0$ и альтернативу $H_1$

2) Подбираем (используя теоремы и знания матстата) критерий $A_1$ и статистику критерия $T$ как правило такие, чтобы во-первых $\forall F \in F_0$ вероятность $P_F(T(x) \in A_1)$ была одна и та же. Если это так, то для этой "одной и той же" вероятности принято обозначение $$P_0(T(x) \in A_1)$$Во-вторых, конечно же, чтобы$$P_0(T(x)\in A_1) \leq \alpha$$В этом неравенстве мы знаем всё, кроме $A_1$ , поэтому находим это множество
 
3) **Классический способ**: Производим наблюдение (собираем выборку) и вычисляем для нее статистику критерия. Если $T \in A_1$, то отвергаем $H_0$, иначе принимаем
   
   **Современный способ**: Составляем критическое множество так, чтобы $A_1 = [z;\;\infty)$, то есть чтобы гипотеза отвергалась при достаточно большой статистике. Затем так же по наблюдениям вычисляется $T(x)$, и по нашей статистике вычисляется *pvalue*, равное$$p_{value} = 1 - G_T(T(x));\;\;\;\;G_T(y) = P(T(X) \leq y)$$
   Если *pvalue* меньше чем $\alpha$, то гипотеза отвергается. Существует теорема, доказывающая, что статистика $p_{value}$ распределена равномерно

>[!info]+ Теорема о свойстве p-value
>
>1) Пусть статистика $T(x)$ такова, что $\forall F \in F_0:\;\;P_F(T(x) > z)$ есть одно и то же значение $P_0(T(x) > z)$.
>
> 2) Пусть критическое множество имеет вид $\{T(X) > z\}$
>
>3) Пусть $P_0(T(x) \leq z) = G_T(z)$ и $G_T(z)$ непрерывна
>
>Тогда при верной $H_0$ статистика $p_{value} = 1 - G_T(T(x))$ имеет *непрерывное* распределение на $[0,1]$

>[!quote]- Доказательство 
>$${\color{goldenrod}P(p_{value} \leq y)} =$$ $$ P(1 - G_T(T(X))\leq y) = P(G_T(T(X))\geq 1 -y) = $$ $$= P(T(X) \geq G_T^{-1}(1-y)) = 1 - G_T(G_T^{-1}(1 - y)) =$$
>$$= {\color{goldenrod}y}$$


&nbsp;

### Ошибки и мощность критерия

*Ошибка I рода* – отклонение верной гипотезы $H_0$
*Ошибка II рода* – принятие неверной гипотезы $H_0$

Так как отклонение $H_0$ $\Leftrightarrow$ выборка попала в критическое множество: $x \in \mathbb{X}_1^\alpha$, а оно выбиралось из такого условия, что $$P_0(x \in \mathbb{X}_1^{\alpha} \leq \alpha)$$Получаем автоматически, что *вероятность ошибки первого рода всегда не больше $\alpha$*

&nbsp;

*Функция мощности критерия* $\mathbb{X}_1^{\alpha}$ есть $$W(F) = W(F, \mathbb{X}_1^{\alpha}) = P_F(X \in \mathbb{X}_1^{\alpha}),\;\;\;F \in \mathbb{F}$$
То есть это есть вероятность попасть в критическое множество при конкретном распределении $F$. Для $F \in F_0$ имеем $W(F) \leq \alpha$, а для $F \in F_1$, соответственно, $W(F) > \alpha$

Тогда *вероятность ошибки первого* рода $$\beta_1(F) = W(F),\;\;\;F\in F_0$$
*Вероятность ошибки второго рода* $$\beta_2(F) = 1 - W(F),\;\;\;F\in F_1$$
В терминах мощности критическое множество строится из условия $$\forall F\in F_0:\;\;W(F, \mathbb{X}_1^{\alpha}) \leq \alpha$$
>[!summary]- Замечание
>Если совсем по-простому: чем больше мощность при $F \in F_1$ (это условие чаще всего опускают и имеют в виду именно этот случай), тем реже мы совершаем ошибку второго рода и принимаем за $H_0$ не то что нужно, а это здорово (уж лучше лишний раз отвергнуть $H_0$, чем принять)

&nbsp;

Критерий *несмещенный*, если $$\sup\limits_{F\in F_0}W(F,\mathbb{X}_1^{\alpha}) \leq \inf\limits_{F\in F_1}W(F, \mathbb{X}_1^{\alpha})$$То есть если вероятность попасть в критическое множество всегда меньше, когда $H_0$ верна. Это довольно логично и понятно что мы бы этого хотели

&nbsp;

Критерий *состоятельный*, если$$W(F,\mathbb{X}_1^{\alpha})\xrightarrow[n \rightarrow \infty]{} 1,\;\;\;F\in F_1$$
То есть если с ростом выборки (и изменением $\mathbb{X}_1^{\alpha}$) мы начинаем отличать $H_0$ от любой альтернативы с вероятностью стремящейся к 1

&nbsp;

Критическое множество $\mathbb{X}_1^*$ *равномерно наиболее мощное*, если $\forall \;\mathbb{X}_1$
1) $W(F, \mathbb{X}_1^*) \leq W(F, \mathbb{X}_1)\;\;\;F\in F_0$
2) $W(F, \mathbb{X}_1^*) \geq W(F, \mathbb{X}_1)\;\;\;F\in F_1$

То есть если наш критерий лучше и при первой гипотезе, и при второй. 

>[!warning]- Замечание
>Надо заметить, что такого практически никогда не бывает. Как правило находят "равномерно более мощный среди каких-то"

&nbsp;

### Простые гипотезы, Лемма Неймана-Пирсона

Наблюдаем выборку $X_1, X_2, \dots, X_n$ из НОРСВ, $X_i \sim F$

*Простая* гипотеза – состоящая из одного элемента. Пусть у нас простые гипотеза и альтернатива

$$H_0: F = F_0$$ $$H_1: F = F_1$$Также иногда записывают то же условие, вводя параметр $\theta \in \{\theta_0, \theta_1\}$
$$H_0:\theta = \theta_0$$ $$H_1: \theta = \theta_1$$
Введём понятие *правдоподобия*. Пусть наша выборка имеет распределение $F \in \mathbb{F}$ и $\forall\;F\in\mathbb{F}$ есть плотность $p_F$ относительно фиксированной (подходящей для всего семейства) $\sigma$-конечной меры $\mu$ на выборочном пространстве. Простыми словами, пусть мы можем записать функцию распределения через интеграл по плотности$$P_F(x\in A) = \int\limits_Ap_F(x)\mu(dx)$$Тогда *функция правдоподобия* есть$$L(X_1,\dots,X_n;F) = \prod\limits_jp_F(X_j)$$
>[!summary]- Примеры мер
>1) $\mathbb{F} = \{F(x,\theta) = 1 - \frac{1}{\theta}e^{-\frac{x}{\theta}}\}$ – множество показательных распределений. Тогда $\mu$ – *мера Лебега*, как мы привыкли
>
> 2) Для дискретного распределения, например Пуассоновского, за $\mu$ можно взять *считающую меру* $\mu(A) = |A \cap \mathbb{Z}|$
> 
>3) Для семейства из двух множеств подойдет мера $\mu(A) = P_0(X_1\in A) + P_1(X_1\in A)$ (по теореме Радона-Никодима можно доказать что она корректна)

&nbsp;

Рассмотрим множества вида$$S_\lambda=\{x\in\mathbb{X}:\;L_1(x)-\lambda L_0(x)\geq0\}$$и *критерий отношения правдоподобий* $\mathbb{X}_1 = S_\lambda$, то есть $H_0$ отвергается, если $$L_1(x) - \lambda L_0(x) \geq 0$$
>[!info]+ Лемма Неймана-Пирсона
>1) Критерий $S_\lambda$ несмешен при $\lambda > 0$
>2) Для любого критерия $R$ такого, что $$P_0(x\in R) \leq P_0(x\in S_\lambda)$$Также верно$$P_1(x\in R) \leq P_1(x\in S_\lambda)$$то есть $S_\lambda$ *наиболее мощный* среди всех критериев с вероятностью ошибки первого рода не больше чем у $S_\lambda$

>[!quote]- Доказательство
>
>**Несмещенность**
>
>1) При $\lambda \geq 1$ при $x \in S_\lambda$ верно $$L_1(x) \geq L_0(x)$$Тогда $${\color{goldenrod}P_0(x \in S_\lambda)} = \int\mathbb{I}_{S_\lambda}(x)L_0(x)\mu^n(dx) \leq \int\mathbb{I}_{S_\lambda}(x)L_1(x)\mu^n(dx) = {\color{goldenrod}P_1(x \in S_\lambda)}$$
>2)  При $\lambda < 1$ при $x \in \bar{S_\lambda}$ верно $$L_1(x)\leq L_0(x)$$Тогда $${\color{goldenrod}1 - P_0(x\in S_\lambda)} = P_0(x \in \bar{S_\lambda})=$$$$ = \int\mathbb{I}_{\bar{S_\lambda}}(x)L_0(x)\mu^n(dx) \geq \int\mathbb{I}_{\bar{S_\lambda}}(x)L_1(x)\mu^n(dx) = $$$$= P_1(x \in \bar{S_\lambda)} = {\color{goldenrod}1 - P_1(x\in S_\lambda)}$$А значит $$P_0(x \in S_\lambda) \leq P_1(x \in S_\lambda)$$
>
>&nbsp;
>
>**Наибольшая мощность**
>Пусть $$P_0(x\in R) \leq P_0(x\in S_\lambda)$$Заметим, что тогда $$\mathbb{I}_R(L_1(x) - \lambda L_0(x)) \leq \mathbb{I}_{S_\lambda}(x)(L_1(x)-\lambda L_0(x)$$
>Действительно, если $L_1(x) - \lambda L_0(x) > 0$, то сокращаем на эту скобу и получаем верное неравенство, если $L_1(x) - \lambda L_0(x) = 0$, то $0=0$, а если $L_1(x) - \lambda L_0(x) < 0$, то правая часть ноль, а левая меньше либо равна нулю.
>
>Тогда перенесем слагаемые с $L_0$ и $L_1$ в разные части и возьмем интеграл
>$$\int \mathbb{I}_{S_\lambda}L_1(x)\mu^n(dx) - \int \mathbb{I}_{R}L_1(x)\mu^n(dx) \geq \lambda(\int \mathbb{I}_{S_\lambda}L_0(x)\mu^n(dx) - \int \mathbb{I}_{R}L_0(x)\mu^n(dx))$$или что то же самое$$P_1(x\in R) - P_1(x\in S_\lambda) \geq {\color{skyblue}\lambda}({\color{skyblue}P_0(x\in R) - P_0(x\in S_\lambda)})$$
>Выделенные множители неотрицательны по условию, а значит неотрицательна и разность слева, то есть$$P_1(x\in R) \leq P_1(x \in S_\lambda)$$

>[!info]+ Теорема о единственности
>Пусть существует критерий $R$ такой, что
>1) $P_0(R) \leq P_0(S_\lambda)$
>2) $P_1(R) \geq_{(=)} P_1(S_\lambda)$
>Тогда $R = S_\lambda$ почти наверное на мере $\mu = \frac{P_0 + P_1}{2}$

>[!quote]- Доказательство
Нам нужно доказать, что $$\mu(\mathbb{I}_{S_\lambda} \neq \mathbb{I}_R)=0$$
&nbsp;
>**1** Для этого рассмотрим подмножество $$M = (\mathbb{I}_{S_\lambda} \neq \mathbb{I}_R)(L_1(x) \neq \lambda L_0(x))$$
Пусть $$\mu(M) > 0$$
И введём связанный с этим подмножеством интеграл $$I_M = \int\limits_M(\mathbb{I}_{S_\lambda} - \mathbb{I}_R)(L_1(x) - \lambda L_0(x))\mu^n(dx)$$
Заметим, что при $x\in M$ подынтегральное выражение строго положительно. Действительно, рассмотрим разные случаи
> 1) $\mathbb{I}_{S_\lambda} = 1,\;\mathbb{I}_{R} = 0$. Тогда под интегралом стоит произведение двух положительных чисел (единицы и величины строго большей нуля, так как иначе она бы не лежала в $M$)
> 2) $\mathbb{I}_{S_\lambda} = 0,\;\mathbb{I}_{R} = 1$. Аналогично, только теперь произведение двух отрицательных чисел
>
Равны они между собой быть не могут в силу принадлежности $M$
>
Значит, $I_M > 0$. Он берется по множеству $M$, а так как при  $x \notin M$ $$(\mathbb{I}_{S_\lambda} - \mathbb{I}_R)(L_1(x) - \lambda L_0(x)) = 0$$
То при остальных значениях интеграл от этого выражения обнуляется. Значит из аддитивности мы можем заключить, что соответствующий интеграл по всему выборочному пространству неотрицателен
$$I_M = \int\limits_\mathbb{X}(\mathbb{I}_{S_\lambda} - \mathbb{I}_R)(L_1(x) - \lambda L_0(x))\mu^n(dx) > 0$$
Перегруппируем его по аналогии с предыдущим доказательством
$$\int\limits_\mathbb{X}(\mathbb{I}_{S_\lambda} - \mathbb{I}_R)L_1(x ) > \lambda\int\limits_{\mathbb{X}}(\mathbb{I}_{S_\lambda} - \mathbb{I}_R)L_0(x)$$
То есть $$P_1(S_\lambda) - P_1(R) > \lambda({\color{skyblue}P_0(S_\lambda) - P_0(R)})\geq 0$$
Выделенное выражение неотрицательно по условию, а значит правая часть неравенства неотрицательна, тогда левая часть положительна, что противоречит условию теоремы. Значит мы сделали неверное предположение о том, что $\mu(M) > 0$. Соответственно, значит $$\mu(M) = 0$$
&nbsp;
>
**2** Отлично. Теперь рассмотрим вторую часть нашего "разбиения", а именно подмножество
$$N = (\mathbb{I}_{S_\lambda} \neq \mathbb{I}_R)(L_1(x) = \lambda L_0(x))$$
Так как $\mathbb{I}_{S_\lambda} \neq \mathbb{I}_R = M \cup N$, осталось доказать что $$\mu(N)=0$$
Из условий теоремы имеем $P_1(R)\geq P_1(S_\lambda)$, то есть $$\int\mathbb{I}_R(x)L_1(x)\mu^n(dx)\geq\int\mathbb{I}_{S_\lambda}(x)L_1(x)\mu^n(dx)$$
Раскроем случаи слева и справа $$\int\limits_{L_1=\lambda L_0}\mathbb{I}_R(x)L_1(x)\mu^n(dx) + \int\limits_{L_1\neq\lambda L_0}\mathbb{I}_R(x)L_1(x)\mu^n(dx) \geq $$ $$\geq \int\limits_{L_1=\lambda L_0}\mathbb{I}_{S_\lambda}(x)L_1(x)\mu^n(dx) + \int\limits_{L_1\neq\lambda L_0}\mathbb{I}_{S_\lambda}(x)L_1(x)\mu^n(dx)$$
Сгруппируем
$$\int\limits_{L_1=\lambda L_0}(\mathbb{I}_R(x)-\mathbb{I}_{S_\lambda}(x))L_1(x)\mu^n(dx) \geq \int\limits_{L_1\neq\lambda L_0}(\mathbb{I}_{S_\lambda}(x)-\mathbb{I}_R(x))L_1(x)\mu^n(dx) $$
Правая часть равна нулю, так как в рамках множества $\mathbb{I}_{S_\lambda} \neq \mathbb{I}_R$ это есть интеграл по подмножеству $M$, мера по которому равна 0, а при $\mathbb{I}_{S_\lambda} = \mathbb{I}_R$ это просто есть интеграл от нуля. Значит
$$\int\limits_{L_1=\lambda L_0}(\mathbb{I}_R(x)-\mathbb{I}_{S_\lambda}(x))L_1(x)\mu^n(dx) \geq 0 \Rightarrow$$$$\Rightarrow \int\limits_{L_1=\lambda L_0}(\mathbb{I}_{S_\lambda}(x)-\mathbb{I}_R(x))L_1(x)\mu^n(dx) \leq 0$$
Заметим, что в рамках множества $N$ и условия под интегралом получается однозначно $\mathbb{I}_{S_\lambda} = 1;\;\;\mathbb{I}_R = 0$. Тогда если $L_1(x)>0$, то интеграл по этому подмножеству тоже больше нуля, так как все множители положительны. Это не подходит под условие, значит $$\mu(N \cap\{L_1(x)>0\})=0$$
А если $L_1(x) = 0$, то из условия под интегралом и $L_0(x) = 0$, а так как в качестве меры была избрана $\mu = \frac{P_1+P_0}{2}$, то$$\mu(N \cap\{L_1(x)=0\})=0$$
Значит, наконец
$$\begin{cases}\mu(N) = 0\\\mu(M)=0\end{cases}\Rightarrow\mu(\mathbb{I}_{S_\lambda} \neq \mathbb{I}_R)=0$$

>[!important]+ Замечание
>	Если для некоторого уровня значимости $\alpha$ $\lambda=\lambda(\alpha)$ и есть решение у $$P_0(S_{\lambda(\alpha)})=\alpha$$
>	то критерий $S_{\lambda(\alpha)}$ самый мощный для этого уровня значимости
>	
>	Это **самое популярное высказываение** о критерии Неймана-Пирсона, именно его спрашивают на собесах и в повседневности, а не формализацию с теоремами 

>[!info]+ Теорема о равномерной мощности в случае сложной альтернативы
>
Пусть $\forall\;F_1 \in \mathbb{F}_1$ уравнение $$P_0(S_{\lambda(\alpha)}) = \alpha$$ имеет решение в задаче с простыми гипотезой и альтернативой $$\begin{cases}H_0:\;F=F_0\\H_1:\;F=F_1\end{cases}$$ и критическое множество не зависит от выбора $F$
>
Тогда $S_\lambda$ – равномерно наиболее мощный критерий этого уровня значимости для сложной альтернативы $$\begin{cases}H_0:\;F=F_0\\H_1:\;F\in\mathbb{F}_1\end{cases}$$

>[!quote]- Доказательство
>
Рассмотрим задачу $$\begin{cases}H_0:\;F=F_0\\H_1:\;F\in\mathbb{F}_1\end{cases}$$
Возьмём $S_\lambda$ в качестве критерия. Тогда $\forall\;F_1,R:\;\;P_0(R)=\alpha$ по лемме Неймана-Пирсона $$P_1(S_{\lambda(\alpha)})\geq P_1(R)$$

- - -

&nbsp;

## Оценка параметров (Б)
Рассмотрим другой подход к нахождению истинного распределения

Пусть у истинного распределения наших данных есть какой-то функционал $$\theta = \Phi(F)$$Это может быть дисперсия, третий момент, квантиль – всё что угодно

Наша практическая задача по выборке $X = (X_1,X_2,\dots,X_n)$ сделать *оценку* $\hat{\theta}$ этого параметра

>[!summary]- Замечание
>Всюду дальше $$\hat{\theta} = (\hat{\theta_1}, \hat{\theta_2},\dots)$$
>Чтобы не плодить целый вектор из оценок

Оценка *состоятельная*, если$$P(|\hat{\theta}_n-\theta|<\varepsilon)\xrightarrow[n \rightarrow\infty]{}0\;\;\;\Leftrightarrow\;\;\; \hat{\theta}_n\xrightarrow[]{P}\theta$$То есть с увеличением n вероятность того, что оценка будет далека от истины, крайне мала

Оценка *сильно состоятельная*, если $$\hat{\theta}_n\xrightarrow[]{\text{п.н.}}\theta$$
>[!info]+ Лемма Слуцкого
>Пусть случайные вектора $\xi_n \xrightarrow[]{d} \xi$, $\eta_n \xrightarrow[]{d} a$. Тогда
>1) $(\xi_n, \eta_n) \xrightarrow[]{d}\ (\xi, a)$
>2) $\forall\;f$ непрерывной в окрестности точки $a$ ($f\in C(\mathbb{R}^k\times B_\varepsilon(a))$)$$f(\xi_n, \eta_n)\xrightarrow[]{d} f(\xi, a)$$

>[!quote]- Доказательство
>
**1** Воспользуемся тем, что $\xi_n \xrightarrow[]{d} \xi\;\;\Leftrightarrow\;\;$характеристические функции сходятся. То есть $$\mathbb{E}e^{i(t;\;\xi_ n)}\xrightarrow[n\rightarrow\infty]{}\mathbb{E}e^{i(t;\;\xi)}$$
Рассмотрим $$|\mathbb{E}e^{i(t;\;\xi_n) + i(s;\;\eta_n)} -\mathbb{E}e^{i(t;\;\xi) + i(s;\;a)}|\leq$$$$\leq |\mathbb{E}e^{i(t;\;\xi_n) + i(s;\;\eta_n)} -\mathbb{E}e^{i(t;\;\xi_n) + i(s;\;a)}| + |\mathbb{E}e^{i(t;\;\xi_n) + i(s;\;a)} -\mathbb{E}e^{i(t;\;\xi) + i(s;\;a)}| \leq$$$$\leq \mathbb{E}|e^{i(t;\;\xi_n)}(e^{i(s;\;\eta_n)}-e^{i(s;\;a)})| + |e^{i(s;\;a)}|\cdot\mathbb{E}|(e^{i(t;\;\xi_n)}-e^{i(t;\;\xi)})|\leq$$ $$\leq {\color{pink}\mathbb{E}|(e^{i(s;\;\eta_n)}-e^{i(s;\;a)})|} + {\color{skyblue}\mathbb{E}|(e^{i(t;\;\xi_n)}-e^{i(t;\;\xi)})|}$$
Голубое слагаемое стремится к нулю по условию теоремы $(\xi_n\xrightarrow[]{d}\xi)$, а розовое слагаемое стремится к нулю, так как если $\eta_n \xrightarrow[]{d} a$, то для любой ограниченной непрерывной функции$$\mathbb{E}f(\eta_n) \rightarrow \mathbb{E}f(a)$$
Значит мы ограничили сверху значение нашей величины суммой двух стремящихся к нулю
>
&nbsp;
>
**2** Введем специфическую функцию $\varphi$, непрерывную в окрестности и единичную на некоторой подокрестности$$\varphi(x) :=\begin{cases}1,\;\;x\in B_{\varepsilon/2}(a)\\0,\;\;x\notin B_{\varepsilon}(a)\\\text{непрерывна}\end{cases}$$
Снова рассмотрим характеристическую функцию
$$|\mathbb{E}e^{itf(\xi_n,\;\eta_n)} - \mathbb{E}e^{itf(\xi,\; a)}| \leq$$$$\leq \underbrace{|\mathbb{E}e^{itf(\xi_n,\;\eta_n)} - \mathbb{E}e^{itf(\xi_n,\;\eta_n)\varphi(\eta_n)}|}_{\color{skyblue}\alpha} + \underbrace{|\mathbb{E}e^{itf(\xi_n,\;\eta_n)\varphi(\eta_n)} - \mathbb{E}e^{itf(\xi,\; a)\overbrace{\varphi(a)}^{1}}|}_{\color{pink}\beta} 
\;{\leq}$$
>
$${\leq} \;|\mathbb{E}e^{itf(\xi_n,\;\eta_n)}(1 - e ^{\varphi(n)})| + {\color{pink}\beta}$$
Заметим, что ${\color{pink}\beta} \rightarrow 0$, так как функция $\Psi(x,y) = e^{itf(x,y)\varphi(y)}$ непрерывная и ограниченная, а значит с учётом доказанного в первом пункте$$\mathbb{E}\Psi(\xi_n,\;\eta_n)\rightarrow\mathbb{E}\Psi(\xi,\;a)$$
Также заметим, что $${\color{skyblue}\alpha} \leq P(\eta_n \notin B_\varepsilon(a))$$
Потому что если $\eta_n$ не попало в окрестность $a$, то $\varphi(\eta_n) = 0$ и ${\color{skyblue} \alpha}$ обращается в ноль. А если попала, то, как видно из последнего вывода длинной цепочки неравенств, в ${\color{skyblue} \alpha}$ присутсвует множитель по модулю не превышающий единицу, отсюда возникает это неравенство
>
Правая часть неравенства стремится к нулю, т.к. если $\eta_n \xrightarrow[]{d} a$, то также $\eta_n \xrightarrow[]{P} a$, так как $a$ есть число
>
Значит снова представили необходимую величину как сумму бесконечно малых. Теорема доказана

&nbsp;

### Метод подстановки
Метод заключается в том, чтобы взять $$\hat\theta = \Phi(\hat{F}_n)$$
То есть мы просто берём необходимый функционал от эмпирической функции распределения

>[!warning]+ Примеры 
> 1) $\Phi(x) = \int xd(F(x)) = \mathbb{E}X_i$. Тогда $$\widehat{\mathbb{E}X_i} = \int xd(\widehat{F_n}(x)) = \sum\frac{1}{n}X_i = \frac{\sum X_i}{n}$$Что, согласитесь, вполне логичная оценка для матожидания
>
> 3) $\Psi(x) = \int x^2d(F(x)) = \mathbb{E}X^2_i$. Аналогично $$\widehat{\mathbb{E}X^2_i} = \frac{\sum X^2_i}{n}$$



>[!info]+ Теорема
>Оценки по методу подстановки для $a = \mathbb{E}X$ и $\sigma^2 = DX$ имеют вид 
>1) $\widehat{a} = \frac{\sum X_i}{n} = \bar{X}$
>2) $\sigma^2 = \frac{1}{n}\sum(X_i - \bar{X})^2$
>
>И являются сильно состоятельными

>[!quote]- Доказательство
>
**1** Первое необходимое нам стремление следует напрямую из УЗБЧ
$$\widehat{a} = \frac{\sum X_i}{n}\xrightarrow[]{\text{п.н.}}a$$
&nbsp;
>
**2** Далее
$$\sigma^2 = \frac{1}{n}\sum(X_i^2 - 2\bar{X}X_i + \bar{X}^2) = \frac{1}{n}\sum X_i^2 - {\color{goldenrod}\frac{1}{n}}\cdot2\bar{X}{\color{goldenrod}\sum X_i} + \bar{X}^2 =$$ $$= \frac{1}{n}\sum X_i^2 - \bar{X}^2$$
Первое слагаемое сходится почти наверное к $\mathbb{E}X^2$, а второе к $(\mathbb{E}(X))^2$, а значит разность почти наверное сходится к дисперсии

&nbsp;

### Метод моментов
Пусть параметр (вектор параметров) $\theta$ лежит во всеобъемлющем множестве $\Theta$. Обозначим $$M(\theta) = \mathbb{E}_\theta(T(X_i)) = \int T(x)d(F(x, \theta))$$Где в роли $T(X_i)$ вполне может выступать и вектор $$T = (T_1,\;T_2,\dots,\;T_n)$$

Далее пусть 
1) $\forall X_i:\;\mathbb{E}_\theta(T(X_i)) < \infty$

3) $M(\theta)$ имеет непрерывную обратную функцию на множестве значений $\theta$

4) $(\frac{1}{n}\sum T_1(X_1),\;\frac{1}{n}\sum T_2(X_1),\dots,\frac{1}{n}\sum T_m(X_1)) \in \Theta$

>[!info]+ Теорема
>
Оценка по методу моментов  имеет вид
$$\widehat{\theta} = M^{-1}(\bar{g})$$
И в условиях **1-3** является состоятельной

>[!quote]- Доказательство
>
По УЗБЧ  $$\bar{g} \xrightarrow[]{\text{п.н.}}\mathbb{E}_\theta g(x) = M(\theta)$$
А так как обратная фунция непрерывна по условию, имеем $$M^{-1}(\bar{g}) \rightarrow M^{-1}(M(\theta)) = \theta$$

&nbsp;

### Метод квантилей

Пусть выполнено
1) Для чисел $\alpha_1,\dots,\alpha_k$$\;\;F_{\theta}(z) = \alpha_i$ имеет единственное решение
2) Для $m(\theta) = (z(\alpha_1),\dots,z(\alpha_k))$ существует непрерывная обратная
3) $(X_{\lceil n\alpha_1\rceil}, X_{\lceil n\alpha_2\rceil},\dots,X_{\lceil n\alpha_k\rceil})\in m(\Theta)$

Тогда *оценка по методу квантилей* есть$$\widehat{\theta} = m^{-1}(X_{})$$