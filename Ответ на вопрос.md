_Почему Инициализация Хавьера лучше случайного значения, почему оно вообще такое? В идеале хотелось бы математику, но и по наитию тоже было бы очень не плохо понять_

Отличный вопрос! Действительно, инициализация Хавьера не случайна и выводится из сугубо математических предположений

Напомню, что инициализация Хавьера задаёт распределение, из которого следует генерировать параметры полносвязных слоёв нейронной сети. Помимо равномерной инициализации, при которой параметры генерируются из равномерного распределения, также существует инициализация из нормального распределения. Для слоя с $n_{in}$ входных чисел и $n_{out}$ выходных эти инициализации имеют вид
$$
W \sim U[-\dfrac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}};\;\dfrac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}}]
$$
$$W \sim N(0, \sqrt{\dfrac{2}{n_{in} + n_{out}}}) $$

- - -

Но почему формулы именно такие? Дело в том, что при инициализации мы боимся двух основных проблем
- Дисперсия значений будет очень большой, что приведёт к появлению больших градиентов и последующим проблемам обучения
- Дисперсия значений будет очень маленькой, что может привести к исчесающим градиентам

Давайте идейно обоснуем эти формулы. Как и всегда в математике, для начала введём основные предположения
1) Веса имеют нулевое среднее: $\mathbb{E}(W_{ij}) = 0$
2) Веса имеют одинаковую (контролируемую) дисперсию: $\mathbb{D}(W_{ij}) = \mathbb{D}(W_{kl}) = \sigma ^ 2$
3) Мы стремимся к тому, чтобы дисперсия проходящих через слой значений не изменялась при прохождении через слой

- - -

Теперь давайте проведём упрощённое доказательство. Рассмотрим выход полносвязного слоя
$$y = \sum\limits_{i=1}^{n_{in}}W_i x_i$$
Тогда дисперсия выходного значения есть:
$$\mathbb{D}(y) = n_{in}\cdot\mathbb{D}(W_i)\cdot\mathbb{D}(x_i) = n_{in}\cdot\sigma^2\cdot\mathbb{D}(x_i)$$
Если мы требуем неизменность дисперсии при прохождении слоя ($\mathbb{D}(y) = \mathbb{D}(x)$), из уравнения следует
$$\sigma^2 = \dfrac{1}{n_{in}}$$
Аналогично, но для обратного прохода по нейросети (backpropagation) получаем
$$\sigma^2 = \dfrac{1}{n_{out}}$$
Если мы возьмём **среднее гармоническое** между этими значениями в качестве некоторого компромисса, получим следующее:
$$\sigma^2 = \dfrac{2}{n_{in} + n_{out}}$$ А это и есть та дисперсия, что используется в инициализации Хавьера при нормальном распределении!

- - -

У равномерного распределения дисперсия зависит от границ распределения $-a, a$:
$$\mathbb{D}(W_{ij}) = \dfrac{a^2}{3}$$
Применяя ту же оценку $\sigma^2 = \dfrac{2}{n_{in} + n_{out}}$ получаем 
$$\dfrac{a^2}{3} = \dfrac{2}{n_{in} + n_{out}} \Rightarrow a = \sqrt{\dfrac{6}{n_{in} + n_{out}}}$$

Как мы видим, эти формулы берутся из вполне конкретных предположений и пытаются контролировать дисперсию проходящих через нейросеть значений